<!doctype html>
<html class="v2 list-page" dir="ltr" itemscope itemtype="http://schema.org/Blog" lang="en" xmlns="http://www.w3.org/1999/xhtml" xmlns:b="http://www.google.com/2005/gml/b" xmlns:data="http://www.google.com/2005/gml/data" xmlns:expr="http://www.google.com/2005/gml/expr"> 
 <head> 
  <link href="https://www.blogger.com/static/v1/widgets/115981500-css_bundle_v2.css" rel="stylesheet" type="text/css"> 
  <title>
Google AI Blog
</title> 
  <meta content="width=device-width, height=device-height, minimum-scale=1.0, initial-scale=1.0, user-scalable=0" name="viewport"> 
  <meta content="IE=Edge" http-equiv="X-UA-Compatible"> 
  <meta content="Google AI Blog" property="og:title"> 
  <meta content="en_US" property="og:locale"> 
  <meta content="http://ai.googleblog.com/" property="og:url"> 
  <meta content="Google AI Blog" property="og:site_name"> <!-- Twitter Card properties --> 
  <meta content="Google AI Blog" property="og:title"> 
  <meta content="summary" name="twitter:card"> 
  <meta content="@googleai" name="twitter:creator"> 
  <link href="https://fonts.googleapis.com/css?family=Roboto:400italic,400,500,500italic,700,700italic" rel="stylesheet" type="text/css"> 
  <link href="https://fonts.googleapis.com/icon?family=Material+Icons" rel="stylesheet"> 
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/1.11.3/jquery.min.js" type="text/javascript"></script> <!-- End --> 
  <style id="page-skin-1" type="text/css"><!--
/*
<Group description="Header Color" selector="header">
<Variable name="header.background.color" description="Header Background"
type="color" default="#ffffff"/>
</Group>
*/
.header-outer {
border-bottom: 1px solid #e0e0e0;
background: #ffffff;
}
html, .Label h2, #sidebar .rss a, .BlogArchive h2, .FollowByEmail h2.title, .widget .post h2 {
font-family: Roboto, sans-serif;
}
.plusfollowers h2.title, .post h2.title, .widget h2.title {
font-family: Roboto, sans-serif;
}
.widget-item-control {
height: 100%;
}
.widget.Header, #header {
position: relative;
height: 100%;
width: 100%;
}
}
.widget.Header .header-logo1 {
float: left;
margin-right: 15px;
padding-right: 15px;
border-right: 1px solid #ddd;
}
.header-title h1 {
color: rgba(39, 37, 37, 0.69);
display: inline-block;
font-size: 36px;
font-family: Roboto, sans-serif;
font-weight: normal;
line-height: 50px;
vertical-align: top;
margin-left: -23px;
}
.header-inner {
background-repeat: no-repeat;
background-position: right 0px;
}
.post-author,
.byline-author {
font-size: 14px;
font-weight: normal;
color: #757575;
color: rgba(0,0,0,.54);
}
.post-content .img-border {
border: 1px solid rgb(235, 235, 235);
padding: 4px;
}
.header-title a {
text-decoration: none !important;
}
pre {
border: 1px solid #bbbbbb;
margin-top: 1em 0 0 0;
padding: 0.99em;
overflow-x: auto;
overflow-y: auto;
}
pre, code {
font-size: 9pt;
background-color: #fafafa;
line-height: 125%;
font-family: monospace;
}
pre, code {
color: #060;
font: 13px/1.54 "courier new",courier,monospace;
}
.header-left .header-logo1 {
width: 128px !important;
}
.header-desc {
line-height: 20px;
margin-top: -22px;
margin-left: 17px;
}
.fb-custom img, .twitter-custom img, .gplus-share img {
cursor: pointer;
opacity: 0.54;
}
.fb-custom img:hover, .twitter-custom img:hover, .gplus-share img:hover {
opacity: 0.87;
}
.fb-like {
width: 80px;
}
.post .share {
float: right;
}
#twitter-share{
border: #CCC solid 1px;
border-radius: 3px;
background-image: -webkit-linear-gradient(top,#ffffff,#dedede);
}
.twitter-follow {
background: url(//3.bp.blogspot.com/-M7uPAxKEeh4/WKrvV1ujKCI/AAAAAAAATZE/cdHhTldtvk4q4ad1Me1XDIgQD9Aul09CACK4B/s1600/twitter-bird.png) no-repeat left center;
padding-left: 18px;
font: normal normal normal 11px/18px 'Helvetica Neue',Arial,sans-serif;
font-weight: bold;
text-shadow: 0 1px 0 rgba(255,255,255,.5);
cursor: pointer;
margin-bottom: 10px;
}
.twitter-fb {
padding-top: 2px;
}
.fb-follow-button  {
background: -webkit-linear-gradient(#4c69ba, #3b55a0);
background: -moz-linear-gradient(#4c69ba, #3b55a0);
background: linear-gradient(#4c69ba, #3b55a0);
border-radius: 2px;
height: 18px;
padding: 4px 0 0 3px;
width: 57px;
border: #4c69ba solid 1px;
}
.fb-follow-button a {
text-decoration: none !important;
text-shadow: 0 -1px 0 #354c8c;
text-align: center;
white-space: nowrap;
font-size: 11px;
color: white;
vertical-align: top;
}
.fb-follow-button a:visited {
color: white;
}
.fb-follow {
padding: 0px 5px 3px 0px;
width: 14px;
vertical-align: bottom;
}
.gplus-wrapper {
margin-top: 3px;
display: inline-block;
vertical-align: top;
}
.twitter-custom, .gplus-share {
margin-right: 12px;
}
.fb-follow-button{
margin: 10px auto;
}
sub, sup {
line-height: 0;
}
/** CUSTOM CODE **/
.post-content td {
width: inherit;
}
--></style> 
  <style id="template-skin-1" type="text/css"><!--
.header-outer {
clear: both;
}
.header-inner {
margin: auto;
padding: 0px;
}
.footer-outer {
background: #f5f5f5;
clear: both;
margin: 0;
}
.footer-inner {
margin: auto;
padding: 0px;
}
.footer-inner-2 {
/* Account for right hand column elasticity. */
max-width: calc(100% - 248px);
}
.google-footer-outer {
clear: both;
}
.cols-wrapper, .google-footer-outer, .footer-inner, .header-inner {
max-width: 978px;
margin-left: auto;
margin-right: auto;
}
.cols-wrapper {
margin: auto;
clear: both;
margin-top: 60px;
margin-bottom: 60px;
overflow: hidden;
}
.col-main-wrapper {
float: left;
width: 100%;
}
.col-main {
margin-right: 278px;
max-width: 660px;
}
.col-right {
float: right;
width: 248px;
margin-left: -278px;
}
/* Tweaks for layout mode. */
body#layout .google-footer-outer {
display: none;
}
body#layout .header-outer, body#layout .footer-outer {
background: none;
}
body#layout .header-inner {
height: initial;
}
body#layout .cols-wrapper {
margin-top: initial;
margin-bottom: initial;
}
--></style> <!-- start all head --> 
  <meta content="text/html; charset=UTF-8" http-equiv="Content-Type"> 
  <meta content="blogger" name="generator"> 
  <link href="//ai.googleblog.com/favicon.ico" rel="icon" type="image/x-icon"> 
  <link href="http://ai.googleblog.com/" rel="canonical"> 
  <link rel="alternate" type="application/atom+xml" title="Google AI Blog - Atom" href="http://ai.googleblog.com/feeds/posts/default"> 
  <link rel="alternate" type="application/rss+xml" title="Google AI Blog - RSS" href="http://ai.googleblog.com/feeds/posts/default?alt=rss"> 
  <link rel="service.post" type="application/atom+xml" title="Google AI Blog - Atom" href="https://www.blogger.com/feeds/8474926331452026626/posts/default"> <!--[if IE]><script type="text/javascript" src="https://www.blogger.com/static/v1/jsbin/3775400722-ieretrofit.js"></script>
<![endif]--> 
  <meta content="http://ai.googleblog.com/" property="og:url"> 
  <meta content="Google AI Blog" property="og:title"> 
  <meta content="The latest news from Google AI" property="og:description"> <!--[if IE]> <script> (function() { var html5 = ("abbr,article,aside,audio,canvas,datalist,details," + "figure,footer,header,hgroup,mark,menu,meter,nav,output," + "progress,section,time,video").split(','); for (var i = 0; i < html5.length; i++) { document.createElement(html5[i]); } try { document.execCommand('BackgroundImageCache', false, true); } catch(e) {} })(); </script> <![endif]--> <!-- end all head --> 
  <base target="_self"> 
  <style>
      html {
        font-family: Roboto, sans-serif;
        -moz-osx-font-smoothing: grayscale;
        -webkit-font-smoothing: antialiased;
      }
      body {
        padding: 0;
        /* This ensures that the scroll bar is always present, which is needed */
        /* because content render happens after page load; otherwise the header */
        /* would "bounce" in-between states. */
        min-height: 150%;
      }
      h2 {
        font-size: 16px;
      }
      h1, h2, h3, h4, h5 {
        line-height: 2em;
      }
      html, h4, h5, h6 {
        font-size: 14px;
      }
      a, a:visited {
        color: #4184F3;
        text-decoration: none;
      }
      a:focus, a:hover, a:active {
        text-decoration: none;
      }
      .Header {
        margin-top: 15px;
      }
      /*.Header h1 {
        font-size: 32px;
        font-weight: 300;
        line-height: 32px;
        height: 42px;
      }*/
      .header-inner .Header .titlewrapper {
        padding: 0;
        margin-top: 30px;
      }
      .header-inner .Header .descriptionwrapper {
        padding: 0;
        margin: 0;
      }
      .cols-wrapper {
        margin-top: 56px;
      }
      .header-outer, .cols-wrapper, .footer-outer, .google-footer-outer {
        padding: 0 60px;
      }
      .header-inner {
        height: 256px;
        position: relative;
      }
      html, .header-inner a {
        color: #212121;
        color: rgba(0,0,0,.87);
      }
      .header-inner .google-logo {
        display: inline-block;
        background-size: contain;
        z-index: 1;
        height: 100px;
        overflow: hidden;
        margin-top: -23px;
        margin-right: 8px;
      }
      .header-left {
        position: absolute;
        top: 50%;
        -webkit-transform: translateY(-50%);
        transform: translateY(-50%);
        margin-top: 2px;
        width: 100%;
        margin-left: -15px;
      }
      .google-logo {
        margin-left: -4px;
      }
      .google-logo img{
        height: 250px;
    	margin-top: -79px;
      }
      #google-footer {
        position: relative;
        font-size: 13px;
        list-style: none;
        text-align: right;
      }
      #google-footer a {
        color: #444;
      }
      #google-footer ul {
        margin: 0;
        padding: 0;
        height: 144px;
        line-height: 144px;
      }
      #google-footer ul li {
        display: inline;
      }
      #google-footer ul li:before {
        color: #999;
        content: "\00b7";
        font-weight: bold;
        margin: 5px;
      }
      #google-footer ul li:first-child:before {
        content: '';
      }
      #google-footer .google-logo-dark {
        left: 0;
        margin-top: -16px;
        position: absolute;
        top: 50%;
      }
      /** Sitemap links. **/
      .footer-inner-2 {
        font-size: 14px;
        padding-top: 42px;
        padding-bottom: 74px;
      }
      .footer-inner-2 .HTML h2 {
        color: #212121;
        color: rgba(0,0,0,.87);
        font-size: 14px;
        font-weight: 500;
        padding-left: 0;
        margin: 10px 0;
      }
      .footer-inner-2 .HTML ul {
        font-weight: normal;
        list-style: none;
        padding-left: 0;
      }
      .footer-inner-2 .HTML li {
        line-height: 24px;
        padding: 0;
      }
      .footer-inner-2 li a {
        color: rgba(65,132,243,.87);
      }
      /** Archive widget. **/
      .BlogArchive {
        font-size: 13px;
        font-weight: normal;
      }
      .BlogArchive .widget-content {
        display: none;
      }
      .BlogArchive h2, .Label h2 {
        color: #4184F3;
        text-decoration: none;
      }
      .BlogArchive .hierarchy li {
        display: inline-block;
      }
      /* Specificity needed here to override widget CSS defaults. */
      .BlogArchive #ArchiveList ul li, .BlogArchive #ArchiveList ul ul li {
        margin: 0;
        padding-left: 0;
        text-indent: 0;
      }
      .BlogArchive .intervalToggle {
        cursor: pointer;
      }
      .BlogArchive .expanded .intervalToggle .new-toggle {
        -ms-transform: rotate(180deg);
        transform: rotate(180deg);
      }
      .BlogArchive .new-toggle {
        float: right;
        padding-top: 3px;
        opacity: 0.87;
      }
      #ArchiveList {
        text-transform: uppercase;
      }
      #ArchiveList .expanded > ul:last-child {
        margin-bottom: 16px;
      }
      #ArchiveList .archivedate {
        width: 100%;
      }
      /* Months */
      .BlogArchive .items {
        max-width: 150px;
        margin-left: -4px;
      }
      .BlogArchive .expanded .items {
        margin-bottom: 10px;
        overflow: hidden;
      }
      .BlogArchive .items > ul {
        float: left;
        height: 32px;
      }
      .BlogArchive .items a {
        padding: 0 4px;
      }
      .Label {
        font-size: 13px;
        font-weight: normal;
      }
      .sidebar-icon {
        display: inline-block;
        width: 24px;
        height: 24px;
        vertical-align: middle;
        margin-right: 12px;
        margin-top: -1px
      }
      .Label a {
        margin-right: 4px;
      }
      .Label .widget-content {
        display: none;
      }
      .FollowByEmail {
        font-size: 13px;
        font-weight: normal;
      }
      .FollowByEmail h2 {
        background: url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABEAAAALCAYAAACZIGYHAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAUBJREFUeNrMkSGLAlEUhb+ZB4JFi8mx2cz+ApvhRUGTcUCrNqNJDYIi+DO0GUwmQXDK2DSIoGgZcSaIjDrzwrK4ssvChj1w0733O+fdp+m6PozH4yQSCfb7Pa7r8pOi0SjJZBLP8zgej4gAIMvlMuPxmADIYrHger1+C6lUKmo+NJ/NZojb7SZDWiwWo1qtks1msW2bw+HwZdkwDHq9HvV6nel0SqvVYrvdIh6Ph3Qch+VyqRYLhQJSSjRNw7IsfN9XgGKxSLfbJZfL0e/3aTabrFYr7vc7IujLcOh8PqunrNdr0uk0pVKJVCpFJBJRgEajweVyod1uMxgM2O12BAGUgRbU8DV2JpOhVquRz+cRQii3+XxOp9NRN3jVR5LPOp1OjEYjlSL8hclkgmmabDabt4d+m+S30vkD/R/IU4ABAPTZgnZdmG/PAAAAAElFTkSuQmCC");
        background-repeat: no-repeat;
        background-position: 0 50%;
        text-indent: 30px;
      }
      .FollowByEmail .widget-content {
        display: none;
      }
      .searchBox input {
        border: 1px solid #eee;
        color: #212121;
        color: rgba(0,0,0,.87);
        font-size: 14px;
        padding: 8px 8px 8px 40px;
        width: 164px;
        font-family: Roboto, sans-serif;
        background: url("https://www.gstatic.com/images/icons/material/system/1x/search_grey600_24dp.png") 8px center no-repeat;
      }
      .searchBox ::-webkit-input-placeholder { /* WebKit, Blink, Edge */
        color:    rgba(0,0,0,.54);
      }
      .searchBox :-moz-placeholder { /* Mozilla Firefox 4 to 18 */
        color:    #000;
        opacity:  0.54;
      }
      .searchBox ::-moz-placeholder { /* Mozilla Firefox 19+ */
        color:    #000;
        opacity:  0.54;
      }
      .searchBox :-ms-input-placeholder { /* Internet Explorer 10-11 */
        color:    #757575;
      }
      .widget-item-control {
        margin-top: 0px;
      }
      .section {
        margin: 0;
        padding: 0;
      }
      #sidebar-top {
        border: 1px solid #eee;
      }
      #sidebar-top > div {
        margin: 16px 0;
      }
      .widget ul {
        line-height: 1.6;
      }
      /*main post*/
      .post {
		display: inline-block;
        margin-bottom:30px;
      }
      #main .post .title {
        margin: 0;
      }
      #main .post .title a {
        color: #212121;
        color: rgba(0,0,0,.87);
        font-weight: normal;
        font-size: 24px;
      }
      #main .post .title a:hover {
        text-decoration:none;
        color:#4184F3;
      }
      .message,  #main .post .post-header {
        margin: 0;
        padding: 0;
      }
      #main .post .post-header .caption, #main .post .post-header .labels-caption,  #main .post .post-footer .caption, #main .post .post-footer .labels-caption {
        color: #444;
        font-weight: 500;
      }
      #main .tr-caption-container td {
        text-align: center;
      }
      #main .post .tr-caption {
        color: #757575;
        color: rgba(0,0,0,.54);
        display: block;
        padding-bottom: 20px;
		line-height: 1.5;
      }
      #main .post .tr-caption-container {
        line-height: 24px;
        padding: 4px 0;
        text-align: center;
      }
      #main .post .post-header .published{
        font-size:11px;
        font-weight:bold;
      }
      .post-header .publishdate {
        font-size: 17px;
        font-weight:normal;
        color: #757575;
        color: rgba(0,0,0,.54);
      }
      #main .post .post-footer{
        font-size:12px;
        padding-bottom: 21px;
      }
      .label-footer {
        margin-bottom: 12px;
        margin-top: 12px;
      }
      .comment-img {
        margin-right: 16px;
        opacity: 0.54;
        vertical-align: middle;
      }
      #main .post .post-header .published {
        margin-bottom: 10px;
        margin-top: -2px;
      }
      .post .post-content {
        color: #212121;
        color: rgba(0,0,0,.87);
        font-size: 15px;
        margin: 5px 0 36px 0;
        line-height: 20px;
      }
      .post-body .post-content ul, .post-body .post-content ol {
        margin: 16px 0;
        padding: 0 48px;
      }
      .post-summary {
        display: none;
      }
      /* Another old-style caption. */
      .post-content div i, .post-content div + i {
        font-size: 14px;
        font-style: normal;
        color: #757575;
        color: rgba(0,0,0,.54);
        display: block;
        line-height: 24px;
        margin-bottom: 16px;
        text-align: left;
      }
      /* Another old-style caption (with link) */
      .post-content a > i {
        color: #4184F3 !important;
      }
      /* Old-style captions for images. */
      .post-content .separator + div:not(.separator) {
        margin-top: -16px;
      }
      /* Capture section headers. */
      .post-content br + br + b, .post-content .space + .space + b, .post-content .separator + b {
        display: inline-block;
      }
      /*.post-content li {
        line-height: 1.5;
      }*/
      /* Override all post images/videos to left align. */
      .post-content .separator, .post-content > div {
        text-align: center;
      }
      .post-content .separator > a, .post-content .separator > span {
        margin-left: 0 !important;
      }
      .post-content img {
        max-width: 100%;
        height: auto;
        width: auto;
      }
      .post-content .tr-caption-container img {
        margin-bottom: 12px;
      }
      .post-content iframe, .post-content embed {
        max-width: 100%;
      }
      .post-content .carousel-container {
        margin-bottom: 48px;
      }
      #main .post-content b {
        font-weight: 500;
      }
      /* These are the main paragraph spacing tweaks. */
      #main .post-content br {
        /*content: ' ';*/
        display: block;
        padding: 4px;
      }
      .post-content .space {
        display: block;
        height: 8px;
      }
      .post-content iframe + .space, .post-content iframe + br {
        padding: 0 !important;
      }
      #main .post .jump-link {
        margin-bottom:10px;
      }
      .post-content img, .post-content iframe {
        margin: 15px 0 20px 0;
      }
      .post-content > img:first-child, .post-content > iframe:first-child {
        margin-top: 0;
      }
      .col-right .section {
        padding: 0 16px;
      }
      #aside {
        background:#fff;
        border:1px solid #eee;
        border-top: 0;
      }
      #aside .widget {
        margin:0;
      }
      #aside .widget h2, #ArchiveList .toggle + a.post-count-link {
        color: #212121;
        color: rgba(0,0,0,.87);
        font-weight: 400 !important;
        margin: 0;
      }
      #ArchiveList .toggle {
        float: right;
      }
      #ArchiveList .toggle .material-icons {
        padding-top: 4px;
      }
      #sidebar .tab {
        cursor: pointer;
      }
      #sidebar .tab .arrow {
        display: inline-block;
        float: right;
      }
      #sidebar .tab .icon {
        display: inline-block;
        vertical-align: top;
        height: 24px;
        width: 24px;
        margin-right: 13px;
        margin-left: -1px;
        margin-top: 1px;
        color: #757575;
        color: rgba(0,0,0,.54);
      }
      #sidebar .widget-content > :first-child {
        padding-top: 8px;
      }
      #sidebar .active .tab .arrow {
        -ms-transform: rotate(180deg);
        transform: rotate(180deg);
      }
      #sidebar .arrow {
        color: #757575;
        color: rgba(0,0,0,.54);
      }
      #sidebar .widget h2 {
        font-size: 14px;
        line-height: 24px;
        display: inline-block;
      }
      #sidebar .widget .BlogArchive {
        padding-bottom: 8px;
      }
      #sidebar .widget {
        border-bottom: 1px solid #eee;
        box-shadow: 0px 1px 0 white;
        margin-bottom: 0;
        padding: 14px 0;
        min-height: 20px;
      }
      #sidebar .widget:last-child {
        border-bottom: none;
        box-shadow: none;
        margin-bottom: 0;
      }
      #sidebar ul {
        margin: 0;
        padding: 0;
      }
      #sidebar ul li {
        list-style:none;
        padding:0;
      }
      #sidebar ul li a {
        line-height: 32px;
      }
      #sidebar .archive {
        background-image: url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAYCAYAAADzoH0MAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAE1JREFUeNpiNDY23s9AAWBioBCwYBM8c+YMVsUmJibEGYBNMS5DaeMFfDYSZQA2v9I3FrB5AZeriI4FmnrBccCT8mhmGs1MwyAzAQQYAKEWG9zm9QFEAAAAAElFTkSuQmCC");
        height: 24px;
        line-height: 24px;
        padding-left: 30px;
      }
      #sidebar .labels {
        background-image: url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABEAAAARCAYAAAA7bUf6AAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAUxJREFUeNpiNDY23s9AAMycOfM7UF05kHkZmzwTMkdSUhKrIcXFxZy3bt3qBjIN8RrS09PDsHnzZjCNDr58+cKQlpbGDjSoHcg1w2oIyAUODg5gARCNzUVIBrUCuVYYhjx//pzhwIEDYAEQDeJjA1CDWIAGNQK59jBxRuSABbkAlwHIgIeHh2HWrFn/1NTU2oDcvSgBS4wBSC5iArqoCsj1YGIgEyAZVMoEchqlBjEB/cZAiUHg2AEGznpKDAImxOeM////B4VLKtBvEUCngZ1ILKivr3/u6+ubBzJAGZQ9gC5aQoqLgAY8BhkAZL4BuQQkxgXE34A4BuiiZEIuAhrwEGhAEZD5DpzYoIaA2UAM4kQADUrHZRDUgAIg8wO2XAwzbQXQa5OweQ1owB10AyA6gS7BgX1u3ry5397eHow3bdo0EyjGi00tQIABANPgyAH1q1eaAAAAAElFTkSuQmCC");
        height: 20px;
        line-height: 20px;
        padding-left: 30px;
      }
      #sidebar .rss a {
        background-image: url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABIAAAASCAYAAABWzo5XAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAX5JREFUeNqsVDGSgkAQHL2rIiIikohIc/EBRkbwAIwuwgfwAXiAD9AHSI7kEkECRCb6AIyINDLx7K0aa6kT7uq0q7YYtnZ7umdnt7darXbr9Zpegeu61DNNc0dvwCcH4/GYJpMJnc9nOhwOVJbl/4hAAokMECZJQtvt9k+kH7qufyEYDAakqqqYxFdRFBqNRmTbNg2HQ0rTlK7XayvR0xqBdDqdkuM4dE/0ULhYLOh4PHYrknG5XGi/31MYhuL/nkwonM1mlGUZ1XXdrsiyLGEDhY7juJEZ1u5tIixDGdYhmYw+B7CAzPP5nDabjdgIAgCksMX1832/3drtdqPT6SQWapomiGEFNkDEdpDMMAzK81ys/7XYy+XyoQgq2WoURSIJ2iIIgp/WZCCTvFm2wgeAU31aI3Q2GhIDMeB53qPYPIcm5VrxXIOIOxsDMStjVawAc1VViRgN22lNBiuQN3GR+SY07hpOoStmFQAKXRRFY93bnpG+fONfedi+BRgAbkS8Fxp7QQIAAAAASUVORK5CYII=");
      }
      #sidebar .subscription a {
        background-image: url("data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABEAAAALCAYAAACZIGYHAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAUBJREFUeNrMkSGLAlEUhb+ZB4JFi8mx2cz+ApvhRUGTcUCrNqNJDYIi+DO0GUwmQXDK2DSIoGgZcSaIjDrzwrK4ssvChj1w0733O+fdp+m6PozH4yQSCfb7Pa7r8pOi0SjJZBLP8zgej4gAIMvlMuPxmADIYrHger1+C6lUKmo+NJ/NZojb7SZDWiwWo1qtks1msW2bw+HwZdkwDHq9HvV6nel0SqvVYrvdIh6Ph3Qch+VyqRYLhQJSSjRNw7IsfN9XgGKxSLfbJZfL0e/3aTabrFYr7vc7IujLcOh8PqunrNdr0uk0pVKJVCpFJBJRgEajweVyod1uMxgM2O12BAGUgRbU8DV2JpOhVquRz+cRQii3+XxOp9NRN3jVR5LPOp1OjEYjlSL8hclkgmmabDabt4d+m+S30vkD/R/IU4ABAPTZgnZdmG/PAAAAAElFTkSuQmCC");
      }
      #sidebar-bottom {
        background: #f5f5f5;
        border-top:1px solid #eee;
      }
      #sidebar-bottom .widget {
        border-bottom: 1px solid #e0e0e0;
        padding: 15px 0;
        text-align: center;
      }
      #sidebar-bottom > div:last-child {
        border-bottom: 0;
      }
      #sidebar-bottom .text {
        line-height: 20px;
      }
      /* Home, forward, and backward pagination. */
      .blog-pager {
        border-top : 1px #e0e0e0 solid;
        padding-top: 10px;
        margin-top: 15px;
        text-align: right !important;
      }
      #blog-pager {
        margin-botom: 0;
        margin-top: -14px;
        padding: 16px 0 0 0;
      }
      #blog-pager a {
        display: inline-block;
      }
      .blog-pager i.disabled {
        opacity: 0.2 !important;
      }
      .blog-pager i {
        color: black;
        margin-left: 16px;
        opacity: 0.54;
      }
      .blog-pager i:hover, .blog-pager i:active {
        opacity: 0.87;
      }
      #blog-pager-older-link, #blog-pager-newer-link {
        float: none;
      }
      .gplus-profile {
        background-color: #fafafa;
        border: 1px solid #eee;
        overflow: hidden;
        width: 212px;
      }
      .gplus-profile-inner {
        margin-left: -1px;
        margin-top: -1px;
      }
      /* Sidebar follow buttons. */
      .followgooglewrapper {
        padding: 12px 0 0 0;
      }
      .loading {
        visibility: hidden;
      }
      .detail-page .post-footer .cmt_iframe_holder {
        padding-top: 40px !important;
      }
      /** Desktop **/
      @media (max-width: 900px) {
        .col-right {
          display: none;
        }
        .col-main {
          margin-right: 0;
          min-width: initial;
        }
        .footer-outer {
          display: none;
        }
        .cols-wrapper {
          min-width: initial;
        }
        .google-footer-outer {
          background-color: #f5f5f5;
        }
      }
      /** Tablet **/
      @media (max-width: 712px) {
        .header-outer, .cols-wrapper, .footer-outer, .google-footer-outer {
          padding: 0 40px;
        }
      }
      /* An extra breakpoint accommodating for long blog titles. */
      @media (max-width: 600px) {
        .header-left {
          height: 100%;
          top: inherit;
          margin-top: 0;
          -webkit-transform: initial;
          transform: initial;
        }
        .header-title {
          margin-top: 88px;
        }
        .header-inner .google-logo {
          height: 40px;
          margin-top: 3px;
        }
        .header-inner .google-logo img {
          height: 200px;
        }
        .header-title h2 {
          font-size: 32px;
          line-height: 40px;
        }
		.header-title h1{
          font-size: 32px;
          line-height: 50px;
		}
        .header-desc {
          bottom: 94px;
          position: absolute;
          margin-left: 14px;
        }
      }
      /** Mobile/small desktop window; also landscape. **/
      @media (max-width: 480px), (max-height: 480px) {
        .header-outer, .cols-wrapper, .footer-outer, .google-footer-outer {
          padding: 0 16px;
        }
        .cols-wrapper {
          margin-top: 0;
        }
        .post-header .publishdate, .post .post-content {
          font-size: 16px;
        }
        .post .post-content {
          line-height: 28px;
          margin-bottom: 30px;
        }
        .post {
          margin-top: 30px;
        }
        .byline-author {
          display: block;
          font-size: 12px;
          line-height: 24px;
          margin-top: 6px;
        }
        #main .post .title a {
          font-weight: 500;
          color: #4c4c4c;
          color: rgba(0,0,0,.70);
        }
        #main .post .post-header {
          padding-bottom: 12px;
        }
        #main .post .post-header .published {
          margin-bottom: -8px;
          margin-top: 3px;
        }
        .post .read-more {
          display: block;
          margin-top: 14px;
        }
        .post .tr-caption {
          font-size: 12px;
        }
        #main .post .title a {
          font-size: 20px;
          line-height: 30px;
        }
        .post-content iframe {
          /* iframe won't keep aspect ratio when scaled down. */
          max-height: 240px;
        }
        .post-content .separator img, .post-content .tr-caption-container img, .post-content iframe {
          max-width: inherit;
          width: calc(100% + 32px);
        }
        /*.post-content table, .post-content td {
          width: 100%;
        }*/
        #blog-pager {
          margin: 0;
          padding: 16px 0;
        }
        /** List page tweaks. **/
        .list-page .post-original {
          display: none;
        }
        .list-page .post-summary {
          display: block;
        }
        .list-page .comment-container {
          display: none;
        }
        .list-page #blog-pager {
          padding-top: 0;
          border: 0;
          margin-top: -8px;
        }
        .list-page .label-footer {
          display: none;
        }
        .list-page #main .post .post-footer {
          border-bottom: 1px solid #eee;
          margin: -16px 0 0 0;
          padding: 0 0 20px 0;
        }
        .list-page .post .share {
          display: none;
        }
        /** Detail page tweaks. **/
        .detail-page .post-footer .cmt_iframe_holder {
          padding-top: 32px !important;
        }
        .detail-page .label-footer {
          margin-bottom: 0;
        }
        .detail-page #main .post .post-footer {
          padding-bottom: 0;
        }
        .detail-page #comments {
          display: none;
        }
      }
      [data-about-pullquote], [data-is-preview], [data-about-syndication] {
        display: none;
      }
    </style> 
  <noscript> 
   <style>
        .loading { visibility: visible }</style> 
  </noscript> 
  <script type="text/javascript">
        (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
        ga('create', 'UA-961555-69', 'auto', 'blogger');
        ga('blogger.send', 'pageview');
      </script> 
  <link href="https://www.blogger.com/dyn-css/authorization.css?targetBlogID=8474926331452026626&amp;zx=c03a94f2-91c5-4ef3-ae7f-4974e97ebcea" media="none" onload="if(media!='all')media='all'" rel="stylesheet">
  <noscript>
   <link href="https://www.blogger.com/dyn-css/authorization.css?targetBlogID=8474926331452026626&amp;zx=c03a94f2-91c5-4ef3-ae7f-4974e97ebcea" rel="stylesheet">
  </noscript> 
 </head> 
 <body> 
  <script type="text/javascript">
      //<![CDATA[
      var axel = Math.random() + "";
      var a = axel * 10000000000000;
      document.write('<iframe src="https://2542116.fls.doubleclick.net/activityi;src=2542116;type=gblog;cat=googl0;ord=ord=' + a + '?" width="1" height="1" frameborder="0" style="display:none"></iframe>');
      //]]>
    </script> 
  <noscript> 
   <img alt="" height="1" src="https://ad.doubleclick.net/ddm/activity/src=2542116;type=gblog;cat=googl0;ord=1?" width="1"> 
  </noscript> <!-- Header --> 
  <div class="header-outer"> 
   <div class="header-inner"> 
    <div class="section" id="header">
     <div class="widget Header" data-version="1" id="Header1"> 
      <div class="header-left"> 
       <div class="header-title"> <a class="google-logo" href="http://ai.googleblog.com/"> <img height="100" src="//2.bp.blogspot.com/-qRz-hnwUdY4/WulXSQ6Rv4I/AAAAAAAATvQ/shk7KsphA0c3E3nUMsDVASqYaH0PhLPNwCK4BGAYYCw/s1600/GoogleAI_logo_horizontal_color_rgb.png"> </a> <a href="/."> <h1> Blog </h1> </a> 
       </div> 
       <div class="header-desc">
         The latest news from Google AI 
       </div> 
      </div> 
     </div>
    </div> 
   </div> 
  </div> <!-- all content wrapper start --> 
  <div class="cols-wrapper loading"> 
   <div class="col-main-wrapper"> 
    <div class="col-main"> 
     <div class="section" id="main">
      <div class="widget Blog" data-version="1" id="Blog1"> 
       <div class="post" data-id="7927167454181190382" itemscope itemtype="http://schema.org/BlogPosting"> 
        <h2 class="title" itemprop="name"> <a href="http://ai.googleblog.com/2021/06/extending-contrastive-learning-to.html" itemprop="url" title="Extending Contrastive Learning to the Supervised Setting"> Extending Contrastive Learning to the Supervised Setting </a> </h2> 
        <div class="post-header"> 
         <div class="published"> <span class="publishdate" itemprop="datePublished"> Friday, June 4, 2021 </span> 
         </div> 
        </div> 
        <div class="post-body"> 
         <div class="post-content" itemprop="articleBody"> 
          <script type="text/template">
                          <span class="byline-author">Posted by AJ Maschinot, Senior Software Engineer and Jenny Huang, Product Manager, Google Research</span>

<p>
In recent years, <a href="https://arxiv.org/pdf/2011.00362.pdf">self-supervised representation learning</a>, which is used in a variety of <a href="https://ai.googleblog.com/2020/04/advancing-self-supervised-and-semi.html">image</a> and <a href="https://arxiv.org/abs/1704.06888">video</a> tasks, has significantly advanced due to the application of <a href="https://arxiv.org/abs/2002.05709">contrastive learning</a>. These contrastive learning approaches typically teach a model to pull together the representations of a target image (a.k.a., the &#8220;anchor&#8221;) and a matching (&#8220;positive&#8221;) image in embedding space, while also pushing apart the anchor from many non-matching (&#8220;negative&#8221;) images. Because labels are assumed to be unavailable in self-supervised learning, the positive is often an augmentation of the anchor, and the negatives are chosen to be the other samples from the training minibatch. However, because of this random sampling, false negatives, i.e., negatives generated from samples of the same class as the anchor, can cause a <a href="https://arxiv.org/abs/2011.11765">degradation</a> in the representation quality. Furthermore, determining the optimal method to generate positives is still an area of <a href="https://ai.googleblog.com/2020/08/understanding-view-selection-for.html">active research</a>.
</p>
<p>
In contrast to the self-supervised approach, a fully-supervised approach could use labeled data to generate positives from existing same-class examples, providing more variability in pretraining than could typically be achieved by simply augmenting the anchor. However, very little work has been done to successfully apply contrastive learning in the fully-supervised domain.
</p>
<p>
In &#8220;<a href="https://arxiv.org/abs/2004.11362">Supervised Contrastive Learning</a>&#8221;, presented at <a href="https://nips.cc/virtual/2020/public/poster_d89a66c7c80a29b1bdbab0f2a1a94af8.html">NeurIPS 2020</a>, we propose a novel loss function, called SupCon, that bridges the gap between self-supervised learning and fully supervised learning and enables contrastive learning to be applied in the supervised setting. Leveraging labeled data, SupCon encourages normalized embeddings from the <em>same class</em> to be pulled closer together, while embeddings from <em>different classes</em> are pushed apart. This simplifies the process of positive selection, while avoiding potential false negatives. Because it accommodates multiple positives per anchor, this approach results in an improved selection of positive examples that are more varied, while still containing semantically relevant information. SupCon also allows label information to play an active role in representation learning rather than restricting it to be used only in downstream training, as is the case for conventional contrastive learning. To the best of our knowledge, this is the first contrastive loss to consistently perform better on large-scale image classification problems than the common approach of using <a href="https://en.wikipedia.org/wiki/Cross_entropy">cross-entropy loss</a> to train the model directly. Importantly, SupCon is straightforward to implement and stable to train, provides consistent improvement to top-1 accuracy for a number of datasets and architectures (including <a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html">Transformer</a> architectures), and is robust to <a href="https://arxiv.org/abs/1903.12261">image corruptions</a> and hyperparameter variations. 
</p>
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://1.bp.blogspot.com/-_fMby-ad2AA/YLpXBtlI7fI/AAAAAAAAHsI/7TE6W0F5yNAhGEZFwPSvIJUM51A0ETs5wCLcBGAsYHQ/s1999/image5.png" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="922" data-original-width="1999" height="296" src="https://1.bp.blogspot.com/-_fMby-ad2AA/YLpXBtlI7fI/AAAAAAAAHsI/7TE6W0F5yNAhGEZFwPSvIJUM51A0ETs5wCLcBGAsYHQ/w640-h296/image5.png" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Self-supervised (<b>left</b>) vs supervised (<b>right</b>) contrastive losses: The self-supervised contrastive loss contrasts a single positive for each anchor (i.e., an augmented version of the same image) against a set of negatives consisting of the entire remainder of the minibatch. The supervised contrastive loss considered in this paper, however, contrasts the set of all samples from the same class as positives against the negatives from the remainder of the batch.</td></tr></tbody></table>
<p>
<b>The Supervised Contrastive Learning Framework</b><br />
SupCon can be seen as a generalization of both the <a href="https://arxiv.org/abs/2002.05709">SimCLR</a> and <a href="https://papers.nips.cc/paper/2016/file/6b180037abbebea991d8b1232f8a8ca9-Paper.pdf">N-pair</a> losses &#8212; the former uses positives generated from the same sample as that of the anchor, and the latter uses positives generated from different samples by exploiting known class labels. The use of many positives and many negatives for each anchor allows SupCon to achieve state-of-the-art performance without the need for <a href="https://arxiv.org/abs/2010.04592">hard negative mining</a> (i.e., searching for negatives similar to the anchor), which can be difficult to tune properly. 
</p>
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://1.bp.blogspot.com/-yfy-d_jamHI/YLpXGzdVOpI/AAAAAAAAHsM/gMN-Us-Rg7sZsycXO7UyF7UTK9JyYTAOgCLcBGAsYHQ/s1680/image1.png" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="1124" data-original-width="1680" src="https://1.bp.blogspot.com/-yfy-d_jamHI/YLpXGzdVOpI/AAAAAAAAHsM/gMN-Us-Rg7sZsycXO7UyF7UTK9JyYTAOgCLcBGAsYHQ/s320/image1.png" width="320" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">SupCon subsumes multiple losses from the literature and is a generalization of the SimCLR and N-Pair losses.</td></tr></tbody></table>
<p>
This method is structurally similar to those used in self-supervised contrastive learning, with modifications for supervised classification. Given an input batch of data, we first apply data augmentation twice to obtain two copies, or &#8220;views,&#8221; of each sample in the batch (though one could create and use any number of augmented views). Both copies are forward propagated through an encoder network, and the resulting embedding is then <a href="https://mathworld.wolfram.com/L2-Norm.html">L2-normalized</a>. Following standard practice, the representation is further propagated through an <a href="https://arxiv.org/abs/2002.05709">optional projection network</a> to help identify meaningful features. The supervised contrastive loss is computed on the normalized outputs of the projection network. Positives for an anchor consist of the  representations originating from the same batch instance as the anchor or from other instances with the same label as the anchor; the negatives are then all remaining instances. To measure performance on downstream tasks, we train a linear classifier on top of the frozen representations.
</p>
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://1.bp.blogspot.com/-dqz--25E0_k/YLpXMDv-uJI/AAAAAAAAHsQ/Qwir9J1mjWUfchGiiaI1okkX-KVitKVagCLcBGAsYHQ/s1999/image3.png" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="940" data-original-width="1999" height="300" src="https://1.bp.blogspot.com/-dqz--25E0_k/YLpXMDv-uJI/AAAAAAAAHsQ/Qwir9J1mjWUfchGiiaI1okkX-KVitKVagCLcBGAsYHQ/w640-h300/image3.png" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Cross-entropy, self-supervised contrastive loss and supervised contrastive loss <strong>Left:</strong> The cross-entropy loss uses labels and a softmax loss to train a classifier. <strong>Middle:</strong> The self-supervised contrastive loss uses a contrastive loss and data augmentations to learn representations. <strong>Right: </strong>The supervised contrastive loss also learns representations using a contrastive loss, but uses label information to sample positives in addition to augmentations of the same image.</td></tr></tbody></table>
<p>
<b>Key Findings </b><br />
SupCon consistently boosts top-1 accuracy compared to cross-entropy, <a href="https://en.wikipedia.org/wiki/Margin_classifier">margin classifiers</a> (with use of labels), and self-supervised contrastive learning techniques on <a href="http://www.cs.toronto.edu/~kriz/cifar.html">CIFAR-10 and CIFAR-100</a> and <a href="https://www.image-net.org/">ImageNet</a> datasets. With SupCon, we achieve excellent top-1 accuracy on the ImageNet dataset with the <a href="https://arxiv.org/abs/1512.03385">ResNet-50 and ResNet-200</a> architectures. On ResNet-200, we achieve a top-1 accuracy of 81.4%, which is a 0.8% improvement over the <a href="https://arxiv.org/abs/1905.00397">state-of-the-art</a> cross-entropy loss using the same architecture (which represents a <a href="https://paperswithcode.com/sota/image-classification-on-imagenet">significant advance for ImageNet</a>). We also compared cross-entropy and SupCon on a <a href="https://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html">Transformer-based ViT-B/16</a> model and found a consistent improvement over cross-entropy (77.8% versus 76% for ImageNet; 92.6% versus 91.6% for CIFAR-10) under the same data augmentation regime (without any higher-resolution fine-tuning).
</p>
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://1.bp.blogspot.com/-bWernq6iaDU/YLpXRjyIU0I/AAAAAAAAHsU/Q_em95oBV5wK1ymi1cZFXGc87a_0ubZ2ACLcBGAsYHQ/s1484/image2.png" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="918" data-original-width="1484" height="397" src="https://1.bp.blogspot.com/-bWernq6iaDU/YLpXRjyIU0I/AAAAAAAAHsU/Q_em95oBV5wK1ymi1cZFXGc87a_0ubZ2ACLcBGAsYHQ/w640-h397/image2.png" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">The SupCon loss consistently outperforms cross-entropy with standard data augmentation strategies (<a href="https://arxiv.org/abs/1805.09501">AutoAugment</a>, <a href="https://arxiv.org/abs/1909.13719">RandAugment</a> and <a href="https://arxiv.org/abs/1905.04899">CutMix</a>). We show top-1 accuracy for ImageNet, on ResNet-50, ResNet-101 and ResNet200.</td></tr></tbody></table>
<p>
We also demonstrate analytically that the gradient of our loss function encourages learning from hard positives and hard negatives. The gradient contributions from hard positives/negatives are large while those for easy positives/negatives are small. This implicit property allows the contrastive loss to sidestep the need for explicit hard mining, which is a delicate but critical part of many losses, such as triplet loss. See the supplementary material of <a href="https://arxiv.org/abs/2004.11362">our paper</a> for a full derivation.
</p>
<p>
SupCon is also more robust to natural corruptions, such as noise, blur and JPEG compression. The <a href="https://arxiv.org/abs/1903.12261">mean Corruption Error</a> (mCE) measures the average degradation in performance compared to the benchmark <a href="https://github.com/hendrycks/robustness">ImageNet-C</a> dataset. The SupCon models have lower mCE values across different corruptions compared to cross-entropy models, showing increased robustness.
</p>
<p>
We show empirically that the SupCon loss is less sensitive than cross-entropy to a range of hyperparameters. Across changes in augmentations, optimizers, and learning rates, we observe significantly lower variance in the output of the contrastive loss. Moreover, applying different batch sizes while holding all other hyperparameters constant results in consistently better top-1 accuracy of SupCon to that of cross-entropy at each batch size.
</p>
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://1.bp.blogspot.com/-KZnJ8P1FABc/YLpXaPwslVI/AAAAAAAAHsc/dinL4h5X7Ici2p7CXGF2UHihm8RJhZwYQCLcBGAsYHQ/s1558/image6.png" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="492" data-original-width="1558" height="202" src="https://1.bp.blogspot.com/-KZnJ8P1FABc/YLpXaPwslVI/AAAAAAAAHsc/dinL4h5X7Ici2p7CXGF2UHihm8RJhZwYQCLcBGAsYHQ/w640-h202/image6.png" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Accuracy of cross-entropy and supervised contrastive loss as a function of hyperparameters and training data size, measured on ImageNet with a ResNet-50 encoder. <strong>Left:</strong> Boxplot showing Top-1 accuracy vs changes in augmentation, optimizer and learning rates. SupCon yields more consistent results across variations in each, which is useful when the best strategies are unknown <em>a priori</em>. <strong>Right:</strong> Top-1 accuracy as a function of batch size shows both losses benefit from larger batch sizes while SupCon has higher Top-1 accuracy, even when trained with small batch sizes.</td></tr></tbody></table>
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://1.bp.blogspot.com/-YZ0t1Cpji9k/YLpXee49WjI/AAAAAAAAHsk/O7P93IFUEN8su4ny9SymjiuNhAyuGoQawCLcBGAsYHQ/s1999/image4.png" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="666" data-original-width="1999" height="214" src="https://1.bp.blogspot.com/-YZ0t1Cpji9k/YLpXee49WjI/AAAAAAAAHsk/O7P93IFUEN8su4ny9SymjiuNhAyuGoQawCLcBGAsYHQ/w640-h214/image4.png" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Accuracy of supervised contrastive loss as a function of training duration and the temperature hyperparameter, measured on ImageNet with a ResNet-50 encoder. <strong>Left: </strong>Top-1 accuracy as a function of SupCon pre-training epochs. <strong>Right: </strong>Top-1 accuracy as a function of temperature during the pre-training stage for SupCon. Temperature is an <a href="https://arxiv.org/pdf/2012.09740.pdf">important hyperparameter</a> in contrastive learning and reducing sensitivity to temperature is desirable.</td></tr></tbody></table>
<p>
<b>Broader Impact and Next Steps</b><br />
This work provides a technical advancement in the field of supervised classification. Supervised contrastive learning can improve both the accuracy and robustness of classifiers with minimal complexity. The classic cross-entropy loss can be seen as a special case of SupCon where the views correspond to the images and the learned embeddings in the final linear layer corresponding to the labels. We note that SupCon benefits from large batch sizes, and being able to train the models on smaller batches is an important topic for future research. 
</p>
<p>
<a href="https://github.com/google-research/google-research/tree/master/supcon">Our Github repository</a> includes Tensorflow code to train the models in the <a href="https://arxiv.org/abs/2004.11362">paper</a>. Our pre-trained models are also <a href="https://tfhub.dev/s?publisher=google&amp;q=supcon">released</a> on TF-Hub. 
</p>
<p>
<b>Acknowledgements</b><br />
<em>The NeurIPS paper was jointly co-authored with Prannay Khosla, Piotr Teterwak, Chen Wang,  Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Special thanks to Jenny Huang for leading the writing process for this blogpost.</em>
</p>
<span itemprop='author' itemscope='itemscope' itemtype='http://schema.org/Person'>
  <meta content='https://plus.google.com/116899029375914044550' itemprop='url'/>
</span>
                        </script> 
          <noscript> <span class="byline-author">Posted by AJ Maschinot, Senior Software Engineer and Jenny Huang, Product Manager, Google Research</span> 
           <p> In recent years, <a href="https://arxiv.org/pdf/2011.00362.pdf">self-supervised representation learning</a>, which is used in a variety of <a href="https://ai.googleblog.com/2020/04/advancing-self-supervised-and-semi.html">image</a> and <a href="https://arxiv.org/abs/1704.06888">video</a> tasks, has significantly advanced due to the application of <a href="https://arxiv.org/abs/2002.05709">contrastive learning</a>. These contrastive learning approaches typically teach a model to pull together the representations of a target image (a.k.a., the “anchor”) and a matching (“positive”) image in embedding space, while also pushing apart the anchor from many non-matching (“negative”) images. Because labels are assumed to be unavailable in self-supervised learning, the positive is often an augmentation of the anchor, and the negatives are chosen to be the other samples from the training minibatch. However, because of this random sampling, false negatives, i.e., negatives generated from samples of the same class as the anchor, can cause a <a href="https://arxiv.org/abs/2011.11765">degradation</a> in the representation quality. Furthermore, determining the optimal method to generate positives is still an area of <a href="https://ai.googleblog.com/2020/08/understanding-view-selection-for.html">active research</a>. </p> 
           <p> In contrast to the self-supervised approach, a fully-supervised approach could use labeled data to generate positives from existing same-class examples, providing more variability in pretraining than could typically be achieved by simply augmenting the anchor. However, very little work has been done to successfully apply contrastive learning in the fully-supervised domain. </p> 
           <p> In “<a href="https://arxiv.org/abs/2004.11362">Supervised Contrastive Learning</a>”, presented at <a href="https://nips.cc/virtual/2020/public/poster_d89a66c7c80a29b1bdbab0f2a1a94af8.html">NeurIPS 2020</a>, we propose a novel loss function, called SupCon, that bridges the gap between self-supervised learning and fully supervised learning and enables contrastive learning to be applied in the supervised setting. Leveraging labeled data, SupCon encourages normalized embeddings from the <em>same class</em> to be pulled closer together, while embeddings from <em>different classes</em> are pushed apart. This simplifies the process of positive selection, while avoiding potential false negatives. Because it accommodates multiple positives per anchor, this approach results in an improved selection of positive examples that are more varied, while still containing semantically relevant information. SupCon also allows label information to play an active role in representation learning rather than restricting it to be used only in downstream training, as is the case for conventional contrastive learning. To the best of our knowledge, this is the first contrastive loss to consistently perform better on large-scale image classification problems than the common approach of using <a href="https://en.wikipedia.org/wiki/Cross_entropy">cross-entropy loss</a> to train the model directly. Importantly, SupCon is straightforward to implement and stable to train, provides consistent improvement to top-1 accuracy for a number of datasets and architectures (including <a href="https://ai.googleblog.com/2017/08/transformer-novel-neural-network.html">Transformer</a> architectures), and is robust to <a href="https://arxiv.org/abs/1903.12261">image corruptions</a> and hyperparameter variations. </p> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td style="text-align: center;"><a href="https://1.bp.blogspot.com/-_fMby-ad2AA/YLpXBtlI7fI/AAAAAAAAHsI/7TE6W0F5yNAhGEZFwPSvIJUM51A0ETs5wCLcBGAsYHQ/s1999/image5.png" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="922" data-original-width="1999" height="296" src="https://1.bp.blogspot.com/-_fMby-ad2AA/YLpXBtlI7fI/AAAAAAAAHsI/7TE6W0F5yNAhGEZFwPSvIJUM51A0ETs5wCLcBGAsYHQ/w640-h296/image5.png" width="640"></a></td>
             </tr>
             <tr>
              <td class="tr-caption" style="text-align: center;">Self-supervised (<b>left</b>) vs supervised (<b>right</b>) contrastive losses: The self-supervised contrastive loss contrasts a single positive for each anchor (i.e., an augmented version of the same image) against a set of negatives consisting of the entire remainder of the minibatch. The supervised contrastive loss considered in this paper, however, contrasts the set of all samples from the same class as positives against the negatives from the remainder of the batch.</td>
             </tr>
            </tbody>
           </table> 
           <p> <b>The Supervised Contrastive Learning Framework</b><br> SupCon can be seen as a generalization of both the <a href="https://arxiv.org/abs/2002.05709">SimCLR</a> and <a href="https://papers.nips.cc/paper/2016/file/6b180037abbebea991d8b1232f8a8ca9-Paper.pdf">N-pair</a> losses — the former uses positives generated from the same sample as that of the anchor, and the latter uses positives generated from different samples by exploiting known class labels. The use of many positives and many negatives for each anchor allows SupCon to achieve state-of-the-art performance without the need for <a href="https://arxiv.org/abs/2010.04592">hard negative mining</a> (i.e., searching for negatives similar to the anchor), which can be difficult to tune properly. </p> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td style="text-align: center;"><a href="https://1.bp.blogspot.com/-yfy-d_jamHI/YLpXGzdVOpI/AAAAAAAAHsM/gMN-Us-Rg7sZsycXO7UyF7UTK9JyYTAOgCLcBGAsYHQ/s1680/image1.png" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="1124" data-original-width="1680" src="https://1.bp.blogspot.com/-yfy-d_jamHI/YLpXGzdVOpI/AAAAAAAAHsM/gMN-Us-Rg7sZsycXO7UyF7UTK9JyYTAOgCLcBGAsYHQ/s320/image1.png" width="320"></a></td>
             </tr>
             <tr>
              <td class="tr-caption" style="text-align: center;">SupCon subsumes multiple losses from the literature and is a generalization of the SimCLR and N-Pair losses.</td>
             </tr>
            </tbody>
           </table> 
           <p> This method is structurally similar to those used in self-supervised contrastive learning, with modifications for supervised classification. Given an input batch of data, we first apply data augmentation twice to obtain two copies, or “views,” of each sample in the batch (though one could create and use any number of augmented views). Both copies are forward propagated through an encoder network, and the resulting embedding is then <a href="https://mathworld.wolfram.com/L2-Norm.html">L2-normalized</a>. Following standard practice, the representation is further propagated through an <a href="https://arxiv.org/abs/2002.05709">optional projection network</a> to help identify meaningful features. The supervised contrastive loss is computed on the normalized outputs of the projection network. Positives for an anchor consist of the representations originating from the same batch instance as the anchor or from other instances with the same label as the anchor; the negatives are then all remaining instances. To measure performance on downstream tasks, we train a linear classifier on top of the frozen representations. </p> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td style="text-align: center;"><a href="https://1.bp.blogspot.com/-dqz--25E0_k/YLpXMDv-uJI/AAAAAAAAHsQ/Qwir9J1mjWUfchGiiaI1okkX-KVitKVagCLcBGAsYHQ/s1999/image3.png" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="940" data-original-width="1999" height="300" src="https://1.bp.blogspot.com/-dqz--25E0_k/YLpXMDv-uJI/AAAAAAAAHsQ/Qwir9J1mjWUfchGiiaI1okkX-KVitKVagCLcBGAsYHQ/w640-h300/image3.png" width="640"></a></td>
             </tr>
             <tr>
              <td class="tr-caption" style="text-align: center;">Cross-entropy, self-supervised contrastive loss and supervised contrastive loss <strong>Left:</strong> The cross-entropy loss uses labels and a softmax loss to train a classifier. <strong>Middle:</strong> The self-supervised contrastive loss uses a contrastive loss and data augmentations to learn representations. <strong>Right: </strong>The supervised contrastive loss also learns representations using a contrastive loss, but uses label information to sample positives in addition to augmentations of the same image.</td>
             </tr>
            </tbody>
           </table> 
           <p> <b>Key Findings </b><br> SupCon consistently boosts top-1 accuracy compared to cross-entropy, <a href="https://en.wikipedia.org/wiki/Margin_classifier">margin classifiers</a> (with use of labels), and self-supervised contrastive learning techniques on <a href="http://www.cs.toronto.edu/~kriz/cifar.html">CIFAR-10 and CIFAR-100</a> and <a href="https://www.image-net.org/">ImageNet</a> datasets. With SupCon, we achieve excellent top-1 accuracy on the ImageNet dataset with the <a href="https://arxiv.org/abs/1512.03385">ResNet-50 and ResNet-200</a> architectures. On ResNet-200, we achieve a top-1 accuracy of 81.4%, which is a 0.8% improvement over the <a href="https://arxiv.org/abs/1905.00397">state-of-the-art</a> cross-entropy loss using the same architecture (which represents a <a href="https://paperswithcode.com/sota/image-classification-on-imagenet">significant advance for ImageNet</a>). We also compared cross-entropy and SupCon on a <a href="https://ai.googleblog.com/2020/12/transformers-for-image-recognition-at.html">Transformer-based ViT-B/16</a> model and found a consistent improvement over cross-entropy (77.8% versus 76% for ImageNet; 92.6% versus 91.6% for CIFAR-10) under the same data augmentation regime (without any higher-resolution fine-tuning). </p> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td style="text-align: center;"><a href="https://1.bp.blogspot.com/-bWernq6iaDU/YLpXRjyIU0I/AAAAAAAAHsU/Q_em95oBV5wK1ymi1cZFXGc87a_0ubZ2ACLcBGAsYHQ/s1484/image2.png" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="918" data-original-width="1484" height="397" src="https://1.bp.blogspot.com/-bWernq6iaDU/YLpXRjyIU0I/AAAAAAAAHsU/Q_em95oBV5wK1ymi1cZFXGc87a_0ubZ2ACLcBGAsYHQ/w640-h397/image2.png" width="640"></a></td>
             </tr>
             <tr>
              <td class="tr-caption" style="text-align: center;">The SupCon loss consistently outperforms cross-entropy with standard data augmentation strategies (<a href="https://arxiv.org/abs/1805.09501">AutoAugment</a>, <a href="https://arxiv.org/abs/1909.13719">RandAugment</a> and <a href="https://arxiv.org/abs/1905.04899">CutMix</a>). We show top-1 accuracy for ImageNet, on ResNet-50, ResNet-101 and ResNet200.</td>
             </tr>
            </tbody>
           </table> 
           <p> We also demonstrate analytically that the gradient of our loss function encourages learning from hard positives and hard negatives. The gradient contributions from hard positives/negatives are large while those for easy positives/negatives are small. This implicit property allows the contrastive loss to sidestep the need for explicit hard mining, which is a delicate but critical part of many losses, such as triplet loss. See the supplementary material of <a href="https://arxiv.org/abs/2004.11362">our paper</a> for a full derivation. </p> 
           <p> SupCon is also more robust to natural corruptions, such as noise, blur and JPEG compression. The <a href="https://arxiv.org/abs/1903.12261">mean Corruption Error</a> (mCE) measures the average degradation in performance compared to the benchmark <a href="https://github.com/hendrycks/robustness">ImageNet-C</a> dataset. The SupCon models have lower mCE values across different corruptions compared to cross-entropy models, showing increased robustness. </p> 
           <p> We show empirically that the SupCon loss is less sensitive than cross-entropy to a range of hyperparameters. Across changes in augmentations, optimizers, and learning rates, we observe significantly lower variance in the output of the contrastive loss. Moreover, applying different batch sizes while holding all other hyperparameters constant results in consistently better top-1 accuracy of SupCon to that of cross-entropy at each batch size. </p> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td style="text-align: center;"><a href="https://1.bp.blogspot.com/-KZnJ8P1FABc/YLpXaPwslVI/AAAAAAAAHsc/dinL4h5X7Ici2p7CXGF2UHihm8RJhZwYQCLcBGAsYHQ/s1558/image6.png" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="492" data-original-width="1558" height="202" src="https://1.bp.blogspot.com/-KZnJ8P1FABc/YLpXaPwslVI/AAAAAAAAHsc/dinL4h5X7Ici2p7CXGF2UHihm8RJhZwYQCLcBGAsYHQ/w640-h202/image6.png" width="640"></a></td>
             </tr>
             <tr>
              <td class="tr-caption" style="text-align: center;">Accuracy of cross-entropy and supervised contrastive loss as a function of hyperparameters and training data size, measured on ImageNet with a ResNet-50 encoder. <strong>Left:</strong> Boxplot showing Top-1 accuracy vs changes in augmentation, optimizer and learning rates. SupCon yields more consistent results across variations in each, which is useful when the best strategies are unknown <em>a priori</em>. <strong>Right:</strong> Top-1 accuracy as a function of batch size shows both losses benefit from larger batch sizes while SupCon has higher Top-1 accuracy, even when trained with small batch sizes.</td>
             </tr>
            </tbody>
           </table> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td style="text-align: center;"><a href="https://1.bp.blogspot.com/-YZ0t1Cpji9k/YLpXee49WjI/AAAAAAAAHsk/O7P93IFUEN8su4ny9SymjiuNhAyuGoQawCLcBGAsYHQ/s1999/image4.png" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="666" data-original-width="1999" height="214" src="https://1.bp.blogspot.com/-YZ0t1Cpji9k/YLpXee49WjI/AAAAAAAAHsk/O7P93IFUEN8su4ny9SymjiuNhAyuGoQawCLcBGAsYHQ/w640-h214/image4.png" width="640"></a></td>
             </tr>
             <tr>
              <td class="tr-caption" style="text-align: center;">Accuracy of supervised contrastive loss as a function of training duration and the temperature hyperparameter, measured on ImageNet with a ResNet-50 encoder. <strong>Left: </strong>Top-1 accuracy as a function of SupCon pre-training epochs. <strong>Right: </strong>Top-1 accuracy as a function of temperature during the pre-training stage for SupCon. Temperature is an <a href="https://arxiv.org/pdf/2012.09740.pdf">important hyperparameter</a> in contrastive learning and reducing sensitivity to temperature is desirable.</td>
             </tr>
            </tbody>
           </table> 
           <p> <b>Broader Impact and Next Steps</b><br> This work provides a technical advancement in the field of supervised classification. Supervised contrastive learning can improve both the accuracy and robustness of classifiers with minimal complexity. The classic cross-entropy loss can be seen as a special case of SupCon where the views correspond to the images and the learned embeddings in the final linear layer corresponding to the labels. We note that SupCon benefits from large batch sizes, and being able to train the models on smaller batches is an important topic for future research. </p> 
           <p> <a href="https://github.com/google-research/google-research/tree/master/supcon">Our Github repository</a> includes Tensorflow code to train the models in the <a href="https://arxiv.org/abs/2004.11362">paper</a>. Our pre-trained models are also <a href="https://tfhub.dev/s?publisher=google&amp;q=supcon">released</a> on TF-Hub. </p> 
           <p> <b>Acknowledgements</b><br> <em>The NeurIPS paper was jointly co-authored with Prannay Khosla, Piotr Teterwak, Chen Wang, Aaron Sarna, Yonglong Tian, Phillip Isola, Aaron Maschinot, Ce Liu, and Dilip Krishnan. Special thanks to Jenny Huang for leading the writing process for this blogpost.</em> </p> <span itemprop="author" itemscope itemtype="http://schema.org/Person"> 
            <meta content="https://plus.google.com/116899029375914044550" itemprop="url"> </span> 
          </noscript> 
         </div> 
        </div> 
        <div class="share"> <span class="twitter-custom social-wrapper" data-href="http://twitter.com/share?text=Google AI Blog:Extending Contrastive Learning to the Supervised Setting&amp;url=http://ai.googleblog.com/2021/06/extending-contrastive-learning-to.html&amp;via=googleai"> <img alt="Share on Twitter" height="24" src="https://www.gstatic.com/images/icons/material/system/2x/post_twitter_black_24dp.png" width="24"> </span> <span class="fb-custom social-wrapper" data-href="https://www.facebook.com/sharer.php?u=http://ai.googleblog.com/2021/06/extending-contrastive-learning-to.html"> <img alt="Share on Facebook" height="24" src="https://www.gstatic.com/images/icons/material/system/2x/post_facebook_black_24dp.png" width="24"> </span> 
        </div> 
       </div> 
       <div class="post" data-id="3872842883218714444" itemscope itemtype="http://schema.org/BlogPosting"> 
        <h2 class="title" itemprop="name"> <a href="http://ai.googleblog.com/2021/06/data-cascades-in-machine-learning.html" itemprop="url" title="Data Cascades in Machine Learning"> Data Cascades in Machine Learning </a> </h2> 
        <div class="post-header"> 
         <div class="published"> <span class="publishdate" itemprop="datePublished"> Friday, June 4, 2021 </span> 
         </div> 
        </div> 
        <div class="post-body"> 
         <div class="post-content" itemprop="articleBody"> 
          <script type="text/template">
                          <span class="byline-author">Nithya Sambasivan, Research Scientist, Google Research</span>

<p>
Data is a <a href="https://ieeexplore.ieee.org/abstract/document/4804817">foundational aspect</a> of machine learning (ML) that can impact performance, fairness, robustness, and scalability of ML systems. Paradoxically, while building ML models is often highly prioritized,  the work related to data itself is often the least prioritized aspect. This data work can require multiple roles (such as data collectors, annotators, and ML developers) and often involves multiple teams (such as database, legal, or licensing teams) to power a data infrastructure, which adds complexity to any data-related project. As such, the field of <a href="https://en.wikipedia.org/wiki/Human%E2%80%93computer_interaction">human-computer interaction</a> (HCI), which is focused on making technology useful and usable for people, can help both to identify potential issues and to assess the impact on models when data-related work is not prioritized.
</p>
<p>
In &#8220;<a href="https://research.google/pubs/pub49953/">&#8216;Everyone wants to do the model work, not the data work&#8217;: Data Cascades in High-Stakes AI</a>&#8221;, published at the 2021 <a href="https://chi2021.acm.org/">ACM CHI Conference</a>, we study and validate downstream effects from data issues that result in technical debt over time (defined as "data cascades"). Specifically, we illustrate the phenomenon of data cascades with the data practices and challenges of ML practitioners across the globe working in important ML domains, such as cancer detection, landslide detection, loan allocation and more &#8212; domains where ML systems have enabled progress, but also where there is opportunity to improve by addressing data cascades. This work is the first that we know of to formalize, measure, and discuss data cascades in ML as applied to real-world projects. We further discuss the opportunity presented by a collective re-imagining of ML data as a high priority, including rewarding ML data work and workers, recognizing the scientific empiricism in ML data research, improving the visibility of data pipelines, and improving data equity around the world.
</p>
<p>
<b>Origins of Data Cascades</b><br/>
We observe that data cascades often originate early in the lifecycle of an ML system, at the stage of data definition and collection. Cascades also tend to be complex and opaque in diagnosis and manifestation, so there are often no clear indicators, tools, or metrics to detect and measure their effects. Because of this, small data-related obstacles can grow into larger and more complex challenges that affect how a model is developed and deployed. Challenges from data cascades include the need to perform costly system-level changes much later in the development process, or the decrease in users&#8217; trust due to model mis-predictions that result from data issues. Nevertheless and encouragingly, we also observe that such data cascades can be avoided through early interventions in ML development.
</p>


<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://1.bp.blogspot.com/-xcHF2HnezK8/YLpLkBLXlcI/AAAAAAAAHsA/-Zvn2iT7KZ4HLnQhcAbDar92AFOV6YlswCLcBGAsYHQ/s1999/image1.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="576" data-original-width="1999" height="184" src="https://1.bp.blogspot.com/-xcHF2HnezK8/YLpLkBLXlcI/AAAAAAAAHsA/-Zvn2iT7KZ4HLnQhcAbDar92AFOV6YlswCLcBGAsYHQ/w640-h184/image1.png" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Different color arrows indicate different types of data cascades, which typically originate upstream, compound over the ML development process, and manifest downstream.</td></tr></tbody></table>

  
<p>
<b>Examples of Data Cascades</b><br/>
One of the most common causes of data cascades is when models that are trained on noise-free datasets are deployed in the often-noisy real world. For example, a common type of data cascade originates from model <em>drifts</em>, which occur when target and independent variables deviate, resulting in less accurate models. Drifts are more common when models closely interact with new digital environments &#8212; including high-stakes domains, such as air quality sensing, ocean sensing, and ultrasound scanning &#8212; because there are no pre-existing and/or curated datasets. Such drifts can lead to more factors that further decrease a model&#8217;s performance (e.g., related to hardware, environmental, and human knowledge). For example, to ensure good model performance, data is often collected in controlled, in-house environments. But in the live systems of new digital environments with resource constraints, it is more common for data to be collected with physical artefacts such as fingerprints, shadows, dust, improper lighting, and pen markings, which can add noise that affects model performance. In other cases, environmental factors such as rain and wind can unexpectedly move image sensors in deployment, which also trigger cascades. As one of the model developers we interviewed reported, even a small drop of oil or water can affect data that could be used to train a cancer prediction model, therefore affecting the model&#8217;s performance. Because drifts are often caused by the noise in real-world environments, they also take the longest &#8212; up to 2-3 years &#8212; to manifest, almost always in production.
</p>
<p>
Another common type of data cascade can occur when ML practitioners are tasked with managing data in domains in which they have limited expertise. For instance, certain kinds of information, such as identifying poaching locations or data collected during underwater exploration, rely on expertise in the biological sciences, social sciences, and community context. However, some developers in our study described having to take a range of data-related actions that surpassed their domain expertise &#8212; e.g., discarding data, correcting values, merging data, or restarting data collection &#8212; leading to data cascades that limited model performance. The practice of relying on technical expertise more than domain expertise (e.g., by engaging with domain experts) is what appeared to set off these cascades. 
</p>
<p>
Two other cascades observed in this paper resulted from conflicting incentives and organizational practices between data collectors, ML developers, and other partners &#8212; for example, one cascade was caused by poor dataset documentation. While work related to data requires careful coordination across multiple teams, this is especially challenging when stakeholders are not aligned on priorities or workflows.
</p>
<p>
<b>How to Address Data Cascades</b><br/>
Addressing data cascades requires a multi-part, systemic approach in ML research and practice: 
</p>
<ol>

<li>Develop and communicate the concept of <em>goodness of the data </em>that an ML system  starts with, similar to how we think about <em>goodness of fit</em> with models. This includes developing standardized metrics and frequently using those metrics to measure data aspects like phenomenological fidelity (how accurately and comprehensively does the data represent the phenomena) and validity (how well the data explains things related to the phenomena captured by the data), similar to how we have developed good metrics to measure model performance, like <a href="https://en.wikipedia.org/wiki/F-score">F1-scores</a>. 

</li><li>Innovate on incentives to recognize work on data, such as welcoming empiricism on data in conference tracks, rewarding dataset maintenance, or rewarding employees for their work on data (collection, labelling, cleaning, or maintenance) in organizations. 

</li><li>Data work often requires coordination across multiple roles and multiple teams, but this is quite limited currently (partly, but not wholly, because of the previously stated factors). Our research points to the value of fostering greater collaboration, transparency, and fairer distribution of benefits between data collectors, domain experts, and ML developers, especially with ML systems that rely on collecting or labelling niche datasets. 

</li><li>Finally, our research across multiple countries indicates that data scarcity is pronounced in lower-income countries, where ML developers face the additional problem of defining and hand-curating new datasets, which makes it difficult to even start developing ML systems. It is important to enable open dataset banks, create data policies, and foster ML literacy of policy makers and civil society to address the current data inequalities globally.
</li>
</ol>
<p>
<b>Conclusion</b><br/>
In this work we both provide empirical evidence and formalize the concept of data cascades in ML systems. We hope to create an awareness of the potential value that could come from incentivising data excellence. We also hope to introduce an under-explored but significant new research agenda for HCI. Our research on data cascades has led to evidence-backed, state-of-the-art guidelines for data collection and evaluation in the revised <a href="https://pair.withgoogle.com/guidebook/">PAIR Guidebook</a>, aimed at ML developers and designers.
</p>
<p>
<b>Acknowledgements</b><br/>
<em>This paper was written in collaboration with Shivani Kapania, Hannah Highfill, Diana Akrong, Praveen Paritosh and Lora Aroyo. We thank our study participants, and Sures Kumar Thoddu Srinivasan, Jose M. Faleiro, Kristen Olson, Biswajeet Malik, Siddhant Agarwal, Manish Gupta, Aneidi Udo-Obong, Divy Thakkar, Di Dang, and Solomon Awosupin.</em>
</p>
<span itemprop='author' itemscope='itemscope' itemtype='http://schema.org/Person'>
  <meta content='https://plus.google.com/116899029375914044550' itemprop='url'/>
</span>
                        </script> 
          <noscript> <span class="byline-author">Nithya Sambasivan, Research Scientist, Google Research</span> 
           <p> Data is a <a href="https://ieeexplore.ieee.org/abstract/document/4804817">foundational aspect</a> of machine learning (ML) that can impact performance, fairness, robustness, and scalability of ML systems. Paradoxically, while building ML models is often highly prioritized, the work related to data itself is often the least prioritized aspect. This data work can require multiple roles (such as data collectors, annotators, and ML developers) and often involves multiple teams (such as database, legal, or licensing teams) to power a data infrastructure, which adds complexity to any data-related project. As such, the field of <a href="https://en.wikipedia.org/wiki/Human%E2%80%93computer_interaction">human-computer interaction</a> (HCI), which is focused on making technology useful and usable for people, can help both to identify potential issues and to assess the impact on models when data-related work is not prioritized. </p> 
           <p> In “<a href="https://research.google/pubs/pub49953/">‘Everyone wants to do the model work, not the data work’: Data Cascades in High-Stakes AI</a>”, published at the 2021 <a href="https://chi2021.acm.org/">ACM CHI Conference</a>, we study and validate downstream effects from data issues that result in technical debt over time (defined as "data cascades"). Specifically, we illustrate the phenomenon of data cascades with the data practices and challenges of ML practitioners across the globe working in important ML domains, such as cancer detection, landslide detection, loan allocation and more — domains where ML systems have enabled progress, but also where there is opportunity to improve by addressing data cascades. This work is the first that we know of to formalize, measure, and discuss data cascades in ML as applied to real-world projects. We further discuss the opportunity presented by a collective re-imagining of ML data as a high priority, including rewarding ML data work and workers, recognizing the scientific empiricism in ML data research, improving the visibility of data pipelines, and improving data equity around the world. </p> 
           <p> <b>Origins of Data Cascades</b><br> We observe that data cascades often originate early in the lifecycle of an ML system, at the stage of data definition and collection. Cascades also tend to be complex and opaque in diagnosis and manifestation, so there are often no clear indicators, tools, or metrics to detect and measure their effects. Because of this, small data-related obstacles can grow into larger and more complex challenges that affect how a model is developed and deployed. Challenges from data cascades include the need to perform costly system-level changes much later in the development process, or the decrease in users’ trust due to model mis-predictions that result from data issues. Nevertheless and encouragingly, we also observe that such data cascades can be avoided through early interventions in ML development. </p> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td style="text-align: center;"><a href="https://1.bp.blogspot.com/-xcHF2HnezK8/YLpLkBLXlcI/AAAAAAAAHsA/-Zvn2iT7KZ4HLnQhcAbDar92AFOV6YlswCLcBGAsYHQ/s1999/image1.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="576" data-original-width="1999" height="184" src="https://1.bp.blogspot.com/-xcHF2HnezK8/YLpLkBLXlcI/AAAAAAAAHsA/-Zvn2iT7KZ4HLnQhcAbDar92AFOV6YlswCLcBGAsYHQ/w640-h184/image1.png" width="640"></a></td>
             </tr>
             <tr>
              <td class="tr-caption" style="text-align: center;">Different color arrows indicate different types of data cascades, which typically originate upstream, compound over the ML development process, and manifest downstream.</td>
             </tr>
            </tbody>
           </table> 
           <p> <b>Examples of Data Cascades</b><br> One of the most common causes of data cascades is when models that are trained on noise-free datasets are deployed in the often-noisy real world. For example, a common type of data cascade originates from model <em>drifts</em>, which occur when target and independent variables deviate, resulting in less accurate models. Drifts are more common when models closely interact with new digital environments — including high-stakes domains, such as air quality sensing, ocean sensing, and ultrasound scanning — because there are no pre-existing and/or curated datasets. Such drifts can lead to more factors that further decrease a model’s performance (e.g., related to hardware, environmental, and human knowledge). For example, to ensure good model performance, data is often collected in controlled, in-house environments. But in the live systems of new digital environments with resource constraints, it is more common for data to be collected with physical artefacts such as fingerprints, shadows, dust, improper lighting, and pen markings, which can add noise that affects model performance. In other cases, environmental factors such as rain and wind can unexpectedly move image sensors in deployment, which also trigger cascades. As one of the model developers we interviewed reported, even a small drop of oil or water can affect data that could be used to train a cancer prediction model, therefore affecting the model’s performance. Because drifts are often caused by the noise in real-world environments, they also take the longest — up to 2-3 years — to manifest, almost always in production. </p> 
           <p> Another common type of data cascade can occur when ML practitioners are tasked with managing data in domains in which they have limited expertise. For instance, certain kinds of information, such as identifying poaching locations or data collected during underwater exploration, rely on expertise in the biological sciences, social sciences, and community context. However, some developers in our study described having to take a range of data-related actions that surpassed their domain expertise — e.g., discarding data, correcting values, merging data, or restarting data collection — leading to data cascades that limited model performance. The practice of relying on technical expertise more than domain expertise (e.g., by engaging with domain experts) is what appeared to set off these cascades. </p> 
           <p> Two other cascades observed in this paper resulted from conflicting incentives and organizational practices between data collectors, ML developers, and other partners — for example, one cascade was caused by poor dataset documentation. While work related to data requires careful coordination across multiple teams, this is especially challenging when stakeholders are not aligned on priorities or workflows. </p> 
           <p> <b>How to Address Data Cascades</b><br> Addressing data cascades requires a multi-part, systemic approach in ML research and practice: </p> 
           <ol> 
            <li>Develop and communicate the concept of <em>goodness of the data </em>that an ML system starts with, similar to how we think about <em>goodness of fit</em> with models. This includes developing standardized metrics and frequently using those metrics to measure data aspects like phenomenological fidelity (how accurately and comprehensively does the data represent the phenomena) and validity (how well the data explains things related to the phenomena captured by the data), similar to how we have developed good metrics to measure model performance, like <a href="https://en.wikipedia.org/wiki/F-score">F1-scores</a>. </li>
            <li>Innovate on incentives to recognize work on data, such as welcoming empiricism on data in conference tracks, rewarding dataset maintenance, or rewarding employees for their work on data (collection, labelling, cleaning, or maintenance) in organizations. </li>
            <li>Data work often requires coordination across multiple roles and multiple teams, but this is quite limited currently (partly, but not wholly, because of the previously stated factors). Our research points to the value of fostering greater collaboration, transparency, and fairer distribution of benefits between data collectors, domain experts, and ML developers, especially with ML systems that rely on collecting or labelling niche datasets. </li>
            <li>Finally, our research across multiple countries indicates that data scarcity is pronounced in lower-income countries, where ML developers face the additional problem of defining and hand-curating new datasets, which makes it difficult to even start developing ML systems. It is important to enable open dataset banks, create data policies, and foster ML literacy of policy makers and civil society to address the current data inequalities globally. </li> 
           </ol> 
           <p> <b>Conclusion</b><br> In this work we both provide empirical evidence and formalize the concept of data cascades in ML systems. We hope to create an awareness of the potential value that could come from incentivising data excellence. We also hope to introduce an under-explored but significant new research agenda for HCI. Our research on data cascades has led to evidence-backed, state-of-the-art guidelines for data collection and evaluation in the revised <a href="https://pair.withgoogle.com/guidebook/">PAIR Guidebook</a>, aimed at ML developers and designers. </p> 
           <p> <b>Acknowledgements</b><br> <em>This paper was written in collaboration with Shivani Kapania, Hannah Highfill, Diana Akrong, Praveen Paritosh and Lora Aroyo. We thank our study participants, and Sures Kumar Thoddu Srinivasan, Jose M. Faleiro, Kristen Olson, Biswajeet Malik, Siddhant Agarwal, Manish Gupta, Aneidi Udo-Obong, Divy Thakkar, Di Dang, and Solomon Awosupin.</em> </p> <span itemprop="author" itemscope itemtype="http://schema.org/Person"> 
            <meta content="https://plus.google.com/116899029375914044550" itemprop="url"> </span> 
          </noscript> 
         </div> 
        </div> 
        <div class="share"> <span class="twitter-custom social-wrapper" data-href="http://twitter.com/share?text=Google AI Blog:Data Cascades in Machine Learning&amp;url=http://ai.googleblog.com/2021/06/data-cascades-in-machine-learning.html&amp;via=googleai"> <img alt="Share on Twitter" height="24" src="https://www.gstatic.com/images/icons/material/system/2x/post_twitter_black_24dp.png" width="24"> </span> <span class="fb-custom social-wrapper" data-href="https://www.facebook.com/sharer.php?u=http://ai.googleblog.com/2021/06/data-cascades-in-machine-learning.html"> <img alt="Share on Facebook" height="24" src="https://www.gstatic.com/images/icons/material/system/2x/post_facebook_black_24dp.png" width="24"> </span> 
        </div> 
       </div> 
       <div class="post" data-id="179707356845109938" itemscope itemtype="http://schema.org/BlogPosting"> 
        <h2 class="title" itemprop="name"> <a href="http://ai.googleblog.com/2021/06/toward-generalized-sim-to-real-transfer.html" itemprop="url" title="Toward Generalized Sim-to-Real Transfer for Robot Learning"> Toward Generalized Sim-to-Real Transfer for Robot Learning </a> </h2> 
        <div class="post-header"> 
         <div class="published"> <span class="publishdate" itemprop="datePublished"> Thursday, June 3, 2021 </span> 
         </div> 
        </div> 
        <div class="post-body"> 
         <div class="post-content" itemprop="articleBody"> 
          <script type="text/template">
                          <span class="byline-author">Daniel Ho, Software Engineer, The Everyday Robot Project and Kanishka Rao, Staff Software Engineer, Robotics at Google</span>

<p>
<a href="https://en.wikipedia.org/wiki/Reinforcement_learning#:~:text=Reinforcement%20learning%20(RL)%20is%20an,supervised%20learning%20and%20unsupervised%20learning.">Reinforcement</a> and <a href="https://arxiv.org/pdf/1811.06711.pdf">imitation learning</a> methods in robotics research can enable autonomous <a href="https://ai.googleblog.com/2021/04/model-based-rl-for-decentralized-multi.html">environmental navigation</a> and efficient <a href="https://ai.googleblog.com/2021/05/learning-to-manipulate-deformable.html">object manipulation</a>, which in turn opens up a breadth of useful real-life applications. <a href="https://ai.googleblog.com/2017/10/closing-simulation-to-reality-gap-for.html">Previous work</a> has demonstrated how robots that learn end-to-end using deep neural networks can reliably and safely interact with the unstructured world around us by comprehending camera observations to take actions and solve tasks. However, while end-to-end learning methods can generalize and scale for complicated robot manipulation tasks, they require hundreds of thousands real world robot training episodes, which can be difficult to obtain. One can attempt to alleviate this constraint by using a simulation of the environment that allows virtual robots to learn more quickly and at scale, but the simulations&#8217; inability to exactly match the real world presents a challenge c
  ommonly referred to as the <a href="https://ai.googleblog.com/2017/10/closing-simulation-to-reality-gap-for.html">sim-to-real gap</a>. One important source of the gap comes from discrepancies between the images rendered in simulation and the real robot camera observations, which then causes the robot to perform poorly in the real world. 
</p>
<p>
To-date, work on bridging this gap has employed a technique called pixel-level domain adaptation, which translates synthetic images to realistic ones at the pixel level. One example of this technique is <a href="https://ai.googleblog.com/2017/10/closing-simulation-to-reality-gap-for.html">GraspGAN</a>, which employs a <a href="https://en.wikipedia.org/wiki/Generative_adversarial_network">generative adversarial network</a> (GAN), a framework that has been very effective at image generation, to model this transformation between simulated and real images given datasets of each domain. These pseudo-real images correct some sim-to-real gap, so policies learned with simulation execute more successfully on real robots. A limitation for their use in sim-to-real transfer, however, is that because GANs translate images at the pixel-level, multi-pixel features or structures that are necessary for robot task learning may be arbitrarily modified or even removed.
</p>
<p>
To address the above limitation, and in collaboration with the <a href="https://x.company/projects/everyday-robots/">Everyday Robot Project</a> at <a href="https://x.company/">X</a>, we introduce two works, <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Rao_RL-CycleGAN_Reinforcement_Learning_Aware_Simulation-to-Real_CVPR_2020_paper.pdf">RL-CycleGAN</a> and <a href="https://arxiv.org/abs/2011.03148">RetinaGAN</a>, that train GANs with robot-specific consistencies &#8212; so that they do not arbitrarily modify visual features that are specifically necessary for robot task learning &#8212; and thus bridge the visual discrepancy between sim and real. We demonstrate how these consistencies preserve features critical to policy learning, eliminating the need for hand-engineered, task-specific tuning, which in turn allows for this sim-to-real methodology to work flexibly across tasks, domains, and learning algorithms. With RL-CycleGAN, we describe our sim-to-real transfer methodology and demonstrate state-of-the-art performance on real world grasping tasks trained with RL. With RetinaGAN, we extend our approach to include imitation learning with a door opening task.
</p>
<p>
<b>RL-CycleGAN</b><br />
In &#8220;<a href="https://arxiv.org/pdf/2006.09001.pdf">RL-CycleGAN: Reinforcement Learning Aware Simulation-To-Real</a>&#8221;, we leverage a variation of <a href="https://junyanz.github.io/CycleGAN/">CycleGAN</a> for sim-to-real adaptation by ensuring consistency of task-relevant features between real and simulated images. CycleGAN encourages preservation of image contents by ensuring an adapted image transformed back to the original domain is identical to the original image, which is called <em>cycle consistency</em>. To further encourage the adapted images to be useful for robotics, the CycleGAN is jointly trained with a <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">reinforcement learning</a> (RL) robot agent that ensures the robot&#8217;s actions are the same given both the original images and those after GAN-adaptation. That is, task-specific features like robot arm or graspable object locations are unaltered, but the GAN may still alter lighting or textural differences between domains that do not affect task-level decisions.
</p>
<p>
<b>Evaluating RL-CycleGAN</b><br />
We evaluated RL-CycleGAN on a robotic <a href="https://arxiv.org/abs/1806.10293">indiscriminate grasping task</a>. Trained on 580,000 real trials and simulations adapted with RL-CycleGAN, the robot grasps objects with 94% success, surpassing the 89% success rate of the prior state-of-the-art sim-to-real method GraspGAN and the 87% mark using real-only data without simulation. With only 28,000 trials, the RL-CycleGAN method reaches 86%, comparable to the previous baselines with 20x the data. Some examples of the RL-CycleGAN output alongside the simulation images are shown below.</p>


<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://1.bp.blogspot.com/-KY56CJIH6bg/YLflfKbYM8I/AAAAAAAAHrk/VCG8g3bqXpY8Dx4p0df0790Nc9n7uxctQCLcBGAsYHQ/s1280/image5.gif" style="margin-left: auto; margin-right: auto; text-align: center;"><img border="0" data-original-height="720" data-original-width="1280" height="360" src="https://1.bp.blogspot.com/-KY56CJIH6bg/YLflfKbYM8I/AAAAAAAAHrk/VCG8g3bqXpY8Dx4p0df0790Nc9n7uxctQCLcBGAsYHQ/w640-h360/image5.gif" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Comparison between simulation images of robot grasping before (<b>left</b>) and after RL-CycleGAN translation (<b>right</b>).</td></tr></tbody></table>


<p>
<b>RetinaGAN</b><br />
While RL-CycleGAN reliably transfers from sim-to-real for the RL domain using task awareness, a natural question arises: can we develop a more flexible sim-to-real transfer technique that applies broadly to different tasks and robot learning techniques?
</p>
<p>
In &#8220;<a href="https://arxiv.org/abs/2011.03148">RetinaGAN: An Object-Aware Approach to Sim-to-Real Transfer</a>&#8221;, presented at <a href="http://www.icra2021.org/">ICRA 2021</a>, we develop such a task-decoupled, algorithm-decoupled GAN approach to sim-to-real transfer by instead focusing on robots&#8217; perception of objects. RetinaGAN enforces strong object-semantic awareness through perception consistency via <a href="https://en.wikipedia.org/wiki/Object_detection">object detection</a> to predict bounding box locations for all objects on all images. In an ideal sim-to-real model, we expect the object detector to predict the same box locations before and after GAN translation, as objects should not change structurally. RetinaGAN is trained toward this ideal by backpropagation, such that there is consistency in perception of objects both when <em>a</em>) simulated images are transformed from simulation to real and then back to simulation and <em>b</em>) when real images are transformed from real to simulation and then back to real. We find this object-based consistency to be more widely applicable than the task-specific consistency required by RL-CycleGAN.
</p>


<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://1.bp.blogspot.com/-2h6q02p44zw/YLfleVLKy_I/AAAAAAAAHrY/LdgeD_X3figI_9sG4IhLJsjVol9GxftrACLcBGAsYHQ/s764/image2.png" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="764" data-original-width="739" height="400" src="https://1.bp.blogspot.com/-2h6q02p44zw/YLfleVLKy_I/AAAAAAAAHrY/LdgeD_X3figI_9sG4IhLJsjVol9GxftrACLcBGAsYHQ/w388-h400/image2.png" width="388" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Diagram of RetinaGAN stages. The simulated image (<b>top left</b>) is transformed by the sim-to-real generator and subsequently by the real-to-sim generator. The real image (<b>bottom left</b>) undergoes the transformation in reverse order. Having separate pipelines that start with the simulated and real images improves the GAN&#8217;s performance.</td></tr></tbody></table>


<p>
<b>Evaluating RetinaGAN on a Real Robot</b><br />
Given the goal of building a more flexible sim-to-real transfer technique, we evaluate RetinaGAN in multiple ways to understand for which tasks and under what conditions it accomplishes sim-to-real transfer.
</p>
<p>
We first apply RetinaGAN to a grasping task. As demonstrated visually below, RetinaGAN emphasizes the translation of realistic object textures, shadows, and lighting, while maintaining the visual quality and saliency of the graspable objects. We couple a pre-trained RetinaGAN model with the distributed reinforcement learning method <a href="https://arxiv.org/abs/1910.02787">Q2-Opt</a> to train a vision-based task model for instance grasping. On real robots, this policy grasps object instances with 80% success when trained on a hundred thousand episodes &#8212; outperforming prior adaptation methods RL-CycleGAN and CycleGAN (both achieving ~68%) and training without domain adaptation (grey bars below: 19% with sim data, 22% with real data, and 54% with mixed data). This gives us confidence that perception consistency is a valuable strategy for sim-to-real transfer. Further, with just 10,000 training episodes (8% of the data), the RL policy with RetinaGAN grasps with 66% success, demonstrating performance of prior methods with significantly less data.
</p>


<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://1.bp.blogspot.com/-CPstEnAHbms/YLfleMt-gPI/AAAAAAAAHrU/lhSTFMMWwNAfWl7JvEHx7_k_AkecqJvWACLcBGAsYHQ/s1201/image1.png" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="678" data-original-width="1201" height="362" src="https://1.bp.blogspot.com/-CPstEnAHbms/YLfleMt-gPI/AAAAAAAAHrU/lhSTFMMWwNAfWl7JvEHx7_k_AkecqJvWACLcBGAsYHQ/w640-h362/image1.png" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Evaluation performance of RL policies on instance grasping, trained with various datasets and sim-to-real methods. Low-Data RetinaGAN uses 8% of the real dataset.</td></tr></tbody></table>


<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://1.bp.blogspot.com/-DTmu0TyTkDA/YLflfPR7zII/AAAAAAAAHrc/WGWtWsfWsNU15GTbYoFxbTlJiaGRAEYfwCLcBGAsYHQ/s600/image4.gif" style="margin-left: auto; margin-right: auto; text-align: center;"><img border="0" data-original-height="300" data-original-width="600" height="320" src="https://1.bp.blogspot.com/-DTmu0TyTkDA/YLflfPR7zII/AAAAAAAAHrc/WGWtWsfWsNU15GTbYoFxbTlJiaGRAEYfwCLcBGAsYHQ/w640-h320/image4.gif" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">The simulated grasping environment (left) is translated to a realistic image (right) using RetinaGAN.</td></tr></tbody></table>


<p>
Next, we pair RetinaGAN with a different learning method, <a href="https://link.springer.com/referenceworkentry/10.1007%2F978-0-387-30164-8_69">behavioral cloning</a>, to open conference room doors given demonstrations by human operators. Using images from both simulated and real demonstrations, we train RetinaGAN to translate the synthetic images to look realistic, bridging the sim-to-real gap. We then train a behavior cloning model to imitate the task-solving actions of the human operators within real and RetinaGAN-adapted sim demonstrations. When evaluating this model by predicting actions to take, the robot enters real conference rooms over 93% of the time, surpassing baselines of 75% and below.
</p>

<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://1.bp.blogspot.com/-pjLpXBYNDIo/YLf_ugJKAPI/AAAAAAAAHr4/Xv1JT8ooY6Qt_OYHN2YkehiYsyuifZ1ngCLcBGAsYHQ/s800/meeting_room_gan_combined_all_linear_3x.gif" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="317" data-original-width="800" height="254" src="https://1.bp.blogspot.com/-pjLpXBYNDIo/YLf_ugJKAPI/AAAAAAAAHr4/Xv1JT8ooY6Qt_OYHN2YkehiYsyuifZ1ngCLcBGAsYHQ/w640-h254/meeting_room_gan_combined_all_linear_3x.gif" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Both of the above images show the same simulation, but RetinaGAN translates simulated door opening images (<b>left</b>) to look more like real robot sensor data (<b>right</b>).</td></tr></tbody></table>

<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://1.bp.blogspot.com/-7oOCC5GC-dw/YLfle0zVJnI/AAAAAAAAHrg/jsswbzFiemYbHrU-aOEKLy2Zd1pe-fKdACLcBGAsYHQ/s1920/image3.gif" style="margin-left: auto; margin-right: auto; text-align: center;"><img border="0" data-original-height="512" data-original-width="1920" height="170" src="https://1.bp.blogspot.com/-7oOCC5GC-dw/YLfle0zVJnI/AAAAAAAAHrg/jsswbzFiemYbHrU-aOEKLy2Zd1pe-fKdACLcBGAsYHQ/w640-h170/image3.gif" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Three examples of the real robot successfully opening conference room doors using the RetinaGAN-trained behavior cloning policy.</td></tr></tbody></table>

<p>
<b>Conclusion</b><br />
This work has demonstrated how additional constraints on GANs may address the visual sim-to-real gap without requiring task-specific tuning; these approaches reach higher real robot success rates with less data collection. RL-CycleGAN translates synthetic images to realistic ones with an RL-consistency loss that automatically preserves task-relevant features. RetinaGAN is an object-aware sim-to-real adaptation technique that transfers robustly across environments and tasks, agnostic to the task learning method. Since RetinaGAN is not trained with any task-specific knowledge, we show how it can be <a href="https://retinagan.github.io/">reused for a novel object pushing task</a>. We hope that work on the sim-to-real gap further generalizes toward solving task-agnostic robotic manipulation in unstructured environments.
</p>
<p>
<b>Acknowledgements</b><br />
<em>Research into RL-CycleGAN was conducted by Kanishka Rao, Chris Harris, Alex Irpan, Sergey Levine, Julian Ibarz, and Mohi Khansari. Research into RetinaGAN was conducted by Daniel Ho, Kanishka Rao, Zhuo Xu, Eric Jang, Mohi Khansari, and Yunfei Bai. We&#8217;d also like to give special thanks to Ivonne Fajardo, Noah Brown, Benjamin Swanson, Christopher Paguyo, Armando Fuentes, and Sphurti More for overseeing the robot operations. We thank Paul Wohlhart, Konstantinos Bousmalis, Daniel Kappler, Alexander Herzog, Anthony Brohan, Yao Lu, Chad Richards, Vincent Vanhoucke, and Mrinal Kalakrishnan, Max Braun and others in the <a href="https://research.google/teams/brain/robotics/">Robotics at Google team</a> and the <a href="https://x.company/projects/everyday-robots/">Everyday Robot Project</a> for valuable discussions and help.</em>
</p>
<span itemprop='author' itemscope='itemscope' itemtype='http://schema.org/Person'>
  <meta content='https://plus.google.com/116899029375914044550' itemprop='url'/>
</span>
                        </script> 
          <noscript> <span class="byline-author">Daniel Ho, Software Engineer, The Everyday Robot Project and Kanishka Rao, Staff Software Engineer, Robotics at Google</span> 
           <p> <a href="https://en.wikipedia.org/wiki/Reinforcement_learning#:~:text=Reinforcement%20learning%20(RL)%20is%20an,supervised%20learning%20and%20unsupervised%20learning.">Reinforcement</a> and <a href="https://arxiv.org/pdf/1811.06711.pdf">imitation learning</a> methods in robotics research can enable autonomous <a href="https://ai.googleblog.com/2021/04/model-based-rl-for-decentralized-multi.html">environmental navigation</a> and efficient <a href="https://ai.googleblog.com/2021/05/learning-to-manipulate-deformable.html">object manipulation</a>, which in turn opens up a breadth of useful real-life applications. <a href="https://ai.googleblog.com/2017/10/closing-simulation-to-reality-gap-for.html">Previous work</a> has demonstrated how robots that learn end-to-end using deep neural networks can reliably and safely interact with the unstructured world around us by comprehending camera observations to take actions and solve tasks. However, while end-to-end learning methods can generalize and scale for complicated robot manipulation tasks, they require hundreds of thousands real world robot training episodes, which can be difficult to obtain. One can attempt to alleviate this constraint by using a simulation of the environment that allows virtual robots to learn more quickly and at scale, but the simulations’ inability to exactly match the real world presents a challenge c ommonly referred to as the <a href="https://ai.googleblog.com/2017/10/closing-simulation-to-reality-gap-for.html">sim-to-real gap</a>. One important source of the gap comes from discrepancies between the images rendered in simulation and the real robot camera observations, which then causes the robot to perform poorly in the real world. </p> 
           <p> To-date, work on bridging this gap has employed a technique called pixel-level domain adaptation, which translates synthetic images to realistic ones at the pixel level. One example of this technique is <a href="https://ai.googleblog.com/2017/10/closing-simulation-to-reality-gap-for.html">GraspGAN</a>, which employs a <a href="https://en.wikipedia.org/wiki/Generative_adversarial_network">generative adversarial network</a> (GAN), a framework that has been very effective at image generation, to model this transformation between simulated and real images given datasets of each domain. These pseudo-real images correct some sim-to-real gap, so policies learned with simulation execute more successfully on real robots. A limitation for their use in sim-to-real transfer, however, is that because GANs translate images at the pixel-level, multi-pixel features or structures that are necessary for robot task learning may be arbitrarily modified or even removed. </p> 
           <p> To address the above limitation, and in collaboration with the <a href="https://x.company/projects/everyday-robots/">Everyday Robot Project</a> at <a href="https://x.company/">X</a>, we introduce two works, <a href="https://openaccess.thecvf.com/content_CVPR_2020/papers/Rao_RL-CycleGAN_Reinforcement_Learning_Aware_Simulation-to-Real_CVPR_2020_paper.pdf">RL-CycleGAN</a> and <a href="https://arxiv.org/abs/2011.03148">RetinaGAN</a>, that train GANs with robot-specific consistencies — so that they do not arbitrarily modify visual features that are specifically necessary for robot task learning — and thus bridge the visual discrepancy between sim and real. We demonstrate how these consistencies preserve features critical to policy learning, eliminating the need for hand-engineered, task-specific tuning, which in turn allows for this sim-to-real methodology to work flexibly across tasks, domains, and learning algorithms. With RL-CycleGAN, we describe our sim-to-real transfer methodology and demonstrate state-of-the-art performance on real world grasping tasks trained with RL. With RetinaGAN, we extend our approach to include imitation learning with a door opening task. </p> 
           <p> <b>RL-CycleGAN</b><br> In “<a href="https://arxiv.org/pdf/2006.09001.pdf">RL-CycleGAN: Reinforcement Learning Aware Simulation-To-Real</a>”, we leverage a variation of <a href="https://junyanz.github.io/CycleGAN/">CycleGAN</a> for sim-to-real adaptation by ensuring consistency of task-relevant features between real and simulated images. CycleGAN encourages preservation of image contents by ensuring an adapted image transformed back to the original domain is identical to the original image, which is called <em>cycle consistency</em>. To further encourage the adapted images to be useful for robotics, the CycleGAN is jointly trained with a <a href="https://en.wikipedia.org/wiki/Reinforcement_learning">reinforcement learning</a> (RL) robot agent that ensures the robot’s actions are the same given both the original images and those after GAN-adaptation. That is, task-specific features like robot arm or graspable object locations are unaltered, but the GAN may still alter lighting or textural differences between domains that do not affect task-level decisions. </p> 
           <p> <b>Evaluating RL-CycleGAN</b><br> We evaluated RL-CycleGAN on a robotic <a href="https://arxiv.org/abs/1806.10293">indiscriminate grasping task</a>. Trained on 580,000 real trials and simulations adapted with RL-CycleGAN, the robot grasps objects with 94% success, surpassing the 89% success rate of the prior state-of-the-art sim-to-real method GraspGAN and the 87% mark using real-only data without simulation. With only 28,000 trials, the RL-CycleGAN method reaches 86%, comparable to the previous baselines with 20x the data. Some examples of the RL-CycleGAN output alongside the simulation images are shown below.</p> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td style="text-align: center;"><a href="https://1.bp.blogspot.com/-KY56CJIH6bg/YLflfKbYM8I/AAAAAAAAHrk/VCG8g3bqXpY8Dx4p0df0790Nc9n7uxctQCLcBGAsYHQ/s1280/image5.gif" style="margin-left: auto; margin-right: auto; text-align: center;"><img border="0" data-original-height="720" data-original-width="1280" height="360" src="https://1.bp.blogspot.com/-KY56CJIH6bg/YLflfKbYM8I/AAAAAAAAHrk/VCG8g3bqXpY8Dx4p0df0790Nc9n7uxctQCLcBGAsYHQ/w640-h360/image5.gif" width="640"></a></td>
             </tr>
             <tr>
              <td class="tr-caption" style="text-align: center;">Comparison between simulation images of robot grasping before (<b>left</b>) and after RL-CycleGAN translation (<b>right</b>).</td>
             </tr>
            </tbody>
           </table> 
           <p> <b>RetinaGAN</b><br> While RL-CycleGAN reliably transfers from sim-to-real for the RL domain using task awareness, a natural question arises: can we develop a more flexible sim-to-real transfer technique that applies broadly to different tasks and robot learning techniques? </p> 
           <p> In “<a href="https://arxiv.org/abs/2011.03148">RetinaGAN: An Object-Aware Approach to Sim-to-Real Transfer</a>”, presented at <a href="http://www.icra2021.org/">ICRA 2021</a>, we develop such a task-decoupled, algorithm-decoupled GAN approach to sim-to-real transfer by instead focusing on robots’ perception of objects. RetinaGAN enforces strong object-semantic awareness through perception consistency via <a href="https://en.wikipedia.org/wiki/Object_detection">object detection</a> to predict bounding box locations for all objects on all images. In an ideal sim-to-real model, we expect the object detector to predict the same box locations before and after GAN translation, as objects should not change structurally. RetinaGAN is trained toward this ideal by backpropagation, such that there is consistency in perception of objects both when <em>a</em>) simulated images are transformed from simulation to real and then back to simulation and <em>b</em>) when real images are transformed from real to simulation and then back to real. We find this object-based consistency to be more widely applicable than the task-specific consistency required by RL-CycleGAN. </p> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td style="text-align: center;"><a href="https://1.bp.blogspot.com/-2h6q02p44zw/YLfleVLKy_I/AAAAAAAAHrY/LdgeD_X3figI_9sG4IhLJsjVol9GxftrACLcBGAsYHQ/s764/image2.png" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="764" data-original-width="739" height="400" src="https://1.bp.blogspot.com/-2h6q02p44zw/YLfleVLKy_I/AAAAAAAAHrY/LdgeD_X3figI_9sG4IhLJsjVol9GxftrACLcBGAsYHQ/w388-h400/image2.png" width="388"></a></td>
             </tr>
             <tr>
              <td class="tr-caption" style="text-align: center;">Diagram of RetinaGAN stages. The simulated image (<b>top left</b>) is transformed by the sim-to-real generator and subsequently by the real-to-sim generator. The real image (<b>bottom left</b>) undergoes the transformation in reverse order. Having separate pipelines that start with the simulated and real images improves the GAN’s performance.</td>
             </tr>
            </tbody>
           </table> 
           <p> <b>Evaluating RetinaGAN on a Real Robot</b><br> Given the goal of building a more flexible sim-to-real transfer technique, we evaluate RetinaGAN in multiple ways to understand for which tasks and under what conditions it accomplishes sim-to-real transfer. </p> 
           <p> We first apply RetinaGAN to a grasping task. As demonstrated visually below, RetinaGAN emphasizes the translation of realistic object textures, shadows, and lighting, while maintaining the visual quality and saliency of the graspable objects. We couple a pre-trained RetinaGAN model with the distributed reinforcement learning method <a href="https://arxiv.org/abs/1910.02787">Q2-Opt</a> to train a vision-based task model for instance grasping. On real robots, this policy grasps object instances with 80% success when trained on a hundred thousand episodes — outperforming prior adaptation methods RL-CycleGAN and CycleGAN (both achieving ~68%) and training without domain adaptation (grey bars below: 19% with sim data, 22% with real data, and 54% with mixed data). This gives us confidence that perception consistency is a valuable strategy for sim-to-real transfer. Further, with just 10,000 training episodes (8% of the data), the RL policy with RetinaGAN grasps with 66% success, demonstrating performance of prior methods with significantly less data. </p> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td style="text-align: center;"><a href="https://1.bp.blogspot.com/-CPstEnAHbms/YLfleMt-gPI/AAAAAAAAHrU/lhSTFMMWwNAfWl7JvEHx7_k_AkecqJvWACLcBGAsYHQ/s1201/image1.png" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="678" data-original-width="1201" height="362" src="https://1.bp.blogspot.com/-CPstEnAHbms/YLfleMt-gPI/AAAAAAAAHrU/lhSTFMMWwNAfWl7JvEHx7_k_AkecqJvWACLcBGAsYHQ/w640-h362/image1.png" width="640"></a></td>
             </tr>
             <tr>
              <td class="tr-caption" style="text-align: center;">Evaluation performance of RL policies on instance grasping, trained with various datasets and sim-to-real methods. Low-Data RetinaGAN uses 8% of the real dataset.</td>
             </tr>
            </tbody>
           </table> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td style="text-align: center;"><a href="https://1.bp.blogspot.com/-DTmu0TyTkDA/YLflfPR7zII/AAAAAAAAHrc/WGWtWsfWsNU15GTbYoFxbTlJiaGRAEYfwCLcBGAsYHQ/s600/image4.gif" style="margin-left: auto; margin-right: auto; text-align: center;"><img border="0" data-original-height="300" data-original-width="600" height="320" src="https://1.bp.blogspot.com/-DTmu0TyTkDA/YLflfPR7zII/AAAAAAAAHrc/WGWtWsfWsNU15GTbYoFxbTlJiaGRAEYfwCLcBGAsYHQ/w640-h320/image4.gif" width="640"></a></td>
             </tr>
             <tr>
              <td class="tr-caption" style="text-align: center;">The simulated grasping environment (left) is translated to a realistic image (right) using RetinaGAN.</td>
             </tr>
            </tbody>
           </table> 
           <p> Next, we pair RetinaGAN with a different learning method, <a href="https://link.springer.com/referenceworkentry/10.1007%2F978-0-387-30164-8_69">behavioral cloning</a>, to open conference room doors given demonstrations by human operators. Using images from both simulated and real demonstrations, we train RetinaGAN to translate the synthetic images to look realistic, bridging the sim-to-real gap. We then train a behavior cloning model to imitate the task-solving actions of the human operators within real and RetinaGAN-adapted sim demonstrations. When evaluating this model by predicting actions to take, the robot enters real conference rooms over 93% of the time, surpassing baselines of 75% and below. </p> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td style="text-align: center;"><a href="https://1.bp.blogspot.com/-pjLpXBYNDIo/YLf_ugJKAPI/AAAAAAAAHr4/Xv1JT8ooY6Qt_OYHN2YkehiYsyuifZ1ngCLcBGAsYHQ/s800/meeting_room_gan_combined_all_linear_3x.gif" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="317" data-original-width="800" height="254" src="https://1.bp.blogspot.com/-pjLpXBYNDIo/YLf_ugJKAPI/AAAAAAAAHr4/Xv1JT8ooY6Qt_OYHN2YkehiYsyuifZ1ngCLcBGAsYHQ/w640-h254/meeting_room_gan_combined_all_linear_3x.gif" width="640"></a></td>
             </tr>
             <tr>
              <td class="tr-caption" style="text-align: center;">Both of the above images show the same simulation, but RetinaGAN translates simulated door opening images (<b>left</b>) to look more like real robot sensor data (<b>right</b>).</td>
             </tr>
            </tbody>
           </table> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td style="text-align: center;"><a href="https://1.bp.blogspot.com/-7oOCC5GC-dw/YLfle0zVJnI/AAAAAAAAHrg/jsswbzFiemYbHrU-aOEKLy2Zd1pe-fKdACLcBGAsYHQ/s1920/image3.gif" style="margin-left: auto; margin-right: auto; text-align: center;"><img border="0" data-original-height="512" data-original-width="1920" height="170" src="https://1.bp.blogspot.com/-7oOCC5GC-dw/YLfle0zVJnI/AAAAAAAAHrg/jsswbzFiemYbHrU-aOEKLy2Zd1pe-fKdACLcBGAsYHQ/w640-h170/image3.gif" width="640"></a></td>
             </tr>
             <tr>
              <td class="tr-caption" style="text-align: center;">Three examples of the real robot successfully opening conference room doors using the RetinaGAN-trained behavior cloning policy.</td>
             </tr>
            </tbody>
           </table> 
           <p> <b>Conclusion</b><br> This work has demonstrated how additional constraints on GANs may address the visual sim-to-real gap without requiring task-specific tuning; these approaches reach higher real robot success rates with less data collection. RL-CycleGAN translates synthetic images to realistic ones with an RL-consistency loss that automatically preserves task-relevant features. RetinaGAN is an object-aware sim-to-real adaptation technique that transfers robustly across environments and tasks, agnostic to the task learning method. Since RetinaGAN is not trained with any task-specific knowledge, we show how it can be <a href="https://retinagan.github.io/">reused for a novel object pushing task</a>. We hope that work on the sim-to-real gap further generalizes toward solving task-agnostic robotic manipulation in unstructured environments. </p> 
           <p> <b>Acknowledgements</b><br> <em>Research into RL-CycleGAN was conducted by Kanishka Rao, Chris Harris, Alex Irpan, Sergey Levine, Julian Ibarz, and Mohi Khansari. Research into RetinaGAN was conducted by Daniel Ho, Kanishka Rao, Zhuo Xu, Eric Jang, Mohi Khansari, and Yunfei Bai. We’d also like to give special thanks to Ivonne Fajardo, Noah Brown, Benjamin Swanson, Christopher Paguyo, Armando Fuentes, and Sphurti More for overseeing the robot operations. We thank Paul Wohlhart, Konstantinos Bousmalis, Daniel Kappler, Alexander Herzog, Anthony Brohan, Yao Lu, Chad Richards, Vincent Vanhoucke, and Mrinal Kalakrishnan, Max Braun and others in the <a href="https://research.google/teams/brain/robotics/">Robotics at Google team</a> and the <a href="https://x.company/projects/everyday-robots/">Everyday Robot Project</a> for valuable discussions and help.</em> </p> <span itemprop="author" itemscope itemtype="http://schema.org/Person"> 
            <meta content="https://plus.google.com/116899029375914044550" itemprop="url"> </span> 
          </noscript> 
         </div> 
        </div> 
        <div class="share"> <span class="twitter-custom social-wrapper" data-href="http://twitter.com/share?text=Google AI Blog:Toward Generalized Sim-to-Real Transfer for Robot Learning&amp;url=http://ai.googleblog.com/2021/06/toward-generalized-sim-to-real-transfer.html&amp;via=googleai"> <img alt="Share on Twitter" height="24" src="https://www.gstatic.com/images/icons/material/system/2x/post_twitter_black_24dp.png" width="24"> </span> <span class="fb-custom social-wrapper" data-href="https://www.facebook.com/sharer.php?u=http://ai.googleblog.com/2021/06/toward-generalized-sim-to-real-transfer.html"> <img alt="Share on Facebook" height="24" src="https://www.gstatic.com/images/icons/material/system/2x/post_facebook_black_24dp.png" width="24"> </span> 
        </div> 
       </div> 
       <div class="post" data-id="224803688131831951" itemscope itemtype="http://schema.org/BlogPosting"> 
        <h2 class="title" itemprop="name"> <a href="http://ai.googleblog.com/2021/06/a-browsable-petascale-reconstruction-of.html" itemprop="url" title="A Browsable Petascale Reconstruction of the Human Cortex"> A Browsable Petascale Reconstruction of the Human Cortex </a> </h2> 
        <div class="post-header"> 
         <div class="published"> <span class="publishdate" itemprop="datePublished"> Tuesday, June 1, 2021 </span> 
         </div> 
        </div> 
        <div class="post-body"> 
         <div class="post-content" itemprop="articleBody"> 
          <script type="text/template">
                          <span class="byline-author">Posted by Tim Blakely, Software Engineer and Micha? Januszewski, Research Scientist, Connectomics at Google</span>

<p>
In January 2020 we <a href="https://ai.googleblog.com/2020/01/releasing-drosophila-hemibrain.html">released</a> the fly &#8220;hemibrain&#8221; <a href="https://elifesciences.org/articles/57443">connectome</a> &#8212; an online database providing the morphological structure and synaptic connectivity of roughly half of the brain of a fruit fly (<em>Drosophila melanogaster</em>). This database and its <a href="https://hemibrain-dot-neuroglancer-demo.appspot.com/#!%7B%22dimensions%22:%7B%22x%22:%5B8e-9%2C%22m%22%5D%2C%22y%22:%5B8e-9%2C%22m%22%5D%2C%22z%22:%5B8e-9%2C%22m%22%5D%7D%2C%22position%22:%5B17114%2C20543%2C18610%5D%2C%22crossSectionScale%22:54.23751620061224%2C%22crossSectionDepth%22:-37.62185354999912%2C%22projectionScale%22:64770.91726975332%2C%22layers%22:%5B%7B%22type%22:%22image%22%2C%22source%22:%22precomputed://gs://neuroglancer-janelia-flyem-hemibrain/emdata/clahe_yz/jpeg%22%2C%22tab%22:%22source%22%2C%22name%22:%22emdata%22%7D%2C%7B%22type%22:%22segmentation%22%2C%22source%22:%22precomputed://gs://neuroglancer-janelia-flyem-hemibrain/v1.0/segmentation%22%2C%22tab%22:%22segments%22%2C%22name%22:%22segmentation%22%7D%2C%7B%22type%22:%22segmentation%22%2C%22source%22:%7B%22url%22:%22precomputed://gs://neuroglancer-janelia-flyem-hemibrain/v1.0/rois%22%2C%22subsources%22:%7B%22default%22:true%2C%22properties%22:true%2C%22mesh%22:true%7D%2C%22enableDefaultSubsources%22:false%7D%2C%22pick%22:false%2C%22tab%22:%22segments%22%2C%22selectedAlpha%22:0%2C%22saturation%22:0%2C%22objectAlpha%22:0.8%2C%22ignoreNullVisibleSet%22:false%2C%22meshSilhouetteRendering%22:3%2C%22colorSeed%22:2685294016%2C%22name%22:%22roi%22%7D%2C%7B%22type%22:%22annotation%22%2C%22source%22:%22precomputed://gs://neuroglancer-janelia-flyem-hemibrain/v1.0/synapses%22%2C%22tab%22:%22rendering%22%2C%22ignoreNullSegmentFilter%22:false%2C%22shader%22:%22#uicontrol%20vec3%20preColor%20color%28default=%5C%22red%5C%22%29%5Cn#uicontrol%20vec3%20postColor%20color%28default=%5C%22blue%5C%22%29%5Cn#uicontrol%20float%20preConfidence%20slider%28min=0%2C%20max=1%2C%20default=0%29%5Cn#uicontrol%20float%20postConfidence%20slider%28min=0%2C%20max=1%2C%20default=0%29%5Cn%5Cnvoid%20main%28%29%20%7B%5Cn%20%20setColor%28defaultColor%28%29%29%3B%5Cn%20%20setEndpointMarkerColor%28%5Cn%20%20%20%20vec4%28preColor%2C%200.5%29%2C%5Cn%20%20%20%20vec4%28postColor%2C%200.5%29%29%3B%5Cn%20%20setEndpointMarkerSize%282.0%2C%202.0%29%3B%5Cn%20%20setLineWidth%282.0%29%3B%5Cn%20%20if%20%28prop_pre_synaptic_confidence%28%29%3C%20preConfidence%20%7C%7C%5Cn%20%20%20%20%20%20prop_post_synaptic_confidence%28%29%3C%20postConfidence%29%20discard%3B%5Cn%7D%5Cn%22%2C%22linkedSegmentationLayer%22:%7B%22pre_synaptic_cell%22:%22segmentation%22%2C%22post_synaptic_cell%22:%22segmentation%22%7D%2C%22filterBySegmentation%22:%5B%22post_synaptic_cell%22%2C%22pre_synaptic_cell%22%5D%2C%22name%22:%22synapse%22%7D%2C%7B%22type%22:%22segmentation%22%2C%22source%22:%22precomputed://gs://neuroglancer-janelia-flyem-hemibrain/mito_20190717.27250582%22%2C%22pick%22:false%2C%22tab%22:%22segments%22%2C%22selectedAlpha%22:0.82%2C%22name%22:%22mito%22%2C%22visible%22:false%7D%2C%7B%22type%22:%22segmentation%22%2C%22source%22:%22precomputed://gs://neuroglancer-janelia-flyem-hemibrain/mask_normalized_round6%22%2C%22pick%22:false%2C%22tab%22:%22segments%22%2C%22selectedAlpha%22:0.53%2C%22segments%22:%5B%222%22%5D%2C%22name%22:%22mask%22%2C%22visible%22:false%7D%5D%2C%22showSlices%22:false%2C%22selectedLayer%22:%7B%22visible%22:true%2C%22layer%22:%22segmentation%22%7D%2C%22layout%22:%22xy-3d%22%2C%22selection%22:%7B%7D%7D">supporting visualization</a> has <a href="https://www.simonsfoundation.org/2021/02/25/the-connected-connectome/">reframed</a> the way that neural circuits are studied and understood in the fly brain. While the fruit fly brain is small enough to attain a relatively complete map using modern mapping techniques, the insights gained are, at best, only partially informative to understanding the most interesting object in neuroscience &#8212; the <em>human</em> brain. 
</p>
<p>
Today, in collaboration with the <a href="https://lichtmanlab.fas.harvard.edu/">Lichtman Laboratory</a> at Harvard University, we are releasing the <a href="https://h01-release.storage.googleapis.com/landing.html">&#8220;H01&#8221; dataset</a>, a 1.4 petabyte rendering of a small sample of human brain tissue, along with a companion paper, &#8220;<a href="https://www.biorxiv.org/content/10.1101/2021.05.29.446289v1">A connectomic study of a petascale fragment of human cerebral cortex</a>.&#8221;  The H01 sample was imaged at 4nm-resolution by <a href="https://onlinelibrary.wiley.com/doi/pdf/10.1111/jmi.12224">serial section electron microscopy</a>, reconstructed and annotated by automated computational techniques, and analyzed for preliminary insights into the structure of the human cortex. The dataset comprises imaging data that covers roughly one cubic millimeter of brain tissue, and includes tens of thousands of reconstructed neurons, millions of neuron fragments, 130 million annotated synapses, 104 proofread cells, and many additional subcellular annotations and structures &#8212; all easily accessible with the <a href="https://h01-release-dot-neuroglancer-demo.appspot.com/#!gs://h01-release/assets/neuroglancer_states/20210601/c3_library.json">Neuroglancer browser interface</a>. H01 is thus far the largest sample of brain tissue imaged and reconstructed in this level of detail, in any species, and the first large-scale study of synaptic connectivity in the human cortex that spans multiple cell types across <a href="https://h01-dot-neuroglancer-demo.appspot.com/#!gs://h01-release/assets/neuroglancer_states/20210601/cortical_layers.json">all layers of the cortex</a>. The primary goals of this project are to produce a novel resource for studying the human brain and to improve and scale the underlying connectomics technologies.
</p>
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://1.bp.blogspot.com/-LHNSzpDQsNg/YLZeqOiXedI/AAAAAAAAHps/96StiGoAdbIAghujEEnd9zTkimdigv9UACLcBGAsYHQ/s1600/image5.png" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="577" data-original-width="1600" height="230" src="https://1.bp.blogspot.com/-LHNSzpDQsNg/YLZeqOiXedI/AAAAAAAAHps/96StiGoAdbIAghujEEnd9zTkimdigv9UACLcBGAsYHQ/w640-h230/image5.png" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Petabyte connectomic reconstruction of a volume of human neocortex. <strong>Left:</strong> Small subvolume of the dataset. <strong>Right:</strong> A subgraph of 5000 neurons and excitatory (green) and inhibitory (red) connections in the dataset. The full graph (connectome) would be far too dense to visualize.</td></tr></tbody></table>
<p>
<b>What is the Human Cortex?</b><br />
The <a href="https://en.wikipedia.org/wiki/Cerebral_cortex">cerebral cortex</a> is the thin surface layer of the brain found in vertebrate animals that has evolved most recently, showing the greatest variation in size among different mammals (it is especially large in humans). Each part of the cerebral cortex is six layered (e.g., <a href="https://h01-dot-neuroglancer-demo.appspot.com/#!gs://h01-release/assets/neuroglancer_states/20210601/l2.json">L2</a>), with <a href="https://h01-release.storage.googleapis.com/explore.html">different kinds of nerve cells</a> (e.g., <a href="https://h01-dot-neuroglancer-demo.appspot.com/#!gs://h01-release/assets/neuroglancer_states/20210601/spiny_stellate.json">spiny stellate</a>) in each layer. The cerebral cortex plays a crucial role in most higher level cognitive functions, such as thinking, memory, planning, perception, language, and attention. Although there has been some progress in understanding the macroscopic organization of this very complicated tissue, its organization at the level of individual nerve cells and their interconnecting synapses is largely unknown. 
</p>
<p>
<b>Human Brain Connectomics: From Surgical Biopsy to a 3D Database</b><br />
Mapping the structure of the brain at the resolution of individual synapses requires high-resolution microscopy techniques that can image biochemically stabilized (<a href="https://en.wikipedia.org/wiki/Fixation_(histology)">fixed</a>) tissue. We collaborated with brain surgeons at <a href="https://www.massgeneral.org/pathology/research/frosch-lab">Massachusetts General Hospital</a> in Boston (MGH) who sometimes remove pieces of normal human cerebral cortex when performing a surgery to cure <a href="https://en.wikipedia.org/wiki/Epilepsy">epilepsy</a> in order to gain access to a site in the deeper brain where an epileptic seizure is being initiated. Patients anonymously donated this tissue, which is normally discarded, to our colleagues in the Lichtman lab. The Harvard researchers cut the tissue into ~5300 individual 30 nanometer sections using an <a href="https://www.wormatlas.org/EMmethods/ATUM.htm">automated tape collecting ultra-microtome</a>, mounted those sections onto silicon wafers, and then imaged the brain tissue at 4 nm resolution in a customized <a href="https://www.zeiss.com/semiconductor-manufacturing-technology/products-solutions/process-control-solutions/multisem-multi-beam-sem.html">61-beam parallelized scanning electron microscope</a> for rapid image acquisition. 
</p>
<p>
Imaging the ~5300 physical sections produced 225 million individual 2D images. Our team then computationally stitched and aligned this data to produce a <em>single</em> 3D volume. While the quality of the data was generally excellent, these alignment pipelines had to robustly handle a number of challenges, including imaging artifacts, missing sections, variation in microscope parameters, and physical stretching and compression of the tissue. Once aligned, a multiscale <a href="https://ai.googleblog.com/2018/07/improving-connectomics-by-order-of.html">flood-filling network</a> pipeline was applied (using thousands of Google Cloud <a href="https://cloud.google.com/tpu">TPUs</a>) to produce a 3D segmentation of each individual cell in the tissue. Additional machine learning pipelines were applied to identify and characterize 130 million <a href="https://h01-dot-neuroglancer-demo.appspot.com/#!gs://h01-release/assets/neuroglancer_states/20210601/synapse_annotations.json">synapses</a>, classify each 3D fragment into various &#8220;subcompartments&#8221; (e.g., <a href="https://h01-dot-neuroglancer-demo.appspot.com/#!gs://h01-release/assets/neuroglancer_states/20210601/axon.json">axon</a>, <a href="https://h01-dot-neuroglancer-demo.appspot.com/#!gs://h01-release/assets/neuroglancer_states/20210601/dendrite.json">dendrite</a>, or <a href="https://h01-dot-neuroglancer-demo.appspot.com/#!gs://h01-release/assets/neuroglancer_states/20210601/soma.json">cell body</a>), and identify other structures of interest such as <a href="https://h01-dot-neuroglancer-demo.appspot.com/#!gs://h01-release/assets/neuroglancer_states/20210601/myelin.json">myelin</a> and <a href="https://h01-dot-neuroglancer-demo.appspot.com/#!gs://h01-release/assets/neuroglancer_states/20210601/cilia.json">cilia</a>. Automated reconstruction results were imperfect, so manual efforts were used to &#8220;proofread&#8221; roughly <a href="https://h01-dot-neuroglancer-demo.appspot.com/#!gs://h01-release/assets/neuroglancer_states/20210601/proofread_104.json">one hundred cells</a> in the data. Over time, we expect to add additional cells to this verified set through additional manual efforts and further advances in automation. 
</p>
<div class="separator" style="clear: both; text-align: center;"><iframe allowfullscreen="" class="BLOG_video_class" height="360" src="https://www.youtube.com/embed/bvlSV_6wKO4" width="640" youtube-src-id="bvlSV_6wKO4"></iframe></div>
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td class="tr-caption" style="text-align: center;">The H01 volume: roughly one cubic millimeter of human brain tissue captured in 1.4 petabytes of images.</td></tr></tbody></table>
<p>
The imaging data, reconstruction results, and annotations are viewable through an interactive web-based 3D visualization interface, called <a href="https://h01-release-dot-neuroglancer-demo.appspot.com/#!gs://h01-release/assets/neuroglancer_states/20210601/c3_library.json">Neuroglancer</a>, that was <a href="https://ai.googleblog.com/2019/08/an-interactive-automated-3d.html">originally developed</a> to visualize the fruit fly brain. Neuroglancer is available as <a href="https://github.com/google/neuroglancer">open-source software</a>, and widely used in the broader connectomics community. Several new features were introduced to support analysis of the H01 dataset, in particular support for searching for specific neurons in the dataset based on their type or other properties. 
</p>
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; width: 100%;">
 <colgroup>
   <col span="1" style="width: 30%;"></col>
   <col span="1" style="width: 5%;"></col>
   <col span="1" style="width: 30%;"></col>
   <col span="1" style="width: 5%;"></col>
   <col span="1" style="width: 30%;"></col>
 </colgroup>
 <tbody>
    <tr>
    <td>
<a href="https://h01-dot-neuroglancer-demo.appspot.com/#!gs://h01-release/assets/neuroglancer_states/20210601/cortical_layers.json" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="976" data-original-width="1880" src="https://1.bp.blogspot.com/-GoOFTeVcOsc/YLZhNRZmRdI/AAAAAAAAHp0/RYOhSneEzBk7VoASsrFPNja08PR4WgohgCLcBGAsYHQ/s320/image4.png" width="100%" />
   </a></td>
      <td>&nbsp;</td>
   <td>
<a href="https://h01-dot-neuroglancer-demo.appspot.com/#!gs://h01-release/assets/neuroglancer_states/20210601/l2.json" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="972" data-original-width="1874" src="https://1.bp.blogspot.com/-QZyDDDlFpLc/YLZhUEb-GBI/AAAAAAAAHp4/IjU5arqo_LE8A5XGuepRPI6m7IesDZaCQCLcBGAsYHQ/s320/image8.png" width="100%" /></a>
   </td>
      <td>&nbsp;</td>
   <td>
<a href="https://h01-dot-neuroglancer-demo.appspot.com/#!gs://h01-release/assets/neuroglancer_states/20210601/excitatory_inhibitory.json" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="928" data-original-width="1874" src="https://1.bp.blogspot.com/-A4RdiUNjbrE/YLZhUICikuI/AAAAAAAAHp8/T3QsNma-c-8eSI33bj1nMERCu5s7UFNUACLcBGAsYHQ/s320/image6.png" width="100%" /></a>
   </td>
  </tr>
  <tr>
    <td><em style="font-size: small; text-align: center;">The volume spans all six cortical layers</em></td><td>&nbsp;</td><td><em style="font-size: small; text-align: center;">Highlighting Layer 2 interneurons</em></td><td>&nbsp;</td><td><em style="font-size: small; text-align: center;">Excitatory and inhibitory incoming synapses</em></td></tr></tbody></table>
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; width: 100%;">
 <colgroup>
   <col span="1" style="width: 30%;"></col>
   <col span="1" style="width: 5%;"></col>
   <col span="1" style="width: 30%;"></col>
   <col span="1" style="width: 5%;"></col>
   <col span="1" style="width: 30%;"></col>
 </colgroup>
 <tbody>
  <tr>
   <td>
<a href="https://h01-dot-neuroglancer-demo.appspot.com/#!gs://h01-release/assets/neuroglancer_states/20210601/subcompartments_render.json" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="931" data-original-width="1880" src="https://1.bp.blogspot.com/-2WSdXDQuNAo/YLZiJe2FA2I/AAAAAAAAHqQ/iY4Nx_F5cM4l6LLGWqs_y-8gH_jMgUm5QCLcBGAsYHQ/s320/image2.png" width="100%" />
   </a></td>
    <td>&nbsp;</td>
   <td>
<a href="https://h01-dot-neuroglancer-demo.appspot.com/#!gs://h01-release/assets/neuroglancer_states/20210601/chandelier.json" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="931" data-original-width="1880" src="https://1.bp.blogspot.com/-XhCJyXYFhhw/YLZiJRfaltI/AAAAAAAAHqI/Sr20T6TutiAXGPGd-r9cuLFJiZJSCj3WgCLcBGAsYHQ/s320/image11.png" width="100%" /></a>
   </td>
    <td>&nbsp;</td>
   <td>
<a href="https://h01-dot-neuroglancer-demo.appspot.com/#!gs://h01-release/assets/neuroglancer_states/20210601/blood_vessels.json" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="928" data-original-width="1876" src="https://1.bp.blogspot.com/-j9VHIgHLG5o/YLZiJdRWlpI/AAAAAAAAHqM/-yyXQ8BWuH4UpBgQL-UPcWQq8zQo1K-iQCLcBGAsYHQ/s320/image1.png" width="100%" /></a>  
   </td>
  </tr>
     <tr>
    <td><em style="font-size: small; text-align: center;">Neuronal subcompartments classified</em></td><td>&nbsp;</td><td><em style="font-size: small; text-align: center;">A Chandelier cell and some of the Pyramidal neurons it inhibits</em></td><td>&nbsp;</td><td><em style="font-size: small; text-align: center;">Blood vessels traced throughout the volume</em></td>
   </tr></tbody></table>
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; width: 100%;">
 <colgroup>
   <col span="1" style="width: 30%;"></col>
   <col span="1" style="width: 5%;"></col>
   <col span="1" style="width: 30%;"></col>
   <col span="1" style="width: 5%;"></col>
   <col span="1" style="width: 30%;"></col>
 </colgroup>
 <tbody>
  <tr>
   <td>
<a href="https://h01-dot-neuroglancer-demo.appspot.com/#!gs://h01-release/assets/neuroglancer_states/20210601/figs/figS19.json" style="margin-left: 1em; margin-right: 1em;"><img alt="" border="0" data-original-height="923" data-original-width="1873" src="https://1.bp.blogspot.com/-GupTziQjsQU/YLZjgZhncMI/AAAAAAAAHq8/P2pvIH8lgnYOqV159TYSF4mTHO820cPqQCLcBGAsYHQ/s320/image9.png" width="100%" /></a>   
   </td>
    <td>&nbsp;</td>
   <td>
<a href="https://h01-dot-neuroglancer-demo.appspot.com/#!gs://h01-release/assets/neuroglancer_states/20210601/axon_whorl.json" style="margin-left: 1em; margin-right: 1em;"><img alt="" border="0" data-original-height="925" data-original-width="1877" src="https://1.bp.blogspot.com/-gOMtm0TBZhg/YLZjgW3BmlI/AAAAAAAAHq4/wKtVSLrEcU8IV8qjHuubb9F7pb4BjOw6QCLcBGAsYHQ/s320/image7.png" width="100%" /></a>
   </td>
    <td>&nbsp;</td>
   <td>
<a href="https://h01-dot-neuroglancer-demo.appspot.com/#!gs://h01-release/assets/neuroglancer_states/20210601/reflexive_junctions.json" style="margin-left: 1em; margin-right: 1em;"><img alt="" border="0" data-original-height="928" data-original-width="1870" src="https://1.bp.blogspot.com/-jNk9BkA_9Ho/YLZjgdEQs9I/AAAAAAAAHq0/6SqnIMBOvu02kgt6cpE5XNDIlrunEZveACLcBGAsYHQ/s320/image10.png" width="100%" /></a>
   </td>
  </tr> 
  <tr>
    <td><em style="font-size: small; text-align: center;">Serial contact between a pair of neurons</em></td><td>&nbsp;</td><td><em style="font-size: small; text-align: center;">An axon with elaborate whorl structures</em></td><td>&nbsp;</td><td><em style="font-size: small; text-align: center;">A neuron with unusual propensity for self-contact (Credit: Rachael Han)</em></td>
  </tr></tbody></table>
  <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td class="tr-caption" style="text-align: center;">The Neuroglancer interface to the H01 volume and annotations. The user can select specific cells on the basis of their layer and type, can view incoming and outgoing synapses for the cell, and much more. (Click on the images above to take you to the Neuroglancer view shown.)</td></tr></tbody></table>
  
<p>
<b>Analysis of the Human Cortex</b><br />
In a <a href="https://www.biorxiv.org/content/10.1101/2021.05.29.446289v1">companion preprint</a>, we show how H01 has already been used to study several interesting aspects of the organization of the human cortex. In particular, new cell types have been discovered, as well as the presence of &#8220;outlier&#8221; axonal inputs, which establish powerful synaptic connections with target dendrites. While these findings are a promising start, the vastness of the H01 dataset will provide a basis for many years of further study by researchers interested in the human cortex.
</p>
<p>
In order to accelerate the analysis of H01, we also provide <a href="https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture">embeddings</a> of the H01 data that were generated by a neural network trained using a variant of the <a href="https://ai.googleblog.com/2020/04/advancing-self-supervised-and-semi.html">SimCLR</a> self-supervised learning technique. <a href="https://h01-release.storage.googleapis.com/embeddings.html">These embeddings</a> provide highly informative representations of local parts of the dataset that can be used to rapidly annotate new structures and develop new ways of clustering and categorizing brain structures according to purely data-driven criteria. We trained these embeddings using Google Cloud <a href="https://cloud.google.com/tpu/docs/training-on-tpu-pods">TPU pods</a> and then performed inference at roughly four billion data locations spread throughout the volume. 
</p>
<p>
<b>Managing Dataset Size with Improved Compression</b><br />
H01 is a petabyte-scale dataset, but is only one-<em>millionth</em> the volume of an entire human brain. Serious technical challenges remain in scaling up synapse-level brain mapping to an <a href="https://www.sciencedirect.com/science/article/abs/pii/S0092867420310011">entire mouse brain</a> (500x bigger than H01), let alone an entire human brain. One of these challenges is data storage: a mouse brain could generate an exabyte worth of data, which is costly to store. To address this, we are today also releasing a paper, &#8220;<a href="https://www.biorxiv.org/content/10.1101/2021.05.29.445828v1">Denoising-based Image Compression for Connectomics</a>&#8221;, that details how a machine learning-based denoising strategy can be used to compress data, such as H01, at least 17-fold (dashed line in the figure below), with negligible loss of accuracy in the automated reconstruction. 
</p><table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://1.bp.blogspot.com/-TpKb0Ycw72Q/YLaHKBDDERI/AAAAAAAAHrM/Om5ioZmrnVYgVKaKRu4d3LkdSn-GrNphwCLcBGAsYHQ/s1999/image5.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="1428" data-original-width="1999" height="286" src="https://1.bp.blogspot.com/-TpKb0Ycw72Q/YLaHKBDDERI/AAAAAAAAHrM/Om5ioZmrnVYgVKaKRu4d3LkdSn-GrNphwCLcBGAsYHQ/w400-h286/image5.png" width="400" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Reconstruction quality of noisy and denoised images as a function of compression rate for <a href="https://en.wikipedia.org/wiki/JPEG_XL">JPEG XL</a> (JXL) and <a href="https://en.wikipedia.org/wiki/AV1#AV1_Image_File_Format_(AVIF)">AV Image Format</a> (AVIF) codecs. Points and lines show the means, and the shaded area covers &#177;1 standard deviation around the mean.</td></tr></tbody></table><p>Random variations in the electron microscopy imaging process lead to image noise that is difficult to compress even in principle, as the noise lacks spatial correlations or other structure that could be described with fewer bytes. Therefore we acquired images of the same piece of tissue in both a &#8220;fast&#8221; acquisition regime (resulting in high amounts of noise) and a &#8220;slow&#8221; acquisition regime (resulting in low amounts of noise) and then trained a neural network to infer &#8220;slow&#8221; scans from &#8220;fast&#8221; scans. Standard image compression codecs were then able to (lossily) compress the &#8220;virtual&#8221; slow scans with fewer artifacts compared to the raw data. We believe this advance has the potential to significantly mitigate the costs associated with future large scale connectomics projects.</p>
<p>
<b>Next Steps</b><br />
But storage is not the only problem. The sheer size of future data sets will require developing new strategies for researchers to organize and access the rich information inherent in connectomic data. These are challenges that will require new modes of interaction between humans and the brain mapping data that will be forthcoming.</p>
<span itemprop='author' itemscope='itemscope' itemtype='http://schema.org/Person'>
  <meta content='https://plus.google.com/116899029375914044550' itemprop='url'/>
</span>
                        </script> 
          <noscript> <span class="byline-author">Posted by Tim Blakely, Software Engineer and Micha? Januszewski, Research Scientist, Connectomics at Google</span> 
           <p> In January 2020 we <a href="https://ai.googleblog.com/2020/01/releasing-drosophila-hemibrain.html">released</a> the fly “hemibrain” <a href="https://elifesciences.org/articles/57443">connectome</a> — an online database providing the morphological structure and synaptic connectivity of roughly half of the brain of a fruit fly (<em>Drosophila melanogaster</em>). This database and its <a href="https://hemibrain-dot-neuroglancer-demo.appspot.com/#!%7B%22dimensions%22:%7B%22x%22:%5B8e-9%2C%22m%22%5D%2C%22y%22:%5B8e-9%2C%22m%22%5D%2C%22z%22:%5B8e-9%2C%22m%22%5D%7D%2C%22position%22:%5B17114%2C20543%2C18610%5D%2C%22crossSectionScale%22:54.23751620061224%2C%22crossSectionDepth%22:-37.62185354999912%2C%22projectionScale%22:64770.91726975332%2C%22layers%22:%5B%7B%22type%22:%22image%22%2C%22source%22:%22precomputed://gs://neuroglancer-janelia-flyem-hemibrain/emdata/clahe_yz/jpeg%22%2C%22tab%22:%22source%22%2C%22name%22:%22emdata%22%7D%2C%7B%22type%22:%22segmentation%22%2C%22source%22:%22precomputed://gs://neuroglancer-janelia-flyem-hemibrain/v1.0/segmentation%22%2C%22tab%22:%22segments%22%2C%22name%22:%22segmentation%22%7D%2C%7B%22type%22:%22segmentation%22%2C%22source%22:%7B%22url%22:%22precomputed://gs://neuroglancer-janelia-flyem-hemibrain/v1.0/rois%22%2C%22subsources%22:%7B%22default%22:true%2C%22properties%22:true%2C%22mesh%22:true%7D%2C%22enableDefaultSubsources%22:false%7D%2C%22pick%22:false%2C%22tab%22:%22segments%22%2C%22selectedAlpha%22:0%2C%22saturation%22:0%2C%22objectAlpha%22:0.8%2C%22ignoreNullVisibleSet%22:false%2C%22meshSilhouetteRendering%22:3%2C%22colorSeed%22:2685294016%2C%22name%22:%22roi%22%7D%2C%7B%22type%22:%22annotation%22%2C%22source%22:%22precomputed://gs://neuroglancer-janelia-flyem-hemibrain/v1.0/synapses%22%2C%22tab%22:%22rendering%22%2C%22ignoreNullSegmentFilter%22:false%2C%22shader%22:%22#uicontrol%20vec3%20preColor%20color%28default=%5C%22red%5C%22%29%5Cn#uicontrol%20vec3%20postColor%20color%28default=%5C%22blue%5C%22%29%5Cn#uicontrol%20float%20preConfidence%20slider%28min=0%2C%20max=1%2C%20default=0%29%5Cn#uicontrol%20float%20postConfidence%20slider%28min=0%2C%20max=1%2C%20default=0%29%5Cn%5Cnvoid%20main%28%29%20%7B%5Cn%20%20setColor%28defaultColor%28%29%29%3B%5Cn%20%20setEndpointMarkerColor%28%5Cn%20%20%20%20vec4%28preColor%2C%200.5%29%2C%5Cn%20%20%20%20vec4%28postColor%2C%200.5%29%29%3B%5Cn%20%20setEndpointMarkerSize%282.0%2C%202.0%29%3B%5Cn%20%20setLineWidth%282.0%29%3B%5Cn%20%20if%20%28prop_pre_synaptic_confidence%28%29%3C%20preConfidence%20%7C%7C%5Cn%20%20%20%20%20%20prop_post_synaptic_confidence%28%29%3C%20postConfidence%29%20discard%3B%5Cn%7D%5Cn%22%2C%22linkedSegmentationLayer%22:%7B%22pre_synaptic_cell%22:%22segmentation%22%2C%22post_synaptic_cell%22:%22segmentation%22%7D%2C%22filterBySegmentation%22:%5B%22post_synaptic_cell%22%2C%22pre_synaptic_cell%22%5D%2C%22name%22:%22synapse%22%7D%2C%7B%22type%22:%22segmentation%22%2C%22source%22:%22precomputed://gs://neuroglancer-janelia-flyem-hemibrain/mito_20190717.27250582%22%2C%22pick%22:false%2C%22tab%22:%22segments%22%2C%22selectedAlpha%22:0.82%2C%22name%22:%22mito%22%2C%22visible%22:false%7D%2C%7B%22type%22:%22segmentation%22%2C%22source%22:%22precomputed://gs://neuroglancer-janelia-flyem-hemibrain/mask_normalized_round6%22%2C%22pick%22:false%2C%22tab%22:%22segments%22%2C%22selectedAlpha%22:0.53%2C%22segments%22:%5B%222%22%5D%2C%22name%22:%22mask%22%2C%22visible%22:false%7D%5D%2C%22showSlices%22:false%2C%22selectedLayer%22:%7B%22visible%22:true%2C%22layer%22:%22segmentation%22%7D%2C%22layout%22:%22xy-3d%22%2C%22selection%22:%7B%7D%7D">supporting visualization</a> has <a href="https://www.simonsfoundation.org/2021/02/25/the-connected-connectome/">reframed</a> the way that neural circuits are studied and understood in the fly brain. While the fruit fly brain is small enough to attain a relatively complete map using modern mapping techniques, the insights gained are, at best, only partially informative to understanding the most interesting object in neuroscience — the <em>human</em> brain. </p> 
           <p> Today, in collaboration with the <a href="https://lichtmanlab.fas.harvard.edu/">Lichtman Laboratory</a> at Harvard University, we are releasing the <a href="https://h01-release.storage.googleapis.com/landing.html">“H01” dataset</a>, a 1.4 petabyte rendering of a small sample of human brain tissue, along with a companion paper, “<a href="https://www.biorxiv.org/content/10.1101/2021.05.29.446289v1">A connectomic study of a petascale fragment of human cerebral cortex</a>.” The H01 sample was imaged at 4nm-resolution by <a href="https://onlinelibrary.wiley.com/doi/pdf/10.1111/jmi.12224">serial section electron microscopy</a>, reconstructed and annotated by automated computational techniques, and analyzed for preliminary insights into the structure of the human cortex. The dataset comprises imaging data that covers roughly one cubic millimeter of brain tissue, and includes tens of thousands of reconstructed neurons, millions of neuron fragments, 130 million annotated synapses, 104 proofread cells, and many additional subcellular annotations and structures — all easily accessible with the <a href="https://h01-release-dot-neuroglancer-demo.appspot.com/#!gs://h01-release/assets/neuroglancer_states/20210601/c3_library.json">Neuroglancer browser interface</a>. H01 is thus far the largest sample of brain tissue imaged and reconstructed in this level of detail, in any species, and the first large-scale study of synaptic connectivity in the human cortex that spans multiple cell types across <a href="https://h01-dot-neuroglancer-demo.appspot.com/#!gs://h01-release/assets/neuroglancer_states/20210601/cortical_layers.json">all layers of the cortex</a>. The primary goals of this project are to produce a novel resource for studying the human brain and to improve and scale the underlying connectomics technologies. </p> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td style="text-align: center;"><a href="https://1.bp.blogspot.com/-LHNSzpDQsNg/YLZeqOiXedI/AAAAAAAAHps/96StiGoAdbIAghujEEnd9zTkimdigv9UACLcBGAsYHQ/s1600/image5.png" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="577" data-original-width="1600" height="230" src="https://1.bp.blogspot.com/-LHNSzpDQsNg/YLZeqOiXedI/AAAAAAAAHps/96StiGoAdbIAghujEEnd9zTkimdigv9UACLcBGAsYHQ/w640-h230/image5.png" width="640"></a></td>
             </tr>
             <tr>
              <td class="tr-caption" style="text-align: center;">Petabyte connectomic reconstruction of a volume of human neocortex. <strong>Left:</strong> Small subvolume of the dataset. <strong>Right:</strong> A subgraph of 5000 neurons and excitatory (green) and inhibitory (red) connections in the dataset. The full graph (connectome) would be far too dense to visualize.</td>
             </tr>
            </tbody>
           </table> 
           <p> <b>What is the Human Cortex?</b><br> The <a href="https://en.wikipedia.org/wiki/Cerebral_cortex">cerebral cortex</a> is the thin surface layer of the brain found in vertebrate animals that has evolved most recently, showing the greatest variation in size among different mammals (it is especially large in humans). Each part of the cerebral cortex is six layered (e.g., <a href="https://h01-dot-neuroglancer-demo.appspot.com/#!gs://h01-release/assets/neuroglancer_states/20210601/l2.json">L2</a>), with <a href="https://h01-release.storage.googleapis.com/explore.html">different kinds of nerve cells</a> (e.g., <a href="https://h01-dot-neuroglancer-demo.appspot.com/#!gs://h01-release/assets/neuroglancer_states/20210601/spiny_stellate.json">spiny stellate</a>) in each layer. The cerebral cortex plays a crucial role in most higher level cognitive functions, such as thinking, memory, planning, perception, language, and attention. Although there has been some progress in understanding the macroscopic organization of this very complicated tissue, its organization at the level of individual nerve cells and their interconnecting synapses is largely unknown. </p> 
           <p> <b>Human Brain Connectomics: From Surgical Biopsy to a 3D Database</b><br> Mapping the structure of the brain at the resolution of individual synapses requires high-resolution microscopy techniques that can image biochemically stabilized (<a href="https://en.wikipedia.org/wiki/Fixation_(histology)">fixed</a>) tissue. We collaborated with brain surgeons at <a href="https://www.massgeneral.org/pathology/research/frosch-lab">Massachusetts General Hospital</a> in Boston (MGH) who sometimes remove pieces of normal human cerebral cortex when performing a surgery to cure <a href="https://en.wikipedia.org/wiki/Epilepsy">epilepsy</a> in order to gain access to a site in the deeper brain where an epileptic seizure is being initiated. Patients anonymously donated this tissue, which is normally discarded, to our colleagues in the Lichtman lab. The Harvard researchers cut the tissue into ~5300 individual 30 nanometer sections using an <a href="https://www.wormatlas.org/EMmethods/ATUM.htm">automated tape collecting ultra-microtome</a>, mounted those sections onto silicon wafers, and then imaged the brain tissue at 4 nm resolution in a customized <a href="https://www.zeiss.com/semiconductor-manufacturing-technology/products-solutions/process-control-solutions/multisem-multi-beam-sem.html">61-beam parallelized scanning electron microscope</a> for rapid image acquisition. </p> 
           <p> Imaging the ~5300 physical sections produced 225 million individual 2D images. Our team then computationally stitched and aligned this data to produce a <em>single</em> 3D volume. While the quality of the data was generally excellent, these alignment pipelines had to robustly handle a number of challenges, including imaging artifacts, missing sections, variation in microscope parameters, and physical stretching and compression of the tissue. Once aligned, a multiscale <a href="https://ai.googleblog.com/2018/07/improving-connectomics-by-order-of.html">flood-filling network</a> pipeline was applied (using thousands of Google Cloud <a href="https://cloud.google.com/tpu">TPUs</a>) to produce a 3D segmentation of each individual cell in the tissue. Additional machine learning pipelines were applied to identify and characterize 130 million <a href="https://h01-dot-neuroglancer-demo.appspot.com/#!gs://h01-release/assets/neuroglancer_states/20210601/synapse_annotations.json">synapses</a>, classify each 3D fragment into various “subcompartments” (e.g., <a href="https://h01-dot-neuroglancer-demo.appspot.com/#!gs://h01-release/assets/neuroglancer_states/20210601/axon.json">axon</a>, <a href="https://h01-dot-neuroglancer-demo.appspot.com/#!gs://h01-release/assets/neuroglancer_states/20210601/dendrite.json">dendrite</a>, or <a href="https://h01-dot-neuroglancer-demo.appspot.com/#!gs://h01-release/assets/neuroglancer_states/20210601/soma.json">cell body</a>), and identify other structures of interest such as <a href="https://h01-dot-neuroglancer-demo.appspot.com/#!gs://h01-release/assets/neuroglancer_states/20210601/myelin.json">myelin</a> and <a href="https://h01-dot-neuroglancer-demo.appspot.com/#!gs://h01-release/assets/neuroglancer_states/20210601/cilia.json">cilia</a>. Automated reconstruction results were imperfect, so manual efforts were used to “proofread” roughly <a href="https://h01-dot-neuroglancer-demo.appspot.com/#!gs://h01-release/assets/neuroglancer_states/20210601/proofread_104.json">one hundred cells</a> in the data. Over time, we expect to add additional cells to this verified set through additional manual efforts and further advances in automation. </p> 
           <div class="separator" style="clear: both; text-align: center;">
            <iframe allowfullscreen class="BLOG_video_class" height="360" src="https://www.youtube.com/embed/bvlSV_6wKO4" width="640" youtube-src-id="bvlSV_6wKO4"></iframe>
           </div> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td class="tr-caption" style="text-align: center;">The H01 volume: roughly one cubic millimeter of human brain tissue captured in 1.4 petabytes of images.</td>
             </tr>
            </tbody>
           </table> 
           <p> The imaging data, reconstruction results, and annotations are viewable through an interactive web-based 3D visualization interface, called <a href="https://h01-release-dot-neuroglancer-demo.appspot.com/#!gs://h01-release/assets/neuroglancer_states/20210601/c3_library.json">Neuroglancer</a>, that was <a href="https://ai.googleblog.com/2019/08/an-interactive-automated-3d.html">originally developed</a> to visualize the fruit fly brain. Neuroglancer is available as <a href="https://github.com/google/neuroglancer">open-source software</a>, and widely used in the broader connectomics community. Several new features were introduced to support analysis of the H01 dataset, in particular support for searching for specific neurons in the dataset based on their type or other properties. </p> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; width: 100%;"> 
            <colgroup> 
             <col span="1" style="width: 30%;">
            </colgroup> 
            <colgroup>
             <col span="1" style="width: 5%;">
            </colgroup> 
            <colgroup>
             <col span="1" style="width: 30%;">
            </colgroup> 
            <colgroup>
             <col span="1" style="width: 5%;">
            </colgroup> 
            <colgroup>
             <col span="1" style="width: 30%;">
            </colgroup>  
            <tbody> 
             <tr> 
              <td> <a href="https://h01-dot-neuroglancer-demo.appspot.com/#!gs://h01-release/assets/neuroglancer_states/20210601/cortical_layers.json" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="976" data-original-width="1880" src="https://1.bp.blogspot.com/-GoOFTeVcOsc/YLZhNRZmRdI/AAAAAAAAHp0/RYOhSneEzBk7VoASsrFPNja08PR4WgohgCLcBGAsYHQ/s320/image4.png" width="100%"> </a></td> 
              <td>&nbsp;</td> 
              <td> <a href="https://h01-dot-neuroglancer-demo.appspot.com/#!gs://h01-release/assets/neuroglancer_states/20210601/l2.json" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="972" data-original-width="1874" src="https://1.bp.blogspot.com/-QZyDDDlFpLc/YLZhUEb-GBI/AAAAAAAAHp4/IjU5arqo_LE8A5XGuepRPI6m7IesDZaCQCLcBGAsYHQ/s320/image8.png" width="100%"></a> </td> 
              <td>&nbsp;</td> 
              <td> <a href="https://h01-dot-neuroglancer-demo.appspot.com/#!gs://h01-release/assets/neuroglancer_states/20210601/excitatory_inhibitory.json" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="928" data-original-width="1874" src="https://1.bp.blogspot.com/-A4RdiUNjbrE/YLZhUICikuI/AAAAAAAAHp8/T3QsNma-c-8eSI33bj1nMERCu5s7UFNUACLcBGAsYHQ/s320/image6.png" width="100%"></a> </td> 
             </tr> 
             <tr> 
              <td><em style="font-size: small; text-align: center;">The volume spans all six cortical layers</em></td>
              <td>&nbsp;</td>
              <td><em style="font-size: small; text-align: center;">Highlighting Layer 2 interneurons</em></td>
              <td>&nbsp;</td>
              <td><em style="font-size: small; text-align: center;">Excitatory and inhibitory incoming synapses</em></td>
             </tr>
            </tbody>
           </table> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; width: 100%;"> 
            <colgroup> 
             <col span="1" style="width: 30%;">
            </colgroup> 
            <colgroup>
             <col span="1" style="width: 5%;">
            </colgroup> 
            <colgroup>
             <col span="1" style="width: 30%;">
            </colgroup> 
            <colgroup>
             <col span="1" style="width: 5%;">
            </colgroup> 
            <colgroup>
             <col span="1" style="width: 30%;">
            </colgroup>  
            <tbody> 
             <tr> 
              <td> <a href="https://h01-dot-neuroglancer-demo.appspot.com/#!gs://h01-release/assets/neuroglancer_states/20210601/subcompartments_render.json" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="931" data-original-width="1880" src="https://1.bp.blogspot.com/-2WSdXDQuNAo/YLZiJe2FA2I/AAAAAAAAHqQ/iY4Nx_F5cM4l6LLGWqs_y-8gH_jMgUm5QCLcBGAsYHQ/s320/image2.png" width="100%"> </a></td> 
              <td>&nbsp;</td> 
              <td> <a href="https://h01-dot-neuroglancer-demo.appspot.com/#!gs://h01-release/assets/neuroglancer_states/20210601/chandelier.json" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="931" data-original-width="1880" src="https://1.bp.blogspot.com/-XhCJyXYFhhw/YLZiJRfaltI/AAAAAAAAHqI/Sr20T6TutiAXGPGd-r9cuLFJiZJSCj3WgCLcBGAsYHQ/s320/image11.png" width="100%"></a> </td> 
              <td>&nbsp;</td> 
              <td> <a href="https://h01-dot-neuroglancer-demo.appspot.com/#!gs://h01-release/assets/neuroglancer_states/20210601/blood_vessels.json" style="margin-left: 1em; margin-right: 1em;"><img border="0" data-original-height="928" data-original-width="1876" src="https://1.bp.blogspot.com/-j9VHIgHLG5o/YLZiJdRWlpI/AAAAAAAAHqM/-yyXQ8BWuH4UpBgQL-UPcWQq8zQo1K-iQCLcBGAsYHQ/s320/image1.png" width="100%"></a> </td> 
             </tr> 
             <tr> 
              <td><em style="font-size: small; text-align: center;">Neuronal subcompartments classified</em></td>
              <td>&nbsp;</td>
              <td><em style="font-size: small; text-align: center;">A Chandelier cell and some of the Pyramidal neurons it inhibits</em></td>
              <td>&nbsp;</td>
              <td><em style="font-size: small; text-align: center;">Blood vessels traced throughout the volume</em></td> 
             </tr>
            </tbody>
           </table> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto; width: 100%;"> 
            <colgroup> 
             <col span="1" style="width: 30%;">
            </colgroup> 
            <colgroup>
             <col span="1" style="width: 5%;">
            </colgroup> 
            <colgroup>
             <col span="1" style="width: 30%;">
            </colgroup> 
            <colgroup>
             <col span="1" style="width: 5%;">
            </colgroup> 
            <colgroup>
             <col span="1" style="width: 30%;">
            </colgroup>  
            <tbody> 
             <tr> 
              <td> <a href="https://h01-dot-neuroglancer-demo.appspot.com/#!gs://h01-release/assets/neuroglancer_states/20210601/figs/figS19.json" style="margin-left: 1em; margin-right: 1em;"><img alt="" border="0" data-original-height="923" data-original-width="1873" src="https://1.bp.blogspot.com/-GupTziQjsQU/YLZjgZhncMI/AAAAAAAAHq8/P2pvIH8lgnYOqV159TYSF4mTHO820cPqQCLcBGAsYHQ/s320/image9.png" width="100%"></a> </td> 
              <td>&nbsp;</td> 
              <td> <a href="https://h01-dot-neuroglancer-demo.appspot.com/#!gs://h01-release/assets/neuroglancer_states/20210601/axon_whorl.json" style="margin-left: 1em; margin-right: 1em;"><img alt="" border="0" data-original-height="925" data-original-width="1877" src="https://1.bp.blogspot.com/-gOMtm0TBZhg/YLZjgW3BmlI/AAAAAAAAHq4/wKtVSLrEcU8IV8qjHuubb9F7pb4BjOw6QCLcBGAsYHQ/s320/image7.png" width="100%"></a> </td> 
              <td>&nbsp;</td> 
              <td> <a href="https://h01-dot-neuroglancer-demo.appspot.com/#!gs://h01-release/assets/neuroglancer_states/20210601/reflexive_junctions.json" style="margin-left: 1em; margin-right: 1em;"><img alt="" border="0" data-original-height="928" data-original-width="1870" src="https://1.bp.blogspot.com/-jNk9BkA_9Ho/YLZjgdEQs9I/AAAAAAAAHq0/6SqnIMBOvu02kgt6cpE5XNDIlrunEZveACLcBGAsYHQ/s320/image10.png" width="100%"></a> </td> 
             </tr> 
             <tr> 
              <td><em style="font-size: small; text-align: center;">Serial contact between a pair of neurons</em></td>
              <td>&nbsp;</td>
              <td><em style="font-size: small; text-align: center;">An axon with elaborate whorl structures</em></td>
              <td>&nbsp;</td>
              <td><em style="font-size: small; text-align: center;">A neuron with unusual propensity for self-contact (Credit: Rachael Han)</em></td> 
             </tr>
            </tbody>
           </table> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td class="tr-caption" style="text-align: center;">The Neuroglancer interface to the H01 volume and annotations. The user can select specific cells on the basis of their layer and type, can view incoming and outgoing synapses for the cell, and much more. (Click on the images above to take you to the Neuroglancer view shown.)</td>
             </tr>
            </tbody>
           </table> 
           <p> <b>Analysis of the Human Cortex</b><br> In a <a href="https://www.biorxiv.org/content/10.1101/2021.05.29.446289v1">companion preprint</a>, we show how H01 has already been used to study several interesting aspects of the organization of the human cortex. In particular, new cell types have been discovered, as well as the presence of “outlier” axonal inputs, which establish powerful synaptic connections with target dendrites. While these findings are a promising start, the vastness of the H01 dataset will provide a basis for many years of further study by researchers interested in the human cortex. </p> 
           <p> In order to accelerate the analysis of H01, we also provide <a href="https://developers.google.com/machine-learning/crash-course/embeddings/video-lecture">embeddings</a> of the H01 data that were generated by a neural network trained using a variant of the <a href="https://ai.googleblog.com/2020/04/advancing-self-supervised-and-semi.html">SimCLR</a> self-supervised learning technique. <a href="https://h01-release.storage.googleapis.com/embeddings.html">These embeddings</a> provide highly informative representations of local parts of the dataset that can be used to rapidly annotate new structures and develop new ways of clustering and categorizing brain structures according to purely data-driven criteria. We trained these embeddings using Google Cloud <a href="https://cloud.google.com/tpu/docs/training-on-tpu-pods">TPU pods</a> and then performed inference at roughly four billion data locations spread throughout the volume. </p> 
           <p> <b>Managing Dataset Size with Improved Compression</b><br> H01 is a petabyte-scale dataset, but is only one-<em>millionth</em> the volume of an entire human brain. Serious technical challenges remain in scaling up synapse-level brain mapping to an <a href="https://www.sciencedirect.com/science/article/abs/pii/S0092867420310011">entire mouse brain</a> (500x bigger than H01), let alone an entire human brain. One of these challenges is data storage: a mouse brain could generate an exabyte worth of data, which is costly to store. To address this, we are today also releasing a paper, “<a href="https://www.biorxiv.org/content/10.1101/2021.05.29.445828v1">Denoising-based Image Compression for Connectomics</a>”, that details how a machine learning-based denoising strategy can be used to compress data, such as H01, at least 17-fold (dashed line in the figure below), with negligible loss of accuracy in the automated reconstruction. </p>
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td style="text-align: center;"><a href="https://1.bp.blogspot.com/-TpKb0Ycw72Q/YLaHKBDDERI/AAAAAAAAHrM/Om5ioZmrnVYgVKaKRu4d3LkdSn-GrNphwCLcBGAsYHQ/s1999/image5.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="1428" data-original-width="1999" height="286" src="https://1.bp.blogspot.com/-TpKb0Ycw72Q/YLaHKBDDERI/AAAAAAAAHrM/Om5ioZmrnVYgVKaKRu4d3LkdSn-GrNphwCLcBGAsYHQ/w400-h286/image5.png" width="400"></a></td>
             </tr>
             <tr>
              <td class="tr-caption" style="text-align: center;">Reconstruction quality of noisy and denoised images as a function of compression rate for <a href="https://en.wikipedia.org/wiki/JPEG_XL">JPEG XL</a> (JXL) and <a href="https://en.wikipedia.org/wiki/AV1#AV1_Image_File_Format_(AVIF)">AV Image Format</a> (AVIF) codecs. Points and lines show the means, and the shaded area covers ±1 standard deviation around the mean.</td>
             </tr>
            </tbody>
           </table>
           <p>Random variations in the electron microscopy imaging process lead to image noise that is difficult to compress even in principle, as the noise lacks spatial correlations or other structure that could be described with fewer bytes. Therefore we acquired images of the same piece of tissue in both a “fast” acquisition regime (resulting in high amounts of noise) and a “slow” acquisition regime (resulting in low amounts of noise) and then trained a neural network to infer “slow” scans from “fast” scans. Standard image compression codecs were then able to (lossily) compress the “virtual” slow scans with fewer artifacts compared to the raw data. We believe this advance has the potential to significantly mitigate the costs associated with future large scale connectomics projects.</p> 
           <p> <b>Next Steps</b><br> But storage is not the only problem. The sheer size of future data sets will require developing new strategies for researchers to organize and access the rich information inherent in connectomic data. These are challenges that will require new modes of interaction between humans and the brain mapping data that will be forthcoming.</p> <span itemprop="author" itemscope itemtype="http://schema.org/Person"> 
            <meta content="https://plus.google.com/116899029375914044550" itemprop="url"> </span> 
          </noscript> 
         </div> 
        </div> 
        <div class="share"> <span class="twitter-custom social-wrapper" data-href="http://twitter.com/share?text=Google AI Blog:A Browsable Petascale Reconstruction of the Human Cortex&amp;url=http://ai.googleblog.com/2021/06/a-browsable-petascale-reconstruction-of.html&amp;via=googleai"> <img alt="Share on Twitter" height="24" src="https://www.gstatic.com/images/icons/material/system/2x/post_twitter_black_24dp.png" width="24"> </span> <span class="fb-custom social-wrapper" data-href="https://www.facebook.com/sharer.php?u=http://ai.googleblog.com/2021/06/a-browsable-petascale-reconstruction-of.html"> <img alt="Share on Facebook" height="24" src="https://www.gstatic.com/images/icons/material/system/2x/post_facebook_black_24dp.png" width="24"> </span> 
        </div> 
       </div> 
       <div class="post" data-id="5228922173972203503" itemscope itemtype="http://schema.org/BlogPosting"> 
        <h2 class="title" itemprop="name"> <a href="http://ai.googleblog.com/2021/05/cross-modal-contrastive-learning-for.html" itemprop="url" title="Cross-Modal Contrastive Learning for Text-to-Image Generation"> Cross-Modal Contrastive Learning for Text-to-Image Generation </a> </h2> 
        <div class="post-header"> 
         <div class="published"> <span class="publishdate" itemprop="datePublished"> Wednesday, May 26, 2021 </span> 
         </div> 
        </div> 
        <div class="post-body"> 
         <div class="post-content" itemprop="articleBody"> 
          <script type="text/template">
                          <span class="byline-author">Posted by Han Zhang, Research Scientist and Jing Yu Koh, Software Engineer, Google Research</span>

<p>
Automatic text-to-image synthesis, in which a model is trained to generate images from text descriptions alone, is a challenging task that has <a href="https://arxiv.org/abs/1605.05396">recently</a> <a href="https://arxiv.org/abs/1612.03242">received</a> <a href="https://arxiv.org/abs/1711.10485">significant</a> <a href="https://arxiv.org/abs/2102.12092">attention</a>. Its study provides rich insights into how machine learning (ML) models capture visual attributes and relate them to text. Compared to other kinds of inputs to guide image creation, such as sketches, object masks or mouse traces (which we have highlighted in <a href="https://arxiv.org/abs/2011.03775">prior work</a>), descriptive sentences are a more intuitive and flexible way to express visual concepts. Hence, a strong automatic text-to-image generation system can also be a useful tool for rapid content creation and could be applied to many other creative applications, similar to other efforts to integrate machine learning into the creation of art (e.g., <a href="https://magenta.tensorflow.org/">Magenta</a>).
</p>
<p>
State-of-the-art image synthesis results are typically achieved using <a href="https://en.wikipedia.org/wiki/Generative_adversarial_network">generative adversarial networks</a> (GANs), which train two models &#8212; a generator, which tries to create realistic images, and a discriminator, which tries to determine if an image is real or fabricated. Many <a href="http://proceedings.mlr.press/v48/reed16.pdf">text-to-image generation models</a> are GANs that are <a href="https://arxiv.org/abs/1610.09585">conditioned</a> using text inputs in order to generate semantically relevant images. This is significantly challenging, especially when long, ambiguous descriptions are provided. Moreover, GAN training can be prone to <a href="https://developers.google.com/machine-learning/gan/problems#mode-collapse">mode collapse</a>, a common failure case for the training process in which the generator learns to produce only a limited set of outputs, so that the discriminator fails to learn robust strategies to recognize fabricated images. To mitigate mode collapse, some approaches use <a href="https://arxiv.org/abs/1711.10485">multi-stage refinement networks</a> that iteratively refine an image. However, such systems require multi-stage training, which is less efficient than simpler single-stage end-to-end models. Other efforts rely on <a href="https://arxiv.org/abs/2011.03775">hierarchical approaches</a> that first model object layouts before finally synthesizing a realistic image. This requires the use of labeled segmentation data, which can be difficult to obtain. 
</p>
<p>
In &#8220;<a href="https://arxiv.org/abs/2101.04702">Cross-Modal Contrastive Learning for Text-to-Image Generation</a>,&#8221; to appear at&nbsp;<a href="http://cvpr2021.thecvf.com/">CVPR 2021</a>, we present the Cross-Modal Contrastive Generative Adversarial Network (XMC-GAN), which addresses text-to-image generation by learning to maximize the mutual information between image and text using inter-modal (image-text) and intra-modal (image-image) contrastive losses. This approach helps the discriminator to learn more robust and discriminative features, so XMC-GAN is less prone to mode collapse even with one-stage training. Importantly, XMC-GAN achieves state-of-the-art performance with a simple one-stage generation, as compared to previous multi-stage or hierarchical approaches. It is end-to-end trainable, and only requires image-text pairs (as opposed to labeled segmentation or bounding box data).
</p>
<p>
<b>Contrastive Losses for Text-to-Image Synthesis</b><br />
The goal of text-to-image synthesis systems is to produce clear, photo-realistic scenes with high semantic fidelity to their conditioned text descriptions. To achieve this, we propose to maximize the mutual information between the corresponding pairs: (1) images (real or generated) with a sentence describing the scene; (2) a generated image and a real image with the same description; and (3) regions of an image (real or generated) and words or phrases associated with them. 
</p>
<p>
In XMC-GAN, this is enforced using <a href="https://ai.googleblog.com/2020/04/advancing-self-supervised-and-semi.html">contrastive losses</a>. Similar to other GANs, XMC-GAN contains a generator for synthesizing images, and a discriminator that is trained to act as a critic between real and generated images. Three sets of data contribute to the contrastive loss in this system &#8212; the real images, the text that describes those images, and the images generated from the text descriptions. The individual loss functions for both the generator and the discriminator are combinations of the loss calculated from whole images with the full text description, combined with the loss calculated from sub-divided images with associated words or phrases. Then, for each batch of training data, we calculate the <a href="https://en.wikipedia.org/wiki/Cosine_similarity">cosine similarity</a> score between each text description and the real images, and likewise, between each text description and the batch of generated images. The goal is for the matching pairs (both text-to-image and real image-to-generated image) to have high similarity scores and for non-matching pairs to have low scores. Enforcing such a contrastive loss allows the discriminator to learn more robust and discriminative features. 
</p>
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://1.bp.blogspot.com/-jdQd4vYX9P0/YK6FPYTM2WI/AAAAAAAAHpQ/p4UuNFwsDFAZdSut4bH5g_iExSI-OCxVQCLcBGAsYHQ/s1232/image1%2B%25287%2529.jpg" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="607" data-original-width="1232" height="316" src="https://1.bp.blogspot.com/-jdQd4vYX9P0/YK6FPYTM2WI/AAAAAAAAHpQ/p4UuNFwsDFAZdSut4bH5g_iExSI-OCxVQCLcBGAsYHQ/w640-h316/image1%2B%25287%2529.jpg" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Inter-modal and intra-modal contrastive learning in our proposed XMC-GAN text-to-image synthesis model.</td></tr></tbody></table>
<p>
<b>Results</b><br />
We apply XMC-GAN to three challenging datasets &#8212; the first was a collection of <a href="https://cocodataset.org/#home">MS-COCO</a> descriptions of MS-COCO images, and the other two were datasets annotated with <a href="https://google.github.io/localized-narratives/">Localized Narratives</a>, one of which covers MS-COCO images (which we call LN-COCO) and the other of which describes <a href="https://ai.googleblog.com/2020/02/open-images-v6-now-featuring-localized.html">Open Images</a> data (LN-OpenImages). We find that XMC-GAN achieves a new state of the art on each. The images generated by XMC-GAN depict scenes that are of higher quality than those generated using other techniques. On MS-COCO, XMC-GAN improves the state-of-the-art <a href="https://en.wikipedia.org/wiki/Fr%C3%A9chet_inception_distance">Fréchet inception distance</a> (FID) score from 24.7 to 9.3, and is significantly preferred by human evaluators.
</p>
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://1.bp.blogspot.com/--RV-2TlLjZc/YK6F9SY4rKI/AAAAAAAAHpY/8yOlv6-099EpnYGW_Log-SFMhQvdOrpsACLcBGAsYHQ/s1270/image3.png" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="888" data-original-width="1270" height="448" src="https://1.bp.blogspot.com/--RV-2TlLjZc/YK6F9SY4rKI/AAAAAAAAHpY/8yOlv6-099EpnYGW_Log-SFMhQvdOrpsACLcBGAsYHQ/w640-h448/image3.png" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Selected qualitative results for generated images on <a href="https://cocodataset.org/">MS-COCO</a>.</td></tr></tbody></table>
<p>
Similarly, human raters prefer the image quality in XMC-GAN generated images 77.3% of the time, and 74.1% prefer its image-text alignment compared to three other state-of-the-art approaches (<a href="https://arxiv.org/abs/1912.08562">CP-GAN</a>, <a href="https://arxiv.org/abs/1904.01480">SD-GAN</a>, and <a href="https://arxiv.org/abs/1910.13321">OP-GAN</a>) .
</p>
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://1.bp.blogspot.com/-mxpIEZ7mSzE/YK6GDO778sI/AAAAAAAAHpc/Gjqfz2e7kucSpPagj5q1iRfSwMv1ursbQCLcBGAsYHQ/s1221/image2.png" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="571" data-original-width="1221" height="300" src="https://1.bp.blogspot.com/-mxpIEZ7mSzE/YK6GDO778sI/AAAAAAAAHpc/Gjqfz2e7kucSpPagj5q1iRfSwMv1ursbQCLcBGAsYHQ/w640-h300/image2.png" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Human evaluation on <a href="https://cocodataset.org/">MS-COCO</a> for image quality and text alignment. Annotators rank (anonymized and order-randomized) generated images from best to worst.</td></tr></tbody></table>
<p>
XMC-GAN also generalizes well to the challenging Localized Narratives dataset, which contains longer and more detailed descriptions. Our <a href="https://arxiv.org/abs/2011.03775">prior work TReCS</a> tackles text-to-image generation for Localized Narratives using mouse trace inputs to improve image generation quality. Despite not receiving mouse trace annotations, XMC-GAN is able to significantly outperform TReCS on image generation on LN-COCO, improving state-of-the-art FID from 48.7 to 14.1. Incorporating mouse traces and other additional inputs into an end-to-end model such as XMC-GAN would be interesting to study in future work.
</p>
<p>
In addition, we also train and evaluate on the LN-OpenImages, which is more challenging than MS-COCO because the dataset is much larger with images that cover a broader range of subject matter and that are more complex (8.4 objects on average). To the best of our knowledge, XMC-GAN is the first text-to-image synthesis model that is trained and evaluated on Open Images. XMC-GAN is able to generate high quality results, and sets a strong benchmark FID score of 26.9 on this very challenging task.
</p>
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://1.bp.blogspot.com/-ypFfNZ6OrGA/YK6GIBd21DI/AAAAAAAAHpg/5485ktwsjAEaGYC3aHtpbHgvNK3ork_2QCLcBGAsYHQ/s1136/image4.png" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="1118" data-original-width="1136" height="630" src="https://1.bp.blogspot.com/-ypFfNZ6OrGA/YK6GIBd21DI/AAAAAAAAHpg/5485ktwsjAEaGYC3aHtpbHgvNK3ork_2QCLcBGAsYHQ/w640-h630/image4.png" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Random samples of real and generated images on Open Images.</td></tr></tbody></table>
<p>
<b>Conclusion and Future Work</b><br />
In this work, we present a cross-modal contrastive learning framework to train GAN models for text-to-image synthesis. We investigate several cross-modal contrastive losses that enforce correspondence between image and text. For both human evaluations and quantitative metrics, XMC-GAN establishes a marked improvement over previous models on multiple datasets. It generates high quality images that match their input descriptions well, including for long, detailed narratives, and does so while being a simpler, end-to-end model. We believe that this represents a significant advance towards creative applications for image generation from natural language descriptions. As we continue this research, we are continually evaluating responsible approaches, potential applications and risk mitigation, in accordance with our <a href="https://www.blog.google/technology/ai/ai-principles/">AI Principles</a>.
</p>
<p>
<b>Acknowledgements</b><br />
<em>This is a joint work with<strong> </strong>Jason Baldridge, Honglak Lee, and Yinfei Yang. We would like to thank Kevin Murphy, Zizhao Zhang, Dilip Krishnan for their helpful feedback. We also want to thank the Google Data Compute team for their work on conducting human evaluations. We are also grateful for general support from the Google Research team. </em>
</p>
<span itemprop='author' itemscope='itemscope' itemtype='http://schema.org/Person'>
  <meta content='https://plus.google.com/116899029375914044550' itemprop='url'/>
</span>
                        </script> 
          <noscript> <span class="byline-author">Posted by Han Zhang, Research Scientist and Jing Yu Koh, Software Engineer, Google Research</span> 
           <p> Automatic text-to-image synthesis, in which a model is trained to generate images from text descriptions alone, is a challenging task that has <a href="https://arxiv.org/abs/1605.05396">recently</a> <a href="https://arxiv.org/abs/1612.03242">received</a> <a href="https://arxiv.org/abs/1711.10485">significant</a> <a href="https://arxiv.org/abs/2102.12092">attention</a>. Its study provides rich insights into how machine learning (ML) models capture visual attributes and relate them to text. Compared to other kinds of inputs to guide image creation, such as sketches, object masks or mouse traces (which we have highlighted in <a href="https://arxiv.org/abs/2011.03775">prior work</a>), descriptive sentences are a more intuitive and flexible way to express visual concepts. Hence, a strong automatic text-to-image generation system can also be a useful tool for rapid content creation and could be applied to many other creative applications, similar to other efforts to integrate machine learning into the creation of art (e.g., <a href="https://magenta.tensorflow.org/">Magenta</a>). </p> 
           <p> State-of-the-art image synthesis results are typically achieved using <a href="https://en.wikipedia.org/wiki/Generative_adversarial_network">generative adversarial networks</a> (GANs), which train two models — a generator, which tries to create realistic images, and a discriminator, which tries to determine if an image is real or fabricated. Many <a href="http://proceedings.mlr.press/v48/reed16.pdf">text-to-image generation models</a> are GANs that are <a href="https://arxiv.org/abs/1610.09585">conditioned</a> using text inputs in order to generate semantically relevant images. This is significantly challenging, especially when long, ambiguous descriptions are provided. Moreover, GAN training can be prone to <a href="https://developers.google.com/machine-learning/gan/problems#mode-collapse">mode collapse</a>, a common failure case for the training process in which the generator learns to produce only a limited set of outputs, so that the discriminator fails to learn robust strategies to recognize fabricated images. To mitigate mode collapse, some approaches use <a href="https://arxiv.org/abs/1711.10485">multi-stage refinement networks</a> that iteratively refine an image. However, such systems require multi-stage training, which is less efficient than simpler single-stage end-to-end models. Other efforts rely on <a href="https://arxiv.org/abs/2011.03775">hierarchical approaches</a> that first model object layouts before finally synthesizing a realistic image. This requires the use of labeled segmentation data, which can be difficult to obtain. </p> 
           <p> In “<a href="https://arxiv.org/abs/2101.04702">Cross-Modal Contrastive Learning for Text-to-Image Generation</a>,” to appear at&nbsp;<a href="http://cvpr2021.thecvf.com/">CVPR 2021</a>, we present the Cross-Modal Contrastive Generative Adversarial Network (XMC-GAN), which addresses text-to-image generation by learning to maximize the mutual information between image and text using inter-modal (image-text) and intra-modal (image-image) contrastive losses. This approach helps the discriminator to learn more robust and discriminative features, so XMC-GAN is less prone to mode collapse even with one-stage training. Importantly, XMC-GAN achieves state-of-the-art performance with a simple one-stage generation, as compared to previous multi-stage or hierarchical approaches. It is end-to-end trainable, and only requires image-text pairs (as opposed to labeled segmentation or bounding box data). </p> 
           <p> <b>Contrastive Losses for Text-to-Image Synthesis</b><br> The goal of text-to-image synthesis systems is to produce clear, photo-realistic scenes with high semantic fidelity to their conditioned text descriptions. To achieve this, we propose to maximize the mutual information between the corresponding pairs: (1) images (real or generated) with a sentence describing the scene; (2) a generated image and a real image with the same description; and (3) regions of an image (real or generated) and words or phrases associated with them. </p> 
           <p> In XMC-GAN, this is enforced using <a href="https://ai.googleblog.com/2020/04/advancing-self-supervised-and-semi.html">contrastive losses</a>. Similar to other GANs, XMC-GAN contains a generator for synthesizing images, and a discriminator that is trained to act as a critic between real and generated images. Three sets of data contribute to the contrastive loss in this system — the real images, the text that describes those images, and the images generated from the text descriptions. The individual loss functions for both the generator and the discriminator are combinations of the loss calculated from whole images with the full text description, combined with the loss calculated from sub-divided images with associated words or phrases. Then, for each batch of training data, we calculate the <a href="https://en.wikipedia.org/wiki/Cosine_similarity">cosine similarity</a> score between each text description and the real images, and likewise, between each text description and the batch of generated images. The goal is for the matching pairs (both text-to-image and real image-to-generated image) to have high similarity scores and for non-matching pairs to have low scores. Enforcing such a contrastive loss allows the discriminator to learn more robust and discriminative features. </p> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td style="text-align: center;"><a href="https://1.bp.blogspot.com/-jdQd4vYX9P0/YK6FPYTM2WI/AAAAAAAAHpQ/p4UuNFwsDFAZdSut4bH5g_iExSI-OCxVQCLcBGAsYHQ/s1232/image1%2B%25287%2529.jpg" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="607" data-original-width="1232" height="316" src="https://1.bp.blogspot.com/-jdQd4vYX9P0/YK6FPYTM2WI/AAAAAAAAHpQ/p4UuNFwsDFAZdSut4bH5g_iExSI-OCxVQCLcBGAsYHQ/w640-h316/image1%2B%25287%2529.jpg" width="640"></a></td>
             </tr>
             <tr>
              <td class="tr-caption" style="text-align: center;">Inter-modal and intra-modal contrastive learning in our proposed XMC-GAN text-to-image synthesis model.</td>
             </tr>
            </tbody>
           </table> 
           <p> <b>Results</b><br> We apply XMC-GAN to three challenging datasets — the first was a collection of <a href="https://cocodataset.org/#home">MS-COCO</a> descriptions of MS-COCO images, and the other two were datasets annotated with <a href="https://google.github.io/localized-narratives/">Localized Narratives</a>, one of which covers MS-COCO images (which we call LN-COCO) and the other of which describes <a href="https://ai.googleblog.com/2020/02/open-images-v6-now-featuring-localized.html">Open Images</a> data (LN-OpenImages). We find that XMC-GAN achieves a new state of the art on each. The images generated by XMC-GAN depict scenes that are of higher quality than those generated using other techniques. On MS-COCO, XMC-GAN improves the state-of-the-art <a href="https://en.wikipedia.org/wiki/Fr%C3%A9chet_inception_distance">Fréchet inception distance</a> (FID) score from 24.7 to 9.3, and is significantly preferred by human evaluators. </p> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td style="text-align: center;"><a href="https://1.bp.blogspot.com/--RV-2TlLjZc/YK6F9SY4rKI/AAAAAAAAHpY/8yOlv6-099EpnYGW_Log-SFMhQvdOrpsACLcBGAsYHQ/s1270/image3.png" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="888" data-original-width="1270" height="448" src="https://1.bp.blogspot.com/--RV-2TlLjZc/YK6F9SY4rKI/AAAAAAAAHpY/8yOlv6-099EpnYGW_Log-SFMhQvdOrpsACLcBGAsYHQ/w640-h448/image3.png" width="640"></a></td>
             </tr>
             <tr>
              <td class="tr-caption" style="text-align: center;">Selected qualitative results for generated images on <a href="https://cocodataset.org/">MS-COCO</a>.</td>
             </tr>
            </tbody>
           </table> 
           <p> Similarly, human raters prefer the image quality in XMC-GAN generated images 77.3% of the time, and 74.1% prefer its image-text alignment compared to three other state-of-the-art approaches (<a href="https://arxiv.org/abs/1912.08562">CP-GAN</a>, <a href="https://arxiv.org/abs/1904.01480">SD-GAN</a>, and <a href="https://arxiv.org/abs/1910.13321">OP-GAN</a>) . </p> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td style="text-align: center;"><a href="https://1.bp.blogspot.com/-mxpIEZ7mSzE/YK6GDO778sI/AAAAAAAAHpc/Gjqfz2e7kucSpPagj5q1iRfSwMv1ursbQCLcBGAsYHQ/s1221/image2.png" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="571" data-original-width="1221" height="300" src="https://1.bp.blogspot.com/-mxpIEZ7mSzE/YK6GDO778sI/AAAAAAAAHpc/Gjqfz2e7kucSpPagj5q1iRfSwMv1ursbQCLcBGAsYHQ/w640-h300/image2.png" width="640"></a></td>
             </tr>
             <tr>
              <td class="tr-caption" style="text-align: center;">Human evaluation on <a href="https://cocodataset.org/">MS-COCO</a> for image quality and text alignment. Annotators rank (anonymized and order-randomized) generated images from best to worst.</td>
             </tr>
            </tbody>
           </table> 
           <p> XMC-GAN also generalizes well to the challenging Localized Narratives dataset, which contains longer and more detailed descriptions. Our <a href="https://arxiv.org/abs/2011.03775">prior work TReCS</a> tackles text-to-image generation for Localized Narratives using mouse trace inputs to improve image generation quality. Despite not receiving mouse trace annotations, XMC-GAN is able to significantly outperform TReCS on image generation on LN-COCO, improving state-of-the-art FID from 48.7 to 14.1. Incorporating mouse traces and other additional inputs into an end-to-end model such as XMC-GAN would be interesting to study in future work. </p> 
           <p> In addition, we also train and evaluate on the LN-OpenImages, which is more challenging than MS-COCO because the dataset is much larger with images that cover a broader range of subject matter and that are more complex (8.4 objects on average). To the best of our knowledge, XMC-GAN is the first text-to-image synthesis model that is trained and evaluated on Open Images. XMC-GAN is able to generate high quality results, and sets a strong benchmark FID score of 26.9 on this very challenging task. </p> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td style="text-align: center;"><a href="https://1.bp.blogspot.com/-ypFfNZ6OrGA/YK6GIBd21DI/AAAAAAAAHpg/5485ktwsjAEaGYC3aHtpbHgvNK3ork_2QCLcBGAsYHQ/s1136/image4.png" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="1118" data-original-width="1136" height="630" src="https://1.bp.blogspot.com/-ypFfNZ6OrGA/YK6GIBd21DI/AAAAAAAAHpg/5485ktwsjAEaGYC3aHtpbHgvNK3ork_2QCLcBGAsYHQ/w640-h630/image4.png" width="640"></a></td>
             </tr>
             <tr>
              <td class="tr-caption" style="text-align: center;">Random samples of real and generated images on Open Images.</td>
             </tr>
            </tbody>
           </table> 
           <p> <b>Conclusion and Future Work</b><br> In this work, we present a cross-modal contrastive learning framework to train GAN models for text-to-image synthesis. We investigate several cross-modal contrastive losses that enforce correspondence between image and text. For both human evaluations and quantitative metrics, XMC-GAN establishes a marked improvement over previous models on multiple datasets. It generates high quality images that match their input descriptions well, including for long, detailed narratives, and does so while being a simpler, end-to-end model. We believe that this represents a significant advance towards creative applications for image generation from natural language descriptions. As we continue this research, we are continually evaluating responsible approaches, potential applications and risk mitigation, in accordance with our <a href="https://www.blog.google/technology/ai/ai-principles/">AI Principles</a>. </p> 
           <p> <b>Acknowledgements</b><br> <em>This is a joint work with<strong> </strong>Jason Baldridge, Honglak Lee, and Yinfei Yang. We would like to thank Kevin Murphy, Zizhao Zhang, Dilip Krishnan for their helpful feedback. We also want to thank the Google Data Compute team for their work on conducting human evaluations. We are also grateful for general support from the Google Research team. </em> </p> <span itemprop="author" itemscope itemtype="http://schema.org/Person"> 
            <meta content="https://plus.google.com/116899029375914044550" itemprop="url"> </span> 
          </noscript> 
         </div> 
        </div> 
        <div class="share"> <span class="twitter-custom social-wrapper" data-href="http://twitter.com/share?text=Google AI Blog:Cross-Modal Contrastive Learning for Text-to-Image Generation&amp;url=http://ai.googleblog.com/2021/05/cross-modal-contrastive-learning-for.html&amp;via=googleai"> <img alt="Share on Twitter" height="24" src="https://www.gstatic.com/images/icons/material/system/2x/post_twitter_black_24dp.png" width="24"> </span> <span class="fb-custom social-wrapper" data-href="https://www.facebook.com/sharer.php?u=http://ai.googleblog.com/2021/05/cross-modal-contrastive-learning-for.html"> <img alt="Share on Facebook" height="24" src="https://www.gstatic.com/images/icons/material/system/2x/post_facebook_black_24dp.png" width="24"> </span> 
        </div> 
       </div> 
       <div class="post" data-id="1489430892938743422" itemscope itemtype="http://schema.org/BlogPosting"> 
        <h2 class="title" itemprop="name"> <a href="http://ai.googleblog.com/2021/05/understanding-contextual-facial.html" itemprop="url" title="Understanding Contextual Facial Expressions Across the Globe"> Understanding Contextual Facial Expressions Across the Globe </a> </h2> 
        <div class="post-header"> 
         <div class="published"> <span class="publishdate" itemprop="datePublished"> Monday, May 24, 2021 </span> 
         </div> 
        </div> 
        <div class="post-body"> 
         <div class="post-content" itemprop="articleBody"> 
          <script type="text/template">
                          <span class="byline-author">Posted by Alan Cowen, Visiting Researcher and Gautam Prasad, Software Engineer, Google Research</span>

<p>
It might seem reasonable to assume that people&#8217;s facial expressions are universal &#8212; so, for example, whether a person is from Brazil, India or Canada, their smile upon seeing close friends or their expression of awe at a fireworks display would look essentially the same. But is that really true? Is the association between these facial expressions and their relevant context across geographies indeed universal? What can similarities &#8212; or differences &#8212; between the situations where someone grins or frowns tell us about how people may be connected across different cultures? 
</p>
<p>
Scientists seeking to answer these questions and to uncover the extent to which people are connected across cultures and geography often use survey-based studies that can rely heavily on local <a href="https://www.tandfonline.com/doi/abs/10.1080/02699931.2016.1202200">language</a>, <a href="https://pubmed.ncbi.nlm.nih.gov/1454891/">norms</a>, and <a href="https://pubmed.ncbi.nlm.nih.gov/16536652/">values</a>. However, such studies are not scalable, and often end up with <a href="https://pubmed.ncbi.nlm.nih.gov/11931516/">small sample sizes</a> and <a href="https://journals.sagepub.com/eprint/SAUES8UM69EN8TSMUGF9/full">inconsistent findings</a>. 
</p>
<p>
In contrast to survey-based studies, studying patterns of facial movement provides a more <a href="https://www.pnas.org/content/105/33/11655">direct</a> understanding of expressive behavior. But analyzing how facial expressions are actually used in everyday life would require researchers to go through <em>millions of hours</em> of real-world footage, which is too time-consuming to do <a href="https://books.google.com/books?hl=en&amp;lr=&amp;id=9xhnDAAAQBAJ&amp;oi=fnd&amp;pg=PR5&amp;dq=Handbook+of+Emotion+Elicitation+and+Assessment&amp;ots=nMBuaYhDC3&amp;sig=ykAzdmJrfEOLogTVtTqxb9hFzT8#v=onepage&amp;q=Handbook%20of%20Emotion%20Elicitation%20and%20Assessment&amp;f=false">manually</a>. In addition, facial expressions and the contexts in which they are exhibited are complicated, requiring large sample sizes in order to make <a href="https://pubmed.ncbi.nlm.nih.gov/2813654/">statistically sound conclusions</a>. While existing studies have produced <a href="https://psycnet.apa.org/record/2002-08416-007">diverging</a> <a href="https://journals.sagepub.com/doi/abs/10.1177/1754073911410740">answers</a> to the question of the universality of facial expressions in given contexts, applying machine learning (ML) in order to appropriately scale the research has the potential to provide clarity.
</p>
<p>
In &#8220;<a href="https://www.nature.com/articles/s41586-020-3037-7">Sixteen facial expressions occur in similar contexts worldwide</a>&#8221;, published in <em><a href="https://www.nature.com/">Nature</a></em>, we present research undertaken in collaboration with UC Berkeley to conduct the first large-scale worldwide analysis of how facial expressions are actually used in everyday life, leveraging <a href="https://en.wikipedia.org/wiki/Deep_learning">deep neural networks</a> (DNNs) to drastically scale up expression analysis in a responsible and thoughtful way. Using a dataset of six million publicly available videos across 144 countries, we analyze the contexts in which people use a variety of facial expressions and demonstrate that rich nuances in facial behavior &#8212; including subtle expressions &#8212; are used in similar social situations around the world.
</p>
<p>
<b>A Deep Neural Network Measuring Facial Expression</b><br />
Facial expressions are not static. If one were to examine a person&#8217;s expression instant by instant, what might at first appear to be &#8220;anger&#8221;, may instead end up being &#8220;awe&#8221;, &#8220;surprise&#8221; or &#8220;confusion&#8221;. The interpretation depends on the dynamics of a person&#8217;s face as their expression presents itself. The challenge in building a neural network to understand facial expressions, then, is that it must interpret the expression within its temporal context. Training such a system requires a large and diverse, cross-cultural dataset of videos with fully annotated expressions.
</p>
<p>
To build the dataset, skilled raters manually searched through a broad collection of publicly available videos to identify those likely to contain clips covering all of our pre-selected expression categories. To ensure that the videos matched the region they were assumed to represent, preference in video selection was given to those that included the geographic location of origin. The faces in the videos were then found using a deep convolutional neural network (CNN) &#8212; similar to the <a href="https://cloud.google.com/vision/docs/detecting-faces">Google Cloud Face Detection API</a> &#8212; that follows faces over the course of the clip using a method based on traditional <a href="https://en.wikipedia.org/wiki/Optical_flow.">optical flow</a>. Using an interface similar to <a href="https://crowdsource.google.com/cs/contribute/facial-expressions/en">Google Crowdsource</a>, annotators then labeled facial expressions across <a href="https://pubmed.ncbi.nlm.nih.gov/31204816/">28 distinct categories</a> if present at any point during the clip. Because the goal was to sample how an average person would perceive an expression, the annotators were not coached or trained, nor were they provided examples or definitions of the target expressions. We discuss additional experiments to evaluate whether the model trained from these annotations was biased below.
</p>
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://1.bp.blogspot.com/-vq_Mtkj33xg/YKvZhjAnf0I/AAAAAAAAHo8/jc5-kBmHYQoscgKKXAcymugMGe_9VzfzgCLcBGAsYHQ/s1142/image1.png" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="818" data-original-width="1142" height="458" src="https://1.bp.blogspot.com/-vq_Mtkj33xg/YKvZhjAnf0I/AAAAAAAAHo8/jc5-kBmHYQoscgKKXAcymugMGe_9VzfzgCLcBGAsYHQ/w640-h458/image1.png" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Raters were presented videos with a single face highlighted for their attention. They observed the subject throughout the duration of the clip and annotated the facial expressions they exhibited. (<a href="https://www.youtube.com/watch?v=B-1xzudnueQ">source video</a>)</td></tr></tbody></table>
<p>
The face detection algorithm established a sequence of locations of each face throughout the video. We then used a pre-trained <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf">Inception network</a> to extract features representing the most salient aspects of facial expressions from the faces. The features were then fed into a <a href="https://en.wikipedia.org/wiki/Long_short-term_memory">long short-term memory</a> (LSTM) network, a type of recurrent neural network that is able to model how a facial expression might evolve over time due to its ability to remember salient information from the past. 
</p>
<p>
In order to ensure that the model was making consistent predictions across a range of demographic groups, we evaluated the model fairness on an <a href="https://psycnet.apa.org/record/2019-32629-001">existing dataset</a> that was constructed using similar facial expression labels, targeting a subset of 16 expressions on which it exhibited the best performance.
</p>
<p>
The model's performance was consistent across all of the demographic groups represented in the evaluation dataset, which provides supporting evidence that the model trained to annotated facial expressions is not measurably biased. The model&#8217;s annotations of those 16 facial expressions across 1,500 images can be explored <a href="https://face28.s3-us-west-1.amazonaws.com/expressnet.html">here</a>. 
</p>
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://1.bp.blogspot.com/-8X3zCp6E4zM/YKvZmstx1OI/AAAAAAAAHpA/bK97kLyNaysewWyzZC1Y3VQQ21CCyFHrgCLcBGAsYHQ/s1928/image2.png" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="1038" data-original-width="1928" height="344" src="https://1.bp.blogspot.com/-8X3zCp6E4zM/YKvZmstx1OI/AAAAAAAAHpA/bK97kLyNaysewWyzZC1Y3VQQ21CCyFHrgCLcBGAsYHQ/w640-h344/image2.png" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">We modeled the selected face in each video by using a CNN to extract features from the face at each frame, which were then fed into an LSTM network to model the changes in the expression over time. (<a href="https://www.youtube.com/watch?v=B-1xzudnueQ">source video</a>)</td></tr></tbody></table>
<p>
<b>Measuring the Contexts Captured in Videos</b><br />
To understand the context of facial expressions across millions of videos, we used DNNs that could capture the fine-grained content and automatically recognize the context. The first DNN modeled a combination of <a href="https://arxiv.org/pdf/1904.08067.pdf">text features</a> (title and description) associated with a video along with the <a href="https://openaccess.thecvf.com/content_ECCVW_2018/papers/11132/Lee_The_2nd_YouTube-8M_Large-Scale_Video_Understanding_Challenge_ECCVW_2018_paper.pdf">actual visual content</a> (<em>video-topic model</em>). In addition, we used a DNN that only relied on text features without any visual information (<em>text-topic model</em>). These models predict thousands of labels describing the videos. In our experiments these models were able to identify hundreds of unique contexts (e.g., wedding, sporting event, or fireworks) showcasing the diversity of the data we used for the analysis.
</p>
<p>
<b>The Covariation Between Expressions and Contexts Around the World</b><br />
In our first experiment, we analyzed 3 million public videos captured on mobile phones. We chose to focus on mobile uploads because they are more likely to contain natural expressions. We correlated the facial expressions that occurred in the videos to the context annotations derived from the <em>video-topic model</em>. We found 16 kinds of facial expressions had distinct associations with everyday social contexts that were consistent across the world. For instance, the expressions that people associate with amusement occurred more often in videos with practical jokes; expressions that people associate with awe, in videos with fireworks; and triumph, with sporting events. These results have strong implications for discussions about the relative <a href="https://www.tandfonline.com/doi/abs/10.1080/026999399379168">importance of psychologically relevant context in facial expression</a>, compared to other factors, such as those unique to an individual, culture, or society. 
</p>
<p>
Our second experiment analyzed a separate set of 3 million videos, but this time we annotated the contexts with the <em>text-topic model. </em>The results verified that the findings in the first experiment were not driven by subtle influences of facial expressions in the video on the annotations of the <em>video-topic model</em>. In other words we used this experiment to verify our conclusions from the first experiment given the possibility that the <em>video-topic model</em> could implicitly be factoring in facial expressions when computing its content labels.
</p>
<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://1.bp.blogspot.com/-aBWPQ90BrP4/YKvZsutVUGI/AAAAAAAAHpE/WKFSM38TFzIDOIL2GF1NFGMBTQ20z3LSQCLcBGAsYHQ/s819/image3.png" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="819" data-original-width="712" height="640" src="https://1.bp.blogspot.com/-aBWPQ90BrP4/YKvZsutVUGI/AAAAAAAAHpE/WKFSM38TFzIDOIL2GF1NFGMBTQ20z3LSQCLcBGAsYHQ/w557-h640/image3.png" width="557" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">We correlated the expression and context annotations across all of the videos within each region. Each expression was found to have specific associations with different contexts that were preserved across 12 world regions. For example, here, in red, we can see that expressions people associate with awe were found more often in the context of fireworks, pets, and toys than in other contexts.</td></tr></tbody></table>
<p>
In both experiments, the correlations between expressions and contexts appeared to be well-preserved across cultures. To quantify exactly how similar the associations between expressions and contexts were across the 12 different world regions we studied, we computed second-order correlations between each pair of regions. These correlations identify the relationships between different expressions and contexts in each region and then compare them with other regions. We found that 70% of the context&#8211;expression associations found in each region are shared across the modern world. 
</p>
<p>
Finally, we asked how many of the 16 kinds of facial expression we measured had distinct associations with different contexts that were preserved around the world. To do so, we applied a method called <a href="https://www.mitpressjournals.org/doi/abs/10.1162/0899766042321814">canonical correlations analysis</a>, which showed that all 16 facial expressions had distinct associations that were preserved across the world.
</p>
<p>
<b>Conclusions</b><br />
We were able to examine the contexts in which facial expressions occur in everyday life across cultures at an unprecedented scale. Machine learning allowed us to analyze millions of videos across the world and discover evidence supporting hypotheses that facial expressions are preserved to a degree in similar contexts across cultures.
</p>
<p>
Our results also leave room for cultural differences. Although the correlations between facial expressions and contexts were 70% consistent around the world, they were up to 30% variable across regions. Neighboring world regions generally had more similar associations between facial expressions and contexts than distant world regions, indicating that the geographic spread of human culture may also play a role in the meanings of facial expressions. 
</p>
<p>
This work shows that we can use machine learning to better understand ourselves and identify common communication elements across cultures. Tools such as DNNs give us the opportunity to provide vast  amounts of diverse data in service of scientific discovery, enabling more confidence in the statistical conclusions. We hope our work provides a template for using the tools of machine learning in a responsible way and sparks more innovative research in other scientific domains.
</p>
<p>
<b>Acknowledgements</b><br />
<em>Special thanks to our co-authors Dacher Keltner from UC Berkeley, along with Florian Schroff, Brendan Jou, and Hartwig Adam from Google Research. We are also grateful for additional support at Google provided by Laura Rapin, Reena Jana, Will Carter, Unni Nair, Christine Robson, Jen Gennai, Sourish Chaudhuri, Greg Corrado, Brian Eoff, Andrew Smart, Raine Serrano, Blaise Aguera y Arcas, Jay Yagnik, and Carson Mcneil.</em>
</p>
<span itemprop='author' itemscope='itemscope' itemtype='http://schema.org/Person'>
  <meta content='https://plus.google.com/116899029375914044550' itemprop='url'/>
</span>
                        </script> 
          <noscript> <span class="byline-author">Posted by Alan Cowen, Visiting Researcher and Gautam Prasad, Software Engineer, Google Research</span> 
           <p> It might seem reasonable to assume that people’s facial expressions are universal — so, for example, whether a person is from Brazil, India or Canada, their smile upon seeing close friends or their expression of awe at a fireworks display would look essentially the same. But is that really true? Is the association between these facial expressions and their relevant context across geographies indeed universal? What can similarities — or differences — between the situations where someone grins or frowns tell us about how people may be connected across different cultures? </p> 
           <p> Scientists seeking to answer these questions and to uncover the extent to which people are connected across cultures and geography often use survey-based studies that can rely heavily on local <a href="https://www.tandfonline.com/doi/abs/10.1080/02699931.2016.1202200">language</a>, <a href="https://pubmed.ncbi.nlm.nih.gov/1454891/">norms</a>, and <a href="https://pubmed.ncbi.nlm.nih.gov/16536652/">values</a>. However, such studies are not scalable, and often end up with <a href="https://pubmed.ncbi.nlm.nih.gov/11931516/">small sample sizes</a> and <a href="https://journals.sagepub.com/eprint/SAUES8UM69EN8TSMUGF9/full">inconsistent findings</a>. </p> 
           <p> In contrast to survey-based studies, studying patterns of facial movement provides a more <a href="https://www.pnas.org/content/105/33/11655">direct</a> understanding of expressive behavior. But analyzing how facial expressions are actually used in everyday life would require researchers to go through <em>millions of hours</em> of real-world footage, which is too time-consuming to do <a href="https://books.google.com/books?hl=en&amp;lr=&amp;id=9xhnDAAAQBAJ&amp;oi=fnd&amp;pg=PR5&amp;dq=Handbook+of+Emotion+Elicitation+and+Assessment&amp;ots=nMBuaYhDC3&amp;sig=ykAzdmJrfEOLogTVtTqxb9hFzT8#v=onepage&amp;q=Handbook%20of%20Emotion%20Elicitation%20and%20Assessment&amp;f=false">manually</a>. In addition, facial expressions and the contexts in which they are exhibited are complicated, requiring large sample sizes in order to make <a href="https://pubmed.ncbi.nlm.nih.gov/2813654/">statistically sound conclusions</a>. While existing studies have produced <a href="https://psycnet.apa.org/record/2002-08416-007">diverging</a> <a href="https://journals.sagepub.com/doi/abs/10.1177/1754073911410740">answers</a> to the question of the universality of facial expressions in given contexts, applying machine learning (ML) in order to appropriately scale the research has the potential to provide clarity. </p> 
           <p> In “<a href="https://www.nature.com/articles/s41586-020-3037-7">Sixteen facial expressions occur in similar contexts worldwide</a>”, published in <em><a href="https://www.nature.com/">Nature</a></em>, we present research undertaken in collaboration with UC Berkeley to conduct the first large-scale worldwide analysis of how facial expressions are actually used in everyday life, leveraging <a href="https://en.wikipedia.org/wiki/Deep_learning">deep neural networks</a> (DNNs) to drastically scale up expression analysis in a responsible and thoughtful way. Using a dataset of six million publicly available videos across 144 countries, we analyze the contexts in which people use a variety of facial expressions and demonstrate that rich nuances in facial behavior — including subtle expressions — are used in similar social situations around the world. </p> 
           <p> <b>A Deep Neural Network Measuring Facial Expression</b><br> Facial expressions are not static. If one were to examine a person’s expression instant by instant, what might at first appear to be “anger”, may instead end up being “awe”, “surprise” or “confusion”. The interpretation depends on the dynamics of a person’s face as their expression presents itself. The challenge in building a neural network to understand facial expressions, then, is that it must interpret the expression within its temporal context. Training such a system requires a large and diverse, cross-cultural dataset of videos with fully annotated expressions. </p> 
           <p> To build the dataset, skilled raters manually searched through a broad collection of publicly available videos to identify those likely to contain clips covering all of our pre-selected expression categories. To ensure that the videos matched the region they were assumed to represent, preference in video selection was given to those that included the geographic location of origin. The faces in the videos were then found using a deep convolutional neural network (CNN) — similar to the <a href="https://cloud.google.com/vision/docs/detecting-faces">Google Cloud Face Detection API</a> — that follows faces over the course of the clip using a method based on traditional <a href="https://en.wikipedia.org/wiki/Optical_flow.">optical flow</a>. Using an interface similar to <a href="https://crowdsource.google.com/cs/contribute/facial-expressions/en">Google Crowdsource</a>, annotators then labeled facial expressions across <a href="https://pubmed.ncbi.nlm.nih.gov/31204816/">28 distinct categories</a> if present at any point during the clip. Because the goal was to sample how an average person would perceive an expression, the annotators were not coached or trained, nor were they provided examples or definitions of the target expressions. We discuss additional experiments to evaluate whether the model trained from these annotations was biased below. </p> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td style="text-align: center;"><a href="https://1.bp.blogspot.com/-vq_Mtkj33xg/YKvZhjAnf0I/AAAAAAAAHo8/jc5-kBmHYQoscgKKXAcymugMGe_9VzfzgCLcBGAsYHQ/s1142/image1.png" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="818" data-original-width="1142" height="458" src="https://1.bp.blogspot.com/-vq_Mtkj33xg/YKvZhjAnf0I/AAAAAAAAHo8/jc5-kBmHYQoscgKKXAcymugMGe_9VzfzgCLcBGAsYHQ/w640-h458/image1.png" width="640"></a></td>
             </tr>
             <tr>
              <td class="tr-caption" style="text-align: center;">Raters were presented videos with a single face highlighted for their attention. They observed the subject throughout the duration of the clip and annotated the facial expressions they exhibited. (<a href="https://www.youtube.com/watch?v=B-1xzudnueQ">source video</a>)</td>
             </tr>
            </tbody>
           </table> 
           <p> The face detection algorithm established a sequence of locations of each face throughout the video. We then used a pre-trained <a href="https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Szegedy_Going_Deeper_With_2015_CVPR_paper.pdf">Inception network</a> to extract features representing the most salient aspects of facial expressions from the faces. The features were then fed into a <a href="https://en.wikipedia.org/wiki/Long_short-term_memory">long short-term memory</a> (LSTM) network, a type of recurrent neural network that is able to model how a facial expression might evolve over time due to its ability to remember salient information from the past. </p> 
           <p> In order to ensure that the model was making consistent predictions across a range of demographic groups, we evaluated the model fairness on an <a href="https://psycnet.apa.org/record/2019-32629-001">existing dataset</a> that was constructed using similar facial expression labels, targeting a subset of 16 expressions on which it exhibited the best performance. </p> 
           <p> The model's performance was consistent across all of the demographic groups represented in the evaluation dataset, which provides supporting evidence that the model trained to annotated facial expressions is not measurably biased. The model’s annotations of those 16 facial expressions across 1,500 images can be explored <a href="https://face28.s3-us-west-1.amazonaws.com/expressnet.html">here</a>. </p> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td style="text-align: center;"><a href="https://1.bp.blogspot.com/-8X3zCp6E4zM/YKvZmstx1OI/AAAAAAAAHpA/bK97kLyNaysewWyzZC1Y3VQQ21CCyFHrgCLcBGAsYHQ/s1928/image2.png" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="1038" data-original-width="1928" height="344" src="https://1.bp.blogspot.com/-8X3zCp6E4zM/YKvZmstx1OI/AAAAAAAAHpA/bK97kLyNaysewWyzZC1Y3VQQ21CCyFHrgCLcBGAsYHQ/w640-h344/image2.png" width="640"></a></td>
             </tr>
             <tr>
              <td class="tr-caption" style="text-align: center;">We modeled the selected face in each video by using a CNN to extract features from the face at each frame, which were then fed into an LSTM network to model the changes in the expression over time. (<a href="https://www.youtube.com/watch?v=B-1xzudnueQ">source video</a>)</td>
             </tr>
            </tbody>
           </table> 
           <p> <b>Measuring the Contexts Captured in Videos</b><br> To understand the context of facial expressions across millions of videos, we used DNNs that could capture the fine-grained content and automatically recognize the context. The first DNN modeled a combination of <a href="https://arxiv.org/pdf/1904.08067.pdf">text features</a> (title and description) associated with a video along with the <a href="https://openaccess.thecvf.com/content_ECCVW_2018/papers/11132/Lee_The_2nd_YouTube-8M_Large-Scale_Video_Understanding_Challenge_ECCVW_2018_paper.pdf">actual visual content</a> (<em>video-topic model</em>). In addition, we used a DNN that only relied on text features without any visual information (<em>text-topic model</em>). These models predict thousands of labels describing the videos. In our experiments these models were able to identify hundreds of unique contexts (e.g., wedding, sporting event, or fireworks) showcasing the diversity of the data we used for the analysis. </p> 
           <p> <b>The Covariation Between Expressions and Contexts Around the World</b><br> In our first experiment, we analyzed 3 million public videos captured on mobile phones. We chose to focus on mobile uploads because they are more likely to contain natural expressions. We correlated the facial expressions that occurred in the videos to the context annotations derived from the <em>video-topic model</em>. We found 16 kinds of facial expressions had distinct associations with everyday social contexts that were consistent across the world. For instance, the expressions that people associate with amusement occurred more often in videos with practical jokes; expressions that people associate with awe, in videos with fireworks; and triumph, with sporting events. These results have strong implications for discussions about the relative <a href="https://www.tandfonline.com/doi/abs/10.1080/026999399379168">importance of psychologically relevant context in facial expression</a>, compared to other factors, such as those unique to an individual, culture, or society. </p> 
           <p> Our second experiment analyzed a separate set of 3 million videos, but this time we annotated the contexts with the <em>text-topic model. </em>The results verified that the findings in the first experiment were not driven by subtle influences of facial expressions in the video on the annotations of the <em>video-topic model</em>. In other words we used this experiment to verify our conclusions from the first experiment given the possibility that the <em>video-topic model</em> could implicitly be factoring in facial expressions when computing its content labels. </p> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td style="text-align: center;"><a href="https://1.bp.blogspot.com/-aBWPQ90BrP4/YKvZsutVUGI/AAAAAAAAHpE/WKFSM38TFzIDOIL2GF1NFGMBTQ20z3LSQCLcBGAsYHQ/s819/image3.png" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="819" data-original-width="712" height="640" src="https://1.bp.blogspot.com/-aBWPQ90BrP4/YKvZsutVUGI/AAAAAAAAHpE/WKFSM38TFzIDOIL2GF1NFGMBTQ20z3LSQCLcBGAsYHQ/w557-h640/image3.png" width="557"></a></td>
             </tr>
             <tr>
              <td class="tr-caption" style="text-align: center;">We correlated the expression and context annotations across all of the videos within each region. Each expression was found to have specific associations with different contexts that were preserved across 12 world regions. For example, here, in red, we can see that expressions people associate with awe were found more often in the context of fireworks, pets, and toys than in other contexts.</td>
             </tr>
            </tbody>
           </table> 
           <p> In both experiments, the correlations between expressions and contexts appeared to be well-preserved across cultures. To quantify exactly how similar the associations between expressions and contexts were across the 12 different world regions we studied, we computed second-order correlations between each pair of regions. These correlations identify the relationships between different expressions and contexts in each region and then compare them with other regions. We found that 70% of the context–expression associations found in each region are shared across the modern world. </p> 
           <p> Finally, we asked how many of the 16 kinds of facial expression we measured had distinct associations with different contexts that were preserved around the world. To do so, we applied a method called <a href="https://www.mitpressjournals.org/doi/abs/10.1162/0899766042321814">canonical correlations analysis</a>, which showed that all 16 facial expressions had distinct associations that were preserved across the world. </p> 
           <p> <b>Conclusions</b><br> We were able to examine the contexts in which facial expressions occur in everyday life across cultures at an unprecedented scale. Machine learning allowed us to analyze millions of videos across the world and discover evidence supporting hypotheses that facial expressions are preserved to a degree in similar contexts across cultures. </p> 
           <p> Our results also leave room for cultural differences. Although the correlations between facial expressions and contexts were 70% consistent around the world, they were up to 30% variable across regions. Neighboring world regions generally had more similar associations between facial expressions and contexts than distant world regions, indicating that the geographic spread of human culture may also play a role in the meanings of facial expressions. </p> 
           <p> This work shows that we can use machine learning to better understand ourselves and identify common communication elements across cultures. Tools such as DNNs give us the opportunity to provide vast amounts of diverse data in service of scientific discovery, enabling more confidence in the statistical conclusions. We hope our work provides a template for using the tools of machine learning in a responsible way and sparks more innovative research in other scientific domains. </p> 
           <p> <b>Acknowledgements</b><br> <em>Special thanks to our co-authors Dacher Keltner from UC Berkeley, along with Florian Schroff, Brendan Jou, and Hartwig Adam from Google Research. We are also grateful for additional support at Google provided by Laura Rapin, Reena Jana, Will Carter, Unni Nair, Christine Robson, Jen Gennai, Sourish Chaudhuri, Greg Corrado, Brian Eoff, Andrew Smart, Raine Serrano, Blaise Aguera y Arcas, Jay Yagnik, and Carson Mcneil.</em> </p> <span itemprop="author" itemscope itemtype="http://schema.org/Person"> 
            <meta content="https://plus.google.com/116899029375914044550" itemprop="url"> </span> 
          </noscript> 
         </div> 
        </div> 
        <div class="share"> <span class="twitter-custom social-wrapper" data-href="http://twitter.com/share?text=Google AI Blog:Understanding Contextual Facial Expressions Across the Globe&amp;url=http://ai.googleblog.com/2021/05/understanding-contextual-facial.html&amp;via=googleai"> <img alt="Share on Twitter" height="24" src="https://www.gstatic.com/images/icons/material/system/2x/post_twitter_black_24dp.png" width="24"> </span> <span class="fb-custom social-wrapper" data-href="https://www.facebook.com/sharer.php?u=http://ai.googleblog.com/2021/05/understanding-contextual-facial.html"> <img alt="Share on Facebook" height="24" src="https://www.gstatic.com/images/icons/material/system/2x/post_facebook_black_24dp.png" width="24"> </span> 
        </div> 
       </div> 
       <div class="post" data-id="6161396262057299957" itemscope itemtype="http://schema.org/BlogPosting"> 
        <h2 class="title" itemprop="name"> <a href="http://ai.googleblog.com/2021/05/kelm-integrating-knowledge-graphs-with.html" itemprop="url" title="KELM: Integrating Knowledge Graphs with Language Model Pre-training Corpora"> KELM: Integrating Knowledge Graphs with Language Model Pre-training Corpora </a> </h2> 
        <div class="post-header"> 
         <div class="published"> <span class="publishdate" itemprop="datePublished"> Thursday, May 20, 2021 </span> 
         </div> 
        </div> 
        <div class="post-body"> 
         <div class="post-content" itemprop="articleBody"> 
          <script type="text/template">
                          <span class="byline-author">Posted by Siamak Shakeri, Staff Software Engineer and Oshin Agarwal, Research Intern, Google Research</span>


<p>
Large pre-trained <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a> (NLP) models, such as <a href="https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html">BERT</a>, <a href="https://arxiv.org/abs/1907.11692">RoBERTa</a>, <a href="https://arxiv.org/abs/2005.14165">GPT-3</a>, <a href="https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html">T5</a> and <a href="https://ai.googleblog.com/2020/08/realm-integrating-retrieval-into.html">REALM</a>, leverage natural language corpora that are derived from the Web and fine-tuned on task specific data, and have made significant advances in various NLP tasks. However, natural language text alone represents a limited coverage of knowledge, and facts may be contained in wordy sentences in many different ways. Furthermore, existence of non-factual information and toxic content in text can eventually cause <a href="https://www.aclweb.org/anthology/2020.findings-emnlp.301/">biases</a> in the resulting models.
</p>
<p>
Alternate sources of information are <a href="https://blog.google/products/search/introducing-knowledge-graph-things-not/">knowledge graphs</a> (KGs), which consist of structured data. KGs are <a href="https://support.google.com/knowledgepanel/answer/9787176?hl=en">factual in nature</a> because the information is usually extracted from more trusted sources, and post-processing filters and human editors ensure inappropriate and incorrect content are removed. Therefore, models that can incorporate them carry the advantages of improved factual accuracy and reduced <a href="https://www.aclweb.org/anthology/2020.findings-emnlp.301/">toxicity</a>. However, their different structural format makes it difficult to integrate them with the existing pre-training corpora in language models.  
</p>
<p>
In &#8220;<a href="https://arxiv.org/abs/2010.12688">Knowledge Graph Based Synthetic Corpus Generation for Knowledge-Enhanced Language Model Pre-training</a>&#8221; (KELM), accepted at <a href="https://2021.naacl.org/">NAACL 2021</a>, we explore converting KGs to synthetic natural language sentences to augment existing pre-training corpora, enabling their integration into the pre-training of language models without architectural changes. To that end, we leverage the publicly available <a href="https://www.wikidata.org/wiki/Wikidata:Main_Page">English Wikidata</a> KG and convert it into natural language text in order to create a synthetic corpus. We then augment <a href="https://ai.googleblog.com/2020/08/realm-integrating-retrieval-into.html">REALM</a>, a retrieval-based language model, with the synthetic corpus as a method of integrating natural language corpora and KGs in pre-training. We have <a href="https://github.com/google-research-datasets/KELM-corpus">released this corpus publicly</a> for the broader research community.
</p>
<p>
<b>Converting KG to Natural Language Text</b><br/>
KGs consist of factual information represented explicitly in a structured format, generally in the form of [subject entity, relation, object entity] <em>triples</em>, e.g., [10x10 photobooks, inception, 2012].  A group of related triples is called an <em>entity subgraph</em>. An example of an entity subgraph that builds on the previous example of a triple is { [10x10 photobooks, instance of, Nonprofit Organization], [10x10 photobooks, inception, 2012] }, which is illustrated in the figure below. A KG can be viewed as interconnected entity subgraphs.
</p>
<p>
Converting subgraphs into natural language text is a standard task in NLP known as <a href="http://nlpprogress.com/english/data_to_text_generation.html">data-to-text generation</a>. Although there have been significant advances on data-to-text-generation on benchmark datasets such as <a href="https://www.aclweb.org/anthology/2020.webnlg-1.7/">WebNLG</a>, converting an <em>entire</em> KG into natural text has additional challenges. The entities and relations in large KGs are more vast and diverse than small benchmark datasets. Moreover, benchmark datasets consist of predefined subgraphs that can form fluent meaningful sentences. With an entire KG, such a segmentation into entity subgraphs needs to be created as well.</p>

<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://1.bp.blogspot.com/-nJJW2LQI2JA/YKbHqaO7zQI/AAAAAAAAHo0/8tpbXWgqR8c4tayq5eghFckfmejsWFVsACLcBGAsYHQ/s960/image2.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="428" data-original-width="960" height="286" src="https://1.bp.blogspot.com/-nJJW2LQI2JA/YKbHqaO7zQI/AAAAAAAAHo0/8tpbXWgqR8c4tayq5eghFckfmejsWFVsACLcBGAsYHQ/w640-h286/image2.png" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">An example illustration of how the pipeline converts an entity subgraph <b>(in bubbles)</b> into synthetic natural sentences <b>(far right)</b>.</td></tr></tbody></table>

<p>
In order to convert the Wikidata KG into synthetic natural sentences, we developed a verbalization pipeline named &#8220;Text from KG Generator&#8221; (TEKGEN), which is made up of the following components: a large training corpus of heuristically aligned Wikipedia text and Wikidata KG triples, a text-to-text generator (<a href="https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html">T5</a>) to convert the KG triples to text, an entity subgraph creator for generating groups of triples to be verbalized together, and finally, a post-processing filter to remove low quality outputs. The result is a corpus containing the entire Wikidata KG as natural text, which we call the <em>Knowledge-Enhanced Language Model (KELM) corpus</em>. It consists of ~18M sentences spanning ~45M triples and ~1500 relations.
</p>

<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://1.bp.blogspot.com/-nmL6-ewUCDY/YKbGXaZSYUI/AAAAAAAAHoY/nViZMBlmECA5SkkO1nPpa2Mxl6m7wW-QQCLcBGAsYHQ/s1600/image1.gif" imageanchor="1" style="margin-left: auto; margin-right: auto; text-align: center;"><img border="0" data-original-height="956" data-original-width="1600" height="382" src="https://1.bp.blogspot.com/-nmL6-ewUCDY/YKbGXaZSYUI/AAAAAAAAHoY/nViZMBlmECA5SkkO1nPpa2Mxl6m7wW-QQCLcBGAsYHQ/w640-h382/image1.gif" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;">Converting a KG to natural language, which is then used for language model augmentation</td></tr></tbody></table>

<p>
<b>Integrating Knowledge Graph and Natural Text for Language Model Pre-training</b><br/>
Our evaluation shows that KG verbalization is an effective method of integrating KGs with natural language text. We demonstrate this by augmenting the retrieval corpus of REALM, which includes only Wikipedia text. 
</p>
<p>
To assess the effectiveness of verbalization, we augment the REALM retrieval corpus with the KELM corpus (i.e., &#8220;verbalized triples&#8221;) and compare its performance against augmentation with concatenated triples <em>without</em> verbalization. We measure the accuracy with each data augmentation technique on two popular open-domain question answering datasets: <a href="https://ai.google.com/research/NaturalQuestions/">Natural Questions</a> and <a href="https://worksheets.codalab.org/worksheets/0xba659fe363cb46e7a505c5b6a774dc8a">Web Questions</a>.
</p>

<table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;"><tbody><tr><td style="text-align: center;"><a href="https://1.bp.blogspot.com/-Bw35C2PhvKM/YKbGyVTUxsI/AAAAAAAAHos/T0FM4l2LWl0icNT1NuPr_3fneC84K_yVgCLcBGAsYHQ/s1928/Screen%2BShot%2B2021-05-20%2Bat%2B4.25.25%2BPM.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="646" data-original-width="1928" height="214" src="https://1.bp.blogspot.com/-Bw35C2PhvKM/YKbGyVTUxsI/AAAAAAAAHos/T0FM4l2LWl0icNT1NuPr_3fneC84K_yVgCLcBGAsYHQ/w640-h214/Screen%2BShot%2B2021-05-20%2Bat%2B4.25.25%2BPM.png" width="640" /></a></td></tr><tr><td class="tr-caption" style="text-align: center;"></td></tr></tbody></table>

<p>
Augmenting REALM with even the concatenated triples improves accuracy, potentially adding information not expressed in text explicitly or at all. However, augmentation with verbalized triples allows for a smoother integration of the KG with the natural language text corpus, as demonstrated by the higher accuracy. We also observed the same trend on a knowledge probe called <a href="https://www.aclweb.org/anthology/D19-1250/">LAMA</a> that queries the model using fill-in-the-blank questions.
</p>

<p>
<b>Conclusion</b><br/>
With KELM, we provide a <a href="https://github.com/google-research-datasets/KELM-corpus">publicly-available corpus</a> of a KG as natural text. We show that KG verbalization can be used to integrate KGs with natural text corpora to overcome their structural differences. This has real-world applications for knowledge-intensive tasks, such as <a href="https://en.wikipedia.org/wiki/Question_answering#:~:text=Question%20answering%20(QA)%20is%20a,humans%20in%20a%20natural%20language.">question answering</a>, where providing factual knowledge is essential. Moreover, such corpora can be applied in pre-training of large language models, and can potentially reduce toxicity and improve factuality. We hope that this work encourages further advances in integrating structured knowledge sources into pre-training of large language models.
</p>
<p>
<b>Acknowledgements</b><br/>
<em>This work has been a collaborative effort involving Oshin Agarwal, Heming Ge, Siamak Shakeri and Rami Al-Rfou. We thank William Woods, Jonni Kanerva, Tania Rojas-Esponda, Jianmo Ni, Aaron Cohen and Itai Rolnick for rating a sample of the synthetic corpus to evaluate its quality. We also thank Kelvin Guu for his valuable feedback on the paper.</em>
</p>
<span itemprop='author' itemscope='itemscope' itemtype='http://schema.org/Person'>
  <meta content='https://plus.google.com/116899029375914044550' itemprop='url'/>
</span>
                        </script> 
          <noscript> <span class="byline-author">Posted by Siamak Shakeri, Staff Software Engineer and Oshin Agarwal, Research Intern, Google Research</span> 
           <p> Large pre-trained <a href="https://en.wikipedia.org/wiki/Natural_language_processing">natural language processing</a> (NLP) models, such as <a href="https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html">BERT</a>, <a href="https://arxiv.org/abs/1907.11692">RoBERTa</a>, <a href="https://arxiv.org/abs/2005.14165">GPT-3</a>, <a href="https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html">T5</a> and <a href="https://ai.googleblog.com/2020/08/realm-integrating-retrieval-into.html">REALM</a>, leverage natural language corpora that are derived from the Web and fine-tuned on task specific data, and have made significant advances in various NLP tasks. However, natural language text alone represents a limited coverage of knowledge, and facts may be contained in wordy sentences in many different ways. Furthermore, existence of non-factual information and toxic content in text can eventually cause <a href="https://www.aclweb.org/anthology/2020.findings-emnlp.301/">biases</a> in the resulting models. </p> 
           <p> Alternate sources of information are <a href="https://blog.google/products/search/introducing-knowledge-graph-things-not/">knowledge graphs</a> (KGs), which consist of structured data. KGs are <a href="https://support.google.com/knowledgepanel/answer/9787176?hl=en">factual in nature</a> because the information is usually extracted from more trusted sources, and post-processing filters and human editors ensure inappropriate and incorrect content are removed. Therefore, models that can incorporate them carry the advantages of improved factual accuracy and reduced <a href="https://www.aclweb.org/anthology/2020.findings-emnlp.301/">toxicity</a>. However, their different structural format makes it difficult to integrate them with the existing pre-training corpora in language models. </p> 
           <p> In “<a href="https://arxiv.org/abs/2010.12688">Knowledge Graph Based Synthetic Corpus Generation for Knowledge-Enhanced Language Model Pre-training</a>” (KELM), accepted at <a href="https://2021.naacl.org/">NAACL 2021</a>, we explore converting KGs to synthetic natural language sentences to augment existing pre-training corpora, enabling their integration into the pre-training of language models without architectural changes. To that end, we leverage the publicly available <a href="https://www.wikidata.org/wiki/Wikidata:Main_Page">English Wikidata</a> KG and convert it into natural language text in order to create a synthetic corpus. We then augment <a href="https://ai.googleblog.com/2020/08/realm-integrating-retrieval-into.html">REALM</a>, a retrieval-based language model, with the synthetic corpus as a method of integrating natural language corpora and KGs in pre-training. We have <a href="https://github.com/google-research-datasets/KELM-corpus">released this corpus publicly</a> for the broader research community. </p> 
           <p> <b>Converting KG to Natural Language Text</b><br> KGs consist of factual information represented explicitly in a structured format, generally in the form of [subject entity, relation, object entity] <em>triples</em>, e.g., [10x10 photobooks, inception, 2012]. A group of related triples is called an <em>entity subgraph</em>. An example of an entity subgraph that builds on the previous example of a triple is { [10x10 photobooks, instance of, Nonprofit Organization], [10x10 photobooks, inception, 2012] }, which is illustrated in the figure below. A KG can be viewed as interconnected entity subgraphs. </p> 
           <p> Converting subgraphs into natural language text is a standard task in NLP known as <a href="http://nlpprogress.com/english/data_to_text_generation.html">data-to-text generation</a>. Although there have been significant advances on data-to-text-generation on benchmark datasets such as <a href="https://www.aclweb.org/anthology/2020.webnlg-1.7/">WebNLG</a>, converting an <em>entire</em> KG into natural text has additional challenges. The entities and relations in large KGs are more vast and diverse than small benchmark datasets. Moreover, benchmark datasets consist of predefined subgraphs that can form fluent meaningful sentences. With an entire KG, such a segmentation into entity subgraphs needs to be created as well.</p> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td style="text-align: center;"><a href="https://1.bp.blogspot.com/-nJJW2LQI2JA/YKbHqaO7zQI/AAAAAAAAHo0/8tpbXWgqR8c4tayq5eghFckfmejsWFVsACLcBGAsYHQ/s960/image2.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="428" data-original-width="960" height="286" src="https://1.bp.blogspot.com/-nJJW2LQI2JA/YKbHqaO7zQI/AAAAAAAAHo0/8tpbXWgqR8c4tayq5eghFckfmejsWFVsACLcBGAsYHQ/w640-h286/image2.png" width="640"></a></td>
             </tr>
             <tr>
              <td class="tr-caption" style="text-align: center;">An example illustration of how the pipeline converts an entity subgraph <b>(in bubbles)</b> into synthetic natural sentences <b>(far right)</b>.</td>
             </tr>
            </tbody>
           </table> 
           <p> In order to convert the Wikidata KG into synthetic natural sentences, we developed a verbalization pipeline named “Text from KG Generator” (TEKGEN), which is made up of the following components: a large training corpus of heuristically aligned Wikipedia text and Wikidata KG triples, a text-to-text generator (<a href="https://ai.googleblog.com/2020/02/exploring-transfer-learning-with-t5.html">T5</a>) to convert the KG triples to text, an entity subgraph creator for generating groups of triples to be verbalized together, and finally, a post-processing filter to remove low quality outputs. The result is a corpus containing the entire Wikidata KG as natural text, which we call the <em>Knowledge-Enhanced Language Model (KELM) corpus</em>. It consists of ~18M sentences spanning ~45M triples and ~1500 relations. </p> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td style="text-align: center;"><a href="https://1.bp.blogspot.com/-nmL6-ewUCDY/YKbGXaZSYUI/AAAAAAAAHoY/nViZMBlmECA5SkkO1nPpa2Mxl6m7wW-QQCLcBGAsYHQ/s1600/image1.gif" imageanchor="1" style="margin-left: auto; margin-right: auto; text-align: center;"><img border="0" data-original-height="956" data-original-width="1600" height="382" src="https://1.bp.blogspot.com/-nmL6-ewUCDY/YKbGXaZSYUI/AAAAAAAAHoY/nViZMBlmECA5SkkO1nPpa2Mxl6m7wW-QQCLcBGAsYHQ/w640-h382/image1.gif" width="640"></a></td>
             </tr>
             <tr>
              <td class="tr-caption" style="text-align: center;">Converting a KG to natural language, which is then used for language model augmentation</td>
             </tr>
            </tbody>
           </table> 
           <p> <b>Integrating Knowledge Graph and Natural Text for Language Model Pre-training</b><br> Our evaluation shows that KG verbalization is an effective method of integrating KGs with natural language text. We demonstrate this by augmenting the retrieval corpus of REALM, which includes only Wikipedia text. </p> 
           <p> To assess the effectiveness of verbalization, we augment the REALM retrieval corpus with the KELM corpus (i.e., “verbalized triples”) and compare its performance against augmentation with concatenated triples <em>without</em> verbalization. We measure the accuracy with each data augmentation technique on two popular open-domain question answering datasets: <a href="https://ai.google.com/research/NaturalQuestions/">Natural Questions</a> and <a href="https://worksheets.codalab.org/worksheets/0xba659fe363cb46e7a505c5b6a774dc8a">Web Questions</a>. </p> 
           <table align="center" cellpadding="0" cellspacing="0" class="tr-caption-container" style="margin-left: auto; margin-right: auto;">
            <tbody>
             <tr>
              <td style="text-align: center;"><a href="https://1.bp.blogspot.com/-Bw35C2PhvKM/YKbGyVTUxsI/AAAAAAAAHos/T0FM4l2LWl0icNT1NuPr_3fneC84K_yVgCLcBGAsYHQ/s1928/Screen%2BShot%2B2021-05-20%2Bat%2B4.25.25%2BPM.png" imageanchor="1" style="margin-left: auto; margin-right: auto;"><img border="0" data-original-height="646" data-original-width="1928" height="214" src="https://1.bp.blogspot.com/-Bw35C2PhvKM/YKbGyVTUxsI/AAAAAAAAHos/T0FM4l2LWl0icNT1NuPr_3fneC84K_yVgCLcBGAsYHQ/w640-h214/Screen%2BShot%2B2021-05-20%2Bat%2B4.25.25%2BPM.png" width="640"></a></td>
             </tr>
             <tr>
              <td class="tr-caption" style="text-align: center;"></td>
             </tr>
            </tbody>
           </table> 
           <p> Augmenting REALM with even the concatenated triples improves accuracy, potentially adding information not expressed in text explicitly or at all. However, augmentation with verbalized triples allows for a smoother integration of the KG with the natural language text corpus, as demonstrated by the higher accuracy. We also observed the same trend on a knowledge probe called <a href="https://www.aclweb.org/anthology/D19-1250/">LAMA</a> that queries the model using fill-in-the-blank questions. </p> 
           <p> <b>Conclusion</b><br> With KELM, we provide a <a href="https://github.com/google-research-datasets/KELM-corpus">publicly-available corpus</a> of a KG as natural text. We show that KG verbalization can be used to integrate KGs with natural text corpora to overcome their structural differences. This has real-world applications for knowledge-intensive tasks, such as <a href="https://en.wikipedia.org/wiki/Question_answering#:~:text=Question%20answering%20(QA)%20is%20a,humans%20in%20a%20natural%20language.">question answering</a>, where providing factual knowledge is essential. Moreover, such corpora can be applied in pre-training of large language models, and can potentially reduce toxicity and improve factuality. We hope that this work encourages further advances in integrating structured knowledge sources into pre-training of large language models. </p> 
           <p> <b>Acknowledgements</b><br> <em>This work has been a collaborative effort involving Oshin Agarwal, Heming Ge, Siamak Shakeri and Rami Al-Rfou. We thank William Woods, Jonni Kanerva, Tania Rojas-Esponda, Jianmo Ni, Aaron Cohen and Itai Rolnick for rating a sample of the synthetic corpus to evaluate its quality. We also thank Kelvin Guu for his valuable feedback on the paper.</em> </p> <span itemprop="author" itemscope itemtype="http://schema.org/Person"> 
            <meta content="https://plus.google.com/116899029375914044550" itemprop="url"> </span> 
          </noscript> 
         </div> 
        </div> 
        <div class="share"> <span class="twitter-custom social-wrapper" data-href="http://twitter.com/share?text=Google AI Blog:KELM: Integrating Knowledge Graphs with Language Model Pre-training Corpora&amp;url=http://ai.googleblog.com/2021/05/kelm-integrating-knowledge-graphs-with.html&amp;via=googleai"> <img alt="Share on Twitter" height="24" src="https://www.gstatic.com/images/icons/material/system/2x/post_twitter_black_24dp.png" width="24"> </span> <span class="fb-custom social-wrapper" data-href="https://www.facebook.com/sharer.php?u=http://ai.googleblog.com/2021/05/kelm-integrating-knowledge-graphs-with.html"> <img alt="Share on Facebook" height="24" src="https://www.gstatic.com/images/icons/material/system/2x/post_facebook_black_24dp.png" width="24"> </span> 
        </div> 
       </div> 
       <div class="blog-pager" id="blog-pager"> <a class="home-link" href="http://ai.googleblog.com/"> <i class="material-icons"> ? </i> </a> <i class="material-icons disabled"> ? </i> <span id="blog-pager-older-link"> <a class="blog-pager-older-link" href="http://ai.googleblog.com/search?updated-max=2021-05-20T14:17:00-07:00&amp;max-results=10" id="Blog1_blog-pager-older-link" title="Older Posts"> <i class="material-icons"> ? </i> </a> </span> 
       </div> 
       <div class="clear"></div> 
      </div>
     </div> 
    </div> 
   </div> 
   <div class="col-right"> 
    <div class="section" id="sidebar-top">
     <div class="widget HTML" data-version="1" id="HTML8"> 
      <div class="widget-content"> 
       <div class="searchBox"> 
        <input type="text" title="Search This Blog" placeholder="Search blog ..."> 
       </div> 
      </div> 
      <div class="clear"></div> <span class="widget-item-control"> <span class="item-control blog-admin"> <a class="quickedit" onclick="return _WidgetManager._PopupConfig(document.getElementById(&quot;HTML8&quot;));" rel="nofollow" target="configHTML8" title="Edit"> <img alt="" height="18" src="https://resources.blogblog.com/img/icon18_wrench_allbkg.png" width="18"> </a> </span> </span> 
      <div class="clear"></div> 
     </div>
    </div> 
    <div id="aside"> 
     <div class="section" id="sidebar">
      <div class="widget Label" data-version="1" id="Label1"> 
       <div class="tab"> 
        <img class="sidebar-icon" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAwCAYAAABXAvmHAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAYpJREFUeNrs2aFuwzAQBmAvKRkMKRjZA4QMDJaWFgyMjuzFRg37DIUlA3uFkoGQSaWzJU+tpri5O9+l/zSfdFJlpe59yTmyVedq1PjfcZMZ70NuQnaF8w8htyE/rABtpviXkLcK88c5HhLkMBfgVan43zfFBNGMjHVGT/s55KP2pAvidbGHd+nzKt1RKSLG3rKF1iPFv6UWiPke8i7kEqGdGsI1O+LYVdqJAjgirwkKYD0ytkJBUNbAMvX8V3q9PhUsYvU1sWD8SO/sQvx2ahxOiNoJCSBCoAHYCEQAC4EKICOQASQEOmAS8RcAFxFN5hiIiugpgC3wk9hQAHH/70EBHXUN7IER5EWMiBgo2+nzOKQv9SCAeEM/OQAkhE/ncccFICB87qzQMia5FsJfOui0zMnmRvipU1ormHQuxGTxUsAcCFLxJQBLBLn4UoAFglW8BkATwS5eC6CBEBWvCShBiIvXBkgQRcVbADiI4uKtABSESvGWgB9EzHt3+tNwyO0qa9SoIYtvAQYAqDJhaWWeMecAAAAASUVORK5CYII="> 
        <h2> Labels </h2> <i class="material-icons arrow"> ? </i> 
       </div> 
       <div class="widget-content list-label-widget-content"> 
        <ul> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/accessibility"> accessibility </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/ACL"> ACL </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/ACM"> ACM </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Acoustic%20Modeling"> Acoustic Modeling </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Adaptive%20Data%20Analysis"> Adaptive Data Analysis </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/ads"> ads </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/adsense"> adsense </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/adwords"> adwords </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Africa"> Africa </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/AI"> AI </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/AI%20for%20Social%20Good"> AI for Social Good </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Algorithms"> Algorithms </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Android"> Android </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Android%20Wear"> Android Wear </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/API"> API </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/App%20Engine"> App Engine </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/App%20Inventor"> App Inventor </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/April%20Fools"> April Fools </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Art"> Art </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Audio"> Audio </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Augmented%20Reality"> Augmented Reality </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Australia"> Australia </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Automatic%20Speech%20Recognition"> Automatic Speech Recognition </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/AutoML"> AutoML </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Awards"> Awards </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/BigQuery"> BigQuery </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Cantonese"> Cantonese </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Chemistry"> Chemistry </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/China"> China </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Chrome"> Chrome </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Cloud%20Computing"> Cloud Computing </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Collaboration"> Collaboration </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Compression"> Compression </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Computational%20Imaging"> Computational Imaging </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Computational%20Photography"> Computational Photography </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Computer%20Science"> Computer Science </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Computer%20Vision"> Computer Vision </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/conference"> conference </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/conferences"> conferences </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Conservation"> Conservation </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/correlate"> correlate </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Course%20Builder"> Course Builder </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/crowd-sourcing"> crowd-sourcing </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/CVPR"> CVPR </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Data%20Center"> Data Center </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Data%20Discovery"> Data Discovery </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/data%20science"> data science </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/datasets"> datasets </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Deep%20Learning"> Deep Learning </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/DeepDream"> DeepDream </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/DeepMind"> DeepMind </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/distributed%20systems"> distributed systems </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Diversity"> Diversity </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Earth%20Engine"> Earth Engine </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/economics"> economics </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Education"> Education </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Electronic%20Commerce%20and%20Algorithms"> Electronic Commerce and Algorithms </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/electronics"> electronics </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/EMEA"> EMEA </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/EMNLP"> EMNLP </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Encryption"> Encryption </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/entities"> entities </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Entity%20Salience"> Entity Salience </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Environment"> Environment </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Europe"> Europe </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Exacycle"> Exacycle </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Expander"> Expander </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Faculty%20Institute"> Faculty Institute </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Faculty%20Summit"> Faculty Summit </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Flu%20Trends"> Flu Trends </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Fusion%20Tables"> Fusion Tables </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/gamification"> gamification </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Gboard"> Gboard </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Gmail"> Gmail </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Google%20Accelerated%20Science"> Google Accelerated Science </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Google%20Books"> Google Books </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Google%20Brain"> Google Brain </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Google%20Cloud%20Platform"> Google Cloud Platform </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Google%20Docs"> Google Docs </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Google%20Drive"> Google Drive </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Google%20Genomics"> Google Genomics </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Google%20Maps"> Google Maps </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Google%20Photos"> Google Photos </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Google%20Play%20Apps"> Google Play Apps </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Google%20Science%20Fair"> Google Science Fair </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Google%20Sheets"> Google Sheets </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Google%20Translate"> Google Translate </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Google%20Trips"> Google Trips </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Google%20Voice%20Search"> Google Voice Search </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Google%2B"> Google+ </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Government"> Government </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/grants"> grants </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Graph"> Graph </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Graph%20Mining"> Graph Mining </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Hardware"> Hardware </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/HCI"> HCI </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Health"> Health </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/High%20Dynamic%20Range%20Imaging"> High Dynamic Range Imaging </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/ICCV"> ICCV </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/ICLR"> ICLR </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/ICML"> ICML </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/ICSE"> ICSE </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Image%20Annotation"> Image Annotation </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Image%20Classification"> Image Classification </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Image%20Processing"> Image Processing </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Inbox"> Inbox </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/India"> India </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Information%20Retrieval"> Information Retrieval </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/internationalization"> internationalization </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Internet%20of%20Things"> Internet of Things </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Interspeech"> Interspeech </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/IPython"> IPython </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Journalism"> Journalism </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/jsm"> jsm </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/jsm2011"> jsm2011 </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/K-12"> K-12 </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Kaggle"> Kaggle </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/KDD"> KDD </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Keyboard%20Input"> Keyboard Input </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Klingon"> Klingon </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Korean"> Korean </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Labs"> Labs </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Linear%20Optimization"> Linear Optimization </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/localization"> localization </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Low-Light%20Photography"> Low-Light Photography </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Machine%20Hearing"> Machine Hearing </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Machine%20Intelligence"> Machine Intelligence </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Machine%20Learning"> Machine Learning </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Machine%20Perception"> Machine Perception </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Machine%20Translation"> Machine Translation </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Magenta"> Magenta </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/MapReduce"> MapReduce </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/market%20algorithms"> market algorithms </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Market%20Research"> Market Research </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Mixed%20Reality"> Mixed Reality </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/ML"> ML </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/ML%20Fairness"> ML Fairness </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/MOOC"> MOOC </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Moore%27s%20Law"> Moore's Law </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Multimodal%20Learning"> Multimodal Learning </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/NAACL"> NAACL </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Natural%20Language%20Processing"> Natural Language Processing </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Natural%20Language%20Understanding"> Natural Language Understanding </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Network%20Management"> Network Management </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Networks"> Networks </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Neural%20Networks"> Neural Networks </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/NeurIPS"> NeurIPS </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Nexus"> Nexus </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Ngram"> Ngram </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/NIPS"> NIPS </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/NLP"> NLP </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/On-device%20Learning"> On-device Learning </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/open%20source"> open source </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/operating%20systems"> operating systems </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Optical%20Character%20Recognition"> Optical Character Recognition </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/optimization"> optimization </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/osdi"> osdi </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/osdi10"> osdi10 </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/patents"> patents </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Peer%20Review"> Peer Review </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/ph.d.%20fellowship"> ph.d. fellowship </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/PhD%20Fellowship"> PhD Fellowship </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/PhotoScan"> PhotoScan </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Physics"> Physics </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/PiLab"> PiLab </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Pixel"> Pixel </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Policy"> Policy </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Professional%20Development"> Professional Development </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Proposals"> Proposals </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Public%20Data%20Explorer"> Public Data Explorer </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/publication"> publication </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Publications"> Publications </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Quantum%20AI"> Quantum AI </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Quantum%20Computing"> Quantum Computing </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Recommender%20Systems"> Recommender Systems </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Reinforcement%20Learning"> Reinforcement Learning </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/renewable%20energy"> renewable energy </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Research"> Research </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Research%20Awards"> Research Awards </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/resource%20optimization"> resource optimization </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Robotics"> Robotics </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/schema.org"> schema.org </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Search"> Search </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/search%20ads"> search ads </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Security%20and%20Privacy"> Security and Privacy </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Self-Supervised%20Learning"> Self-Supervised Learning </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Semantic%20Models"> Semantic Models </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Semi-supervised%20Learning"> Semi-supervised Learning </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/SIGCOMM"> SIGCOMM </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/SIGMOD"> SIGMOD </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Site%20Reliability%20Engineering"> Site Reliability Engineering </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Social%20Networks"> Social Networks </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Software"> Software </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Sound%20Search"> Sound Search </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Speech"> Speech </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Speech%20Recognition"> Speech Recognition </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/statistics"> statistics </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Structured%20Data"> Structured Data </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Style%20Transfer"> Style Transfer </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Supervised%20Learning"> Supervised Learning </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Systems"> Systems </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/TensorBoard"> TensorBoard </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/TensorFlow"> TensorFlow </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/TPU"> TPU </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Translate"> Translate </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/trends"> trends </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/TTS"> TTS </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/TV"> TV </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/UI"> UI </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/University%20Relations"> University Relations </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/UNIX"> UNIX </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Unsupervised%20Learning"> Unsupervised Learning </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/User%20Experience"> User Experience </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/video"> video </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Video%20Analysis"> Video Analysis </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Virtual%20Reality"> Virtual Reality </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Vision%20Research"> Vision Research </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Visiting%20Faculty"> Visiting Faculty </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Visualization"> Visualization </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/VLDB"> VLDB </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Voice%20Search"> Voice Search </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Wiki"> Wiki </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/wikipedia"> wikipedia </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/WWW"> WWW </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/Year%20in%20Review"> Year in Review </a> </li> 
         <li> <a dir="ltr" href="http://ai.googleblog.com/search/label/YouTube"> YouTube </a> </li> 
        </ul> 
        <div class="clear"></div> <span class="widget-item-control"> <span class="item-control blog-admin"> <a class="quickedit" onclick="return _WidgetManager._PopupConfig(document.getElementById(&quot;Label1&quot;));" rel="nofollow" target="configLabel1" title="Edit"> <img alt="" height="18" src="https://resources.blogblog.com/img/icon18_wrench_allbkg.png" width="18"> </a> </span> </span> 
        <div class="clear"></div> 
       </div> 
      </div>
      <div class="widget BlogArchive" data-version="1" id="BlogArchive1"> 
       <div class="tab"> <i class="material-icons icon"> ? </i> 
        <h2> Archive </h2> <i class="material-icons arrow"> ? </i> 
       </div> 
       <div class="widget-content"> 
        <div id="ArchiveList"> 
         <div id="BlogArchive1_ArchiveList"> 
          <ul class="hierarchy"> 
           <li class="archivedate expanded"> 
            <div class="intervalToggle"> <span class="new-toggle" href="javascript:void(0)"> <i class="material-icons arrow"> ? </i> </span> <a class="toggle" href="javascript:void(0)" style="display: none"> <span class="zippy toggle-open"> <i class="material-icons"> ? </i> &nbsp; </span> </a> <a class="post-count-link" href="http://ai.googleblog.com/2021/"> 2021 </a> 
            </div> 
            <div class="items"> 
             <ul class="hierarchy"> 
              <li class="archivedate expanded"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2021/06/"> Jun </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2021/05/"> May </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2021/04/"> Apr </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2021/03/"> Mar </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2021/02/"> Feb </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2021/01/"> Jan </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
            </div> </li> 
          </ul> 
          <ul class="hierarchy"> 
           <li class="archivedate collapsed"> 
            <div class="intervalToggle"> <span class="new-toggle" href="javascript:void(0)"> <i class="material-icons arrow"> ? </i> </span> <a class="toggle" href="javascript:void(0)" style="display: none"> <span class="zippy"> <i class="material-icons"> ? </i> &nbsp; </span> </a> <a class="post-count-link" href="http://ai.googleblog.com/2020/"> 2020 </a> 
            </div> 
            <div class="items"> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2020/12/"> Dec </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2020/11/"> Nov </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2020/10/"> Oct </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2020/09/"> Sep </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2020/08/"> Aug </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2020/07/"> Jul </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2020/06/"> Jun </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2020/05/"> May </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2020/04/"> Apr </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2020/03/"> Mar </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2020/02/"> Feb </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2020/01/"> Jan </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
            </div> </li> 
          </ul> 
          <ul class="hierarchy"> 
           <li class="archivedate collapsed"> 
            <div class="intervalToggle"> <span class="new-toggle" href="javascript:void(0)"> <i class="material-icons arrow"> ? </i> </span> <a class="toggle" href="javascript:void(0)" style="display: none"> <span class="zippy"> <i class="material-icons"> ? </i> &nbsp; </span> </a> <a class="post-count-link" href="http://ai.googleblog.com/2019/"> 2019 </a> 
            </div> 
            <div class="items"> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2019/12/"> Dec </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2019/11/"> Nov </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2019/10/"> Oct </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2019/09/"> Sep </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2019/08/"> Aug </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2019/07/"> Jul </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2019/06/"> Jun </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2019/05/"> May </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2019/04/"> Apr </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2019/03/"> Mar </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2019/02/"> Feb </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2019/01/"> Jan </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
            </div> </li> 
          </ul> 
          <ul class="hierarchy"> 
           <li class="archivedate collapsed"> 
            <div class="intervalToggle"> <span class="new-toggle" href="javascript:void(0)"> <i class="material-icons arrow"> ? </i> </span> <a class="toggle" href="javascript:void(0)" style="display: none"> <span class="zippy"> <i class="material-icons"> ? </i> &nbsp; </span> </a> <a class="post-count-link" href="http://ai.googleblog.com/2018/"> 2018 </a> 
            </div> 
            <div class="items"> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2018/12/"> Dec </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2018/11/"> Nov </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2018/10/"> Oct </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2018/09/"> Sep </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2018/08/"> Aug </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2018/07/"> Jul </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2018/06/"> Jun </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2018/05/"> May </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2018/04/"> Apr </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2018/03/"> Mar </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2018/02/"> Feb </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2018/01/"> Jan </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
            </div> </li> 
          </ul> 
          <ul class="hierarchy"> 
           <li class="archivedate collapsed"> 
            <div class="intervalToggle"> <span class="new-toggle" href="javascript:void(0)"> <i class="material-icons arrow"> ? </i> </span> <a class="toggle" href="javascript:void(0)" style="display: none"> <span class="zippy"> <i class="material-icons"> ? </i> &nbsp; </span> </a> <a class="post-count-link" href="http://ai.googleblog.com/2017/"> 2017 </a> 
            </div> 
            <div class="items"> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2017/12/"> Dec </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2017/11/"> Nov </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2017/10/"> Oct </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2017/09/"> Sep </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2017/08/"> Aug </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2017/07/"> Jul </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2017/06/"> Jun </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2017/05/"> May </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2017/04/"> Apr </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2017/03/"> Mar </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2017/02/"> Feb </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2017/01/"> Jan </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
            </div> </li> 
          </ul> 
          <ul class="hierarchy"> 
           <li class="archivedate collapsed"> 
            <div class="intervalToggle"> <span class="new-toggle" href="javascript:void(0)"> <i class="material-icons arrow"> ? </i> </span> <a class="toggle" href="javascript:void(0)" style="display: none"> <span class="zippy"> <i class="material-icons"> ? </i> &nbsp; </span> </a> <a class="post-count-link" href="http://ai.googleblog.com/2016/"> 2016 </a> 
            </div> 
            <div class="items"> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2016/12/"> Dec </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2016/11/"> Nov </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2016/10/"> Oct </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2016/09/"> Sep </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2016/08/"> Aug </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2016/07/"> Jul </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2016/06/"> Jun </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2016/05/"> May </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2016/04/"> Apr </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2016/03/"> Mar </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2016/02/"> Feb </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2016/01/"> Jan </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
            </div> </li> 
          </ul> 
          <ul class="hierarchy"> 
           <li class="archivedate collapsed"> 
            <div class="intervalToggle"> <span class="new-toggle" href="javascript:void(0)"> <i class="material-icons arrow"> ? </i> </span> <a class="toggle" href="javascript:void(0)" style="display: none"> <span class="zippy"> <i class="material-icons"> ? </i> &nbsp; </span> </a> <a class="post-count-link" href="http://ai.googleblog.com/2015/"> 2015 </a> 
            </div> 
            <div class="items"> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2015/12/"> Dec </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2015/11/"> Nov </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2015/10/"> Oct </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2015/09/"> Sep </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2015/08/"> Aug </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2015/07/"> Jul </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2015/06/"> Jun </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2015/05/"> May </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2015/04/"> Apr </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2015/03/"> Mar </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2015/02/"> Feb </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2015/01/"> Jan </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
            </div> </li> 
          </ul> 
          <ul class="hierarchy"> 
           <li class="archivedate collapsed"> 
            <div class="intervalToggle"> <span class="new-toggle" href="javascript:void(0)"> <i class="material-icons arrow"> ? </i> </span> <a class="toggle" href="javascript:void(0)" style="display: none"> <span class="zippy"> <i class="material-icons"> ? </i> &nbsp; </span> </a> <a class="post-count-link" href="http://ai.googleblog.com/2014/"> 2014 </a> 
            </div> 
            <div class="items"> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2014/12/"> Dec </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2014/11/"> Nov </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2014/10/"> Oct </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2014/09/"> Sep </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2014/08/"> Aug </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2014/07/"> Jul </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2014/06/"> Jun </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2014/05/"> May </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2014/04/"> Apr </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2014/03/"> Mar </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2014/02/"> Feb </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2014/01/"> Jan </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
            </div> </li> 
          </ul> 
          <ul class="hierarchy"> 
           <li class="archivedate collapsed"> 
            <div class="intervalToggle"> <span class="new-toggle" href="javascript:void(0)"> <i class="material-icons arrow"> ? </i> </span> <a class="toggle" href="javascript:void(0)" style="display: none"> <span class="zippy"> <i class="material-icons"> ? </i> &nbsp; </span> </a> <a class="post-count-link" href="http://ai.googleblog.com/2013/"> 2013 </a> 
            </div> 
            <div class="items"> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2013/12/"> Dec </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2013/11/"> Nov </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2013/10/"> Oct </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2013/09/"> Sep </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2013/08/"> Aug </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2013/07/"> Jul </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2013/06/"> Jun </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2013/05/"> May </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2013/04/"> Apr </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2013/03/"> Mar </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2013/02/"> Feb </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2013/01/"> Jan </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
            </div> </li> 
          </ul> 
          <ul class="hierarchy"> 
           <li class="archivedate collapsed"> 
            <div class="intervalToggle"> <span class="new-toggle" href="javascript:void(0)"> <i class="material-icons arrow"> ? </i> </span> <a class="toggle" href="javascript:void(0)" style="display: none"> <span class="zippy"> <i class="material-icons"> ? </i> &nbsp; </span> </a> <a class="post-count-link" href="http://ai.googleblog.com/2012/"> 2012 </a> 
            </div> 
            <div class="items"> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2012/12/"> Dec </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2012/10/"> Oct </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2012/09/"> Sep </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2012/08/"> Aug </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2012/07/"> Jul </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2012/06/"> Jun </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2012/05/"> May </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2012/04/"> Apr </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2012/03/"> Mar </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2012/02/"> Feb </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2012/01/"> Jan </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
            </div> </li> 
          </ul> 
          <ul class="hierarchy"> 
           <li class="archivedate collapsed"> 
            <div class="intervalToggle"> <span class="new-toggle" href="javascript:void(0)"> <i class="material-icons arrow"> ? </i> </span> <a class="toggle" href="javascript:void(0)" style="display: none"> <span class="zippy"> <i class="material-icons"> ? </i> &nbsp; </span> </a> <a class="post-count-link" href="http://ai.googleblog.com/2011/"> 2011 </a> 
            </div> 
            <div class="items"> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2011/12/"> Dec </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2011/11/"> Nov </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2011/09/"> Sep </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2011/08/"> Aug </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2011/07/"> Jul </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2011/06/"> Jun </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2011/05/"> May </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2011/04/"> Apr </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2011/03/"> Mar </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2011/02/"> Feb </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2011/01/"> Jan </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
            </div> </li> 
          </ul> 
          <ul class="hierarchy"> 
           <li class="archivedate collapsed"> 
            <div class="intervalToggle"> <span class="new-toggle" href="javascript:void(0)"> <i class="material-icons arrow"> ? </i> </span> <a class="toggle" href="javascript:void(0)" style="display: none"> <span class="zippy"> <i class="material-icons"> ? </i> &nbsp; </span> </a> <a class="post-count-link" href="http://ai.googleblog.com/2010/"> 2010 </a> 
            </div> 
            <div class="items"> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2010/12/"> Dec </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2010/11/"> Nov </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2010/10/"> Oct </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2010/09/"> Sep </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2010/08/"> Aug </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2010/07/"> Jul </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2010/06/"> Jun </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2010/05/"> May </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2010/04/"> Apr </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2010/03/"> Mar </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2010/02/"> Feb </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2010/01/"> Jan </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
            </div> </li> 
          </ul> 
          <ul class="hierarchy"> 
           <li class="archivedate collapsed"> 
            <div class="intervalToggle"> <span class="new-toggle" href="javascript:void(0)"> <i class="material-icons arrow"> ? </i> </span> <a class="toggle" href="javascript:void(0)" style="display: none"> <span class="zippy"> <i class="material-icons"> ? </i> &nbsp; </span> </a> <a class="post-count-link" href="http://ai.googleblog.com/2009/"> 2009 </a> 
            </div> 
            <div class="items"> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2009/12/"> Dec </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2009/11/"> Nov </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2009/08/"> Aug </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2009/07/"> Jul </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2009/06/"> Jun </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2009/05/"> May </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2009/04/"> Apr </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2009/03/"> Mar </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2009/02/"> Feb </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2009/01/"> Jan </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
            </div> </li> 
          </ul> 
          <ul class="hierarchy"> 
           <li class="archivedate collapsed"> 
            <div class="intervalToggle"> <span class="new-toggle" href="javascript:void(0)"> <i class="material-icons arrow"> ? </i> </span> <a class="toggle" href="javascript:void(0)" style="display: none"> <span class="zippy"> <i class="material-icons"> ? </i> &nbsp; </span> </a> <a class="post-count-link" href="http://ai.googleblog.com/2008/"> 2008 </a> 
            </div> 
            <div class="items"> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2008/12/"> Dec </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2008/11/"> Nov </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2008/10/"> Oct </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2008/09/"> Sep </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2008/07/"> Jul </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2008/05/"> May </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2008/04/"> Apr </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2008/03/"> Mar </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2008/02/"> Feb </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
            </div> </li> 
          </ul> 
          <ul class="hierarchy"> 
           <li class="archivedate collapsed"> 
            <div class="intervalToggle"> <span class="new-toggle" href="javascript:void(0)"> <i class="material-icons arrow"> ? </i> </span> <a class="toggle" href="javascript:void(0)" style="display: none"> <span class="zippy"> <i class="material-icons"> ? </i> &nbsp; </span> </a> <a class="post-count-link" href="http://ai.googleblog.com/2007/"> 2007 </a> 
            </div> 
            <div class="items"> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2007/10/"> Oct </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2007/09/"> Sep </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2007/08/"> Aug </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2007/07/"> Jul </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2007/06/"> Jun </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2007/02/"> Feb </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
            </div> </li> 
          </ul> 
          <ul class="hierarchy"> 
           <li class="archivedate collapsed"> 
            <div class="intervalToggle"> <span class="new-toggle" href="javascript:void(0)"> <i class="material-icons arrow"> ? </i> </span> <a class="toggle" href="javascript:void(0)" style="display: none"> <span class="zippy"> <i class="material-icons"> ? </i> &nbsp; </span> </a> <a class="post-count-link" href="http://ai.googleblog.com/2006/"> 2006 </a> 
            </div> 
            <div class="items"> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2006/12/"> Dec </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2006/11/"> Nov </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2006/09/"> Sep </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2006/08/"> Aug </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2006/07/"> Jul </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2006/06/"> Jun </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2006/04/"> Apr </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2006/03/"> Mar </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
             <ul class="hierarchy"> 
              <li class="archivedate collapsed"> 
               <div class=""> <a class="post-count-link" href="http://ai.googleblog.com/2006/02/"> Feb </a> 
               </div> 
               <div class="items"> 
               </div> </li> 
             </ul> 
            </div> </li> 
          </ul> 
         </div> 
        </div> 
        <div class="clear"></div> <span class="widget-item-control"> <span class="item-control blog-admin"> <a class="quickedit" onclick="return _WidgetManager._PopupConfig(document.getElementById(&quot;BlogArchive1&quot;));" rel="nofollow" target="configBlogArchive1" title="Edit"> <img alt="" height="18" src="https://resources.blogblog.com/img/icon18_wrench_allbkg.png" width="18"> </a> </span> </span> 
        <div class="clear"></div> 
       </div> 
      </div>
      <div class="widget HTML" data-version="1" id="HTML6"> 
       <div class="widget-content"> <a href="http://googleaiblog.blogspot.com/atom.xml"> <img src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAwCAYAAABXAvmHAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAihJREFUeNrsWa9Pw0AU7viRMDFRBAkzJDMIBIhJJhCzk7NILIqMv4AEhdz+BCY3OYssAlGBoAJREpZwAlHEBO8lr8nSvNeVbu1dyX3JlzTrXfa+u/e9d7c5joWFhYVO1Fa8PwH2gK6m+BRwAvSlAdsrgr8E1jUuMH73GTAEzrkBWymTewZlihhLmgDXIAFuHgGVQOUF7OSYM1p6PgTuA1vAZlUEvAnPdapcMY0VICECekQ0XRfYrqoHsAGNgXfAoMomRiFDEhOZkkL3S88hMaB2LwXp0bj+ps2edpToZpjfoIDQtBeU+xjoDzP2G/gCPKZ5f8WsCAFJoJgOCcFdWSTeL9YQMSvTA1h9BkI5jaiXhLpSCL/8mVZY0UpyJ9ZdOkniu1dmJ96BpzQu9w6s28gcOq9j6pwLdR8/36NK5CQKwJSMrb2MhhSglBpt4UjsrdsnNu0B3J0HCozbCc4TjyY2srEgos/4RQljCzNxl4ireQD8FOq+T+W0mTB2g7njhlR+Sy2jsXFvU658U8YTbeaGpdIu7mWkEAq5ZtIjIhFZdtfX7QHckSvB2B6zC3VdAkZk0kAQwaXTk/CzTXK3wjIExCs6ZJpTnE4uY1KV+KzFzA3KTiFPENHJkOPcsfpLhwe4btoSuvUqAR+6TOxlCE6ZfKUsJLgsqGW8OpqAGx2X+sLxrwUog+JUeQRMDBIwyXOcnlPtPnL0/UsT/8LnOxYWFhZG4leAAQAAQHEaYuzHbAAAAABJRU5ErkJggg==" class="sidebar-icon"> <h2>Feed</h2> </a> 
       </div> 
       <div class="clear"></div> <span class="widget-item-control"> <span class="item-control blog-admin"> <a class="quickedit" onclick="return _WidgetManager._PopupConfig(document.getElementById(&quot;HTML6&quot;));" rel="nofollow" target="configHTML6" title="Edit"> <img alt="" height="18" src="https://resources.blogblog.com/img/icon18_wrench_allbkg.png" width="18"> </a> </span> </span> 
       <div class="clear"></div> 
      </div>
     </div> 
     <div class="section" id="sidebar-bottom">
      <div class="widget HTML" data-version="1" id="HTML5"> 
       <div class="widget-content"> 
        <div class="share followgooglewrapper"> <button data-href="https://twitter.com/intent/follow?original_referer=http://ai.googleblog.com/&amp;screen_name=googleai" onclick="sharingPopup(this);" id="twitter-share"><span class="twitter-follow">Follow @googleai</span></button> 
         <script>
      function sharingPopup (button) {
      var url = button.getAttribute("data-href");
	    window.open(
 url,'popUpWindow','height=500,width=500,left=10,top=10,resizable=yes,scrollbars=yes,toolbar=yes,menubar=no,location=no,directories=no,status=yes');
          }
    </script> 
        </div> 
       </div> 
       <div class="clear"></div> <span class="widget-item-control"> <span class="item-control blog-admin"> <a class="quickedit" onclick="return _WidgetManager._PopupConfig(document.getElementById(&quot;HTML5&quot;));" rel="nofollow" target="configHTML5" title="Edit"> <img alt="" height="18" src="https://resources.blogblog.com/img/icon18_wrench_allbkg.png" width="18"> </a> </span> </span> 
       <div class="clear"></div> 
      </div>
      <div class="widget HTML" data-version="1" id="HTML1"> 
       <div class="widget-content">
         Give us feedback in our <a href="http://support.google.com/bin/static.py?hl=en&amp;page=portal_groups.cs">Product Forums</a>. 
       </div> 
       <div class="clear"></div> <span class="widget-item-control"> <span class="item-control blog-admin"> <a class="quickedit" onclick="return _WidgetManager._PopupConfig(document.getElementById(&quot;HTML1&quot;));" rel="nofollow" target="configHTML1" title="Edit"> <img alt="" height="18" src="https://resources.blogblog.com/img/icon18_wrench_allbkg.png" width="18"> </a> </span> </span> 
       <div class="clear"></div> 
      </div>
     </div> 
    </div> 
   </div> 
   <div style="clear:both;"></div> 
  </div> <!-- Footer --> 
  <div class="google-footer-outer loading"> 
   <div id="google-footer"> <a href="//www.google.com/"> <img class="google-logo-dark" height="36" src="data:image/png;base64,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" style="margin-top: -16px;" width="92"> </a> 
    <ul> 
     <li> <a href="//www.google.com/"> Google </a> </li> 
     <li> <a href="//www.google.com/policies/privacy/"> Privacy </a> </li> 
     <li> <a href="//www.google.com/policies/terms/"> Terms </a> </li> 
    </ul> 
   </div> 
  </div> 
  <script type="text/javascript">
      //<![CDATA[
      // Social sharing popups.
      var postEl = document.getElementsByClassName('social-wrapper');
      var postCount = postEl.length;
      for(i=0; i<postCount;i++){
        postEl[i].addEventListener("click", function(event){
          var postUrl = this.getAttribute("data-href");
          window.open(
            postUrl,'popUpWindow','height=500,width=500,left=10,top=10,resizable=yes,scrollbars=yes,toolbar=yes,menubar=no,location=no,directories=no,status=yes');
        });}
      //]]>
    </script> 
  <script type="text/javascript">
      //<![CDATA[
      var BreakpointHandler = function() {
        this.initted = false;
        this.isHomePage = false;
        this.isMobile = false;
      };
      BreakpointHandler.prototype.finalizeSummary = function(summaryHtml, lastNode) {
        // Use $.trim for IE8 compatibility
        summaryHtml = $.trim(summaryHtml).replace(/(<br>|\s)+$/,'');
        if (lastNode.nodeType == 3) {
          var lastChar = summaryHtml.slice(-1);
          if (!lastChar.match(/[.”"?]/)) {
            if (!lastChar.match(/[A-Za-z]/)) {
              summaryHtml = summaryHtml.slice(0, -1);
            }
            summaryHtml += ' ...';
          }
        } else if (lastNode.nodeType == 1 && (lastNode.nodeName == 'I' || lastNode.nodeName == 'A')) {
          summaryHtml += ' ...';
        }
        return summaryHtml;
      };
      BreakpointHandler.prototype.generateSummaryFromContent = function(content, numWords) {
        var seenWords = 0;
        var summaryHtml = '';
        for (var i=0; i < content.childNodes.length; i++) {
          var node = content.childNodes[i];
          var nodeText;
          if (node.nodeType == 1) {
            if (node.hasAttribute('data-about-pullquote')) {
              continue;
            }
            nodeText = node.textContent;
            if (nodeText === undefined) {
              // innerText for IE8
              nodeText = node.innerText;
            }
            if (node.nodeName == 'DIV' || node.nodeName == 'B') 